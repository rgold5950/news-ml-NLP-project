{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Article counts by publisher:\n",
      "Breitbart           104\n",
      "NY Post              61\n",
      "CNN                  57\n",
      "Reuters              56\n",
      "NPR                  54\n",
      "NY Times             50\n",
      "Washington Post      50\n",
      "Buzzfeed News        48\n",
      "Atlantic             48\n",
      "Business Insider     41\n",
      "Guardian             35\n",
      "National Review      32\n",
      "Fox News             28\n",
      "Name: publication, dtype: int64\n",
      "\n",
      "Article counts by date:\n",
      "2016-12-02    362\n",
      "2016-09-01    302\n",
      "Name: date, dtype: int64\n",
      "\n",
      "Final dataset:\n",
      "\n",
      "Date: 2016-09-01 \n",
      "\n",
      "Breitbart           104\n",
      "NY Post              61\n",
      "CNN                  57\n",
      "Reuters              56\n",
      "NPR                  54\n",
      "NY Times             50\n",
      "Washington Post      50\n",
      "Buzzfeed News        48\n",
      "Atlantic             48\n",
      "Business Insider     41\n",
      "Guardian             35\n",
      "National Review      32\n",
      "Fox News             28\n",
      "Name: publication, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "##########################################################################################\n",
    "processDate = \"2016-09-01\"\n",
    "\n",
    "def removeNonASCIICharacters(textString): \n",
    "    return \"\".join(i for i in textString if ord(i)<128)\n",
    "\n",
    "df = pd.read_csv(r'C:\\Users\\goldm\\Capstone\\data\\articles.csv')\n",
    "df.drop_duplicates('content')\n",
    "df = df[~df['content'].isnull()]\n",
    "df=df[df['content'].str.len()>=200]\n",
    "\n",
    "targetString=\"(Want to get this briefing by email?\"\n",
    "df['NYT summary']=df['content'].map(lambda d: d[:len(targetString)]==targetString)\n",
    "df=df[df['NYT summary']==False]\n",
    "\n",
    "# The following removes a warning that appears in many of the Atlantic articles.\n",
    "# Since it is commonly at the beginning, it brings a lot of noise to the search for similar articles\n",
    "# And subsequently to the assessment of sentiment\n",
    "targetString=\"For us to continue writing great stories, we need to display ads.             Please select the extension that is blocking ads.     Please follow the steps below\"\n",
    "df['content']=df['content'].str.replace(targetString,'')\n",
    "\n",
    "# This is also for some Atlantic articles for the same reasons as above\n",
    "targetString=\"This article is part of a feature we also send out via email as The Atlantic Daily, a newsletter with stories, ideas, and images from The Atlantic, written specially for subscribers. To sign up, please enter your email address in the field provided here.\"\n",
    "df=df[df['content'].str.contains(targetString)==False]\n",
    "\n",
    "# This is also for some Atlantic articles for the same reasons as above\n",
    "targetString=\"This article is part of a feature we also send out via email as Politics  Policy Daily, a daily roundup of events and ideas in American politics written specially for newsletter subscribers. To sign up, please enter your email address in the field provided here.\"\n",
    "df=df[df['content'].str.contains(targetString)==False]\n",
    "\n",
    "# More Atlantic-specific removals (for daily summaries with multiple stories contained)\n",
    "df=df[df['content'].str.contains(\"To sign up, please enter your email address in the field\")==False]\n",
    "\n",
    "# Remove daily CNN summary\n",
    "targetString=\"CNN Student News\"\n",
    "df=df[df['content'].str.contains(targetString)==False]\n",
    "\n",
    "print(\"\\nArticle counts by publisher:\")\n",
    "print(df['publication'].value_counts())\n",
    "\n",
    "print(\"\\nArticle counts by date:\")\n",
    "print(df['date'].value_counts())\n",
    "\n",
    "# Restrict to articles on the provided input date.\n",
    "# This date is considered mandatory for topic clustering but is not required for sentiment\n",
    "# since sentiment only processes a specified list of articles.\n",
    "# For topic clustering it is essential to have the date as it is\n",
    "# enormously significant in article matching.\n",
    "# if processDate!=None:\n",
    "#     df=df[df['date']==processDate]\n",
    "# df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Remove non-ASCII characters\n",
    "df['content no nonascii']=df['content'].map(lambda x: removeNonASCIICharacters(x))\n",
    "\n",
    "print(\"\\nFinal dataset:\\n\\nDate:\",processDate,\"\\n\")\n",
    "print(df['publication'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_df = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'runParams' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-09c4c617ba0d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mstop_words\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mstop_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloadStopWords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrunParams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'stop_words_file'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;31m#NOTE: alternative to importing a flat file of stop words is to just import stop words from the various different libraries.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# from nltk.corpus import stopwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'runParams' is not defined"
     ]
    }
   ],
   "source": [
    "def loadStopWords(stopWordsFileName):\n",
    "    stop_words=[]\n",
    "    f=open(stopWordsFileName,'r')\n",
    "    for l in f.readlines():\n",
    "        stop_words.append(l.replace('\\n', ''))\n",
    "    return stop_words\n",
    "stop_words = loadStopWords(runParams['stop_words_file'][0])\n",
    "#NOTE: alternative to importing a flat file of stop words is to just import stop words from the various different libraries. \n",
    "# from nltk.corpus import stopwords\n",
    "# stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NNP', 'NNPS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import csv\n",
    "\n",
    "runParams={'tfidf_maxdf':      [0.5],\n",
    "           'input_file':       ['./data/articles.csv'],\n",
    "           'story_threshold':  [0.26],\n",
    "           'process_date':     ['2016-09-01'],\n",
    "           'parts_of_speech':  [['PROPER', 'VERB']],\n",
    "           'lemma_conversion': [False],\n",
    "           'ngram_max':        [3],\n",
    "           'tfidf_binary':     [False],\n",
    "           'tfidf_norm':       ['l2'],\n",
    "           'nlp_library':      ['nltk'],\n",
    "           'max_length':       [50],\n",
    "           'stop_words_file':  ['./data/stopWords.txt'],\n",
    "           'tfidf_mindf':      [2],\n",
    "           'display_graph':    [True],\n",
    "           'article_stats':    [False]}\n",
    "# Use parameter grid even if there is only one set of parameters\n",
    "parameterGrid = ParameterGrid(runParams)\n",
    "\n",
    "partsOfSpeech=[]\n",
    "pos_nlp_mapping = {}\n",
    "pos_nlp_mapping['nltk']={'VERB':['VB','VBD','VBG','VBN','VBP','VBZ'],'PROPER':['NNP','NNPS'],'COMMON':['NN','NNS']}\n",
    "\n",
    "for pos in runParams['parts_of_speech'][0]:\n",
    "    partsOfSpeech.append(pos_nlp_mapping['nltk'][pos])\n",
    "partsOfSpeech=[item for sublist in partsOfSpeech for item in sublist]\n",
    "print(partsOfSpeech)\n",
    "\n",
    "import nltk as nl\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "stringToConvert = article_df['content']\n",
    "partsOfSpeech = partsOfSpeech\n",
    "stop_words = stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# initializing the \"story map\" - aka the ground truth used to measure the accuracy of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk as nl\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "def stringNLTKProcess(nl, stringToConvert,partsOfSpeech,stop_words,maxWords=None,lemmatizer=None):\n",
    "    #parses the paragraph into sentences\n",
    "    sentences = nl.sent_tokenize(stringToConvert)\n",
    "    str = []\n",
    "    for sentence in sentences:\n",
    "        wordString=[]\n",
    "        for word, pos in nl.pos_tag(nl.word_tokenize(sentence)):\n",
    "            # The following condition avoids any POS which corresponds to punctuation (and takes all others\n",
    "            if partsOfSpeech == None:\n",
    "                if pos[0]>='A' and pos[0]<='Z':\n",
    "                    wordString.append(word)\n",
    "            elif pos in partsOfSpeech:\n",
    "                wordString.append(word)\n",
    "        for wrd in wordString:\n",
    "            #converts all string characters into lowercase elements\n",
    "            wrdlower=wrd.lower()\n",
    "            if wrdlower not in stop_words and wrdlower!=\"'s'\":\n",
    "                if maxWords==None or len(str)<maxWords:\n",
    "                    if lemmatizer==None:\n",
    "                        str.append(wrdlower)\n",
    "                    else:\n",
    "                        str.append(lemmatizer.lemmatize(wrd.lower(), pos='v'))\n",
    "            if maxWords!=None and len(str)==maxWords:\n",
    "                return ' '.join(str)\n",
    "    return ' '.join(str)\n",
    "\n",
    "def removeSpacesAndPunctuation(textString): \n",
    "    return \"\".join(i for i in textString if (ord(i)>=48 and ord(i)<=57) or (ord(i)>=97 and ord(i)<=122))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setupStoryMapAndReportList(args=None, reportArticleList=None,storyMapFileName=None):\n",
    "    # Story Map is used in fitting if grid search is applied (As ground truth)\n",
    "    # It is also used in graph if no threshold provided (to determine colours, not to determine location)\n",
    "    # Report Article List is used at the end to create a report with, for each\n",
    "    # article in the list, the set of articles within tolerance, and the key words for each\n",
    "    if args==None:\n",
    "        articleList=reportArticleList\n",
    "        fileName=storyMapFileName\n",
    "    else:\n",
    "        articleList=args['article_id_list']\n",
    "        fileName=args['story_map_validation']\n",
    "    \n",
    "    reportArticleList=articleList\n",
    "    if fileName!=None:\n",
    "        storyMap=readStoryMapFromFile(fileName)\n",
    "        if reportArticleList==None:\n",
    "            reportArticleList=[]\n",
    "            for story, articleList in storyMap.items():\n",
    "                reportArticleList.append(articleList[0])\n",
    "    else:\n",
    "        storyMap=None\n",
    "    return storyMap,reportArticleList\n",
    "\n",
    "def readStoryMapFromFile(filename):\n",
    "    return readDictFromCsvFile(filename,'StoryMap')\n",
    "\n",
    "def readGridParameterRangeFromFile(filename):\n",
    "    return readDictFromCsvFile(filename, 'GridParameters')\n",
    "\n",
    "def readDictFromCsvFile(filename,schema):\n",
    "    gridParamDict={} \n",
    "    with open(filename,'r') as f:\n",
    "        for row in f:\n",
    "            row=row[:-1] # Exclude the carriage return\n",
    "            row=row.split(',')\n",
    "            key=row[0]\n",
    "            vals=row[1:]\n",
    "            \n",
    "            if schema=='GridParameters':\n",
    "                if key in ['story_threshold','tfidf_maxdf']:\n",
    "                    finalVals=list(float(n) for n in vals)\n",
    "                elif key in ['ngram_max','tfidf_mindf','max_length']:\n",
    "                    finalVals=list(int(n) for n in vals)\n",
    "                elif key in ['lemma_conversion','tfidf_binary']:\n",
    "                    finalVals = list(str2bool(n) for n in vals)\n",
    "                elif key in ['parts_of_speech']:\n",
    "                    listlist=[]\n",
    "                    for v in vals:\n",
    "                        listlist.append(v_split('+'))\n",
    "                    finalVals = listlist\n",
    "                elif key in ['tfidf_norm','nlp_library']:\n",
    "                    finalVals=vals\n",
    "                else:\n",
    "                    print(key)\n",
    "                    print(\"KEY ERROR\")\n",
    "                    return\n",
    "            elif schema == 'StoryMap':\n",
    "                finalVals = list(int(n) for n in vals)\n",
    "            else:\n",
    "                print(schema)\n",
    "                print('SCHEMA ERROR')\n",
    "                return\n",
    "            \n",
    "            gridParamDict[key]=finalVals\n",
    "    return gridParamDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "storyMap,reportArticleList=setupStoryMapAndReportList(storyMapFileName='storyMapForValidation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_df['input to vectorizer'] = article_df['content no nonascii'].map(lambda x: stringNLTKProcess(nl, x,partsOfSpeech,stop_words,maxWords=None,lemmatizer=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessAndVectorize(articleDataFrame, args, pos_nlp_mapping, nlp,nl, wordnet_lemmatizer,stop_words):\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    vectorizer = TfidfVectorizer(analyzer='word',\n",
    "                                ngram_range=(1,args['ngram_max'][0]),\n",
    "                                lowercase=True,\n",
    "                                binary=args['tfidf_binary'][0],\n",
    "                                **optArgsForVectorizer)\n",
    "    tfidVectors=vectorizer.fit_transform(articleDataFrame['input to vectorizer'])\n",
    "    terms=vectorizer.get_feature_names()\n",
    "    return tfidVectors, terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialiseAllNonZeroCoords(tfidVectors):\n",
    "    #This function just exists isnce it seems to be expensive and I'd rather not call it multiple times\n",
    "    #Hence it is intended to be called outside of loops in order to simplify the row specific processing\n",
    "    values=[]\n",
    "    nzc=zip(*tfidVectors.nonzero())\n",
    "    \n",
    "    #In Python 3 the zip can only be iterated through one time before it is automatically realeased\n",
    "    ## So need to copy the results otherwise the main loop below will no longer work\n",
    "    pointList=[]\n",
    "    for i,j in nzc:\n",
    "        pointList.append([i,j])\n",
    "        \n",
    "    for row in range(tfidVectors.shape[0]):\n",
    "        rowList=[]\n",
    "        for i,j in pointList:\n",
    "            if row==i:\n",
    "                rowList.append(j)\n",
    "        values.append(rowList)\n",
    "        \n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoreCurrentParamGuess(tfidVectors,storyMap,articleDataFrame,threshold,printErrors=False):\n",
    "    #Work with distances relative to first item in each cluster - even though this is clearly arbitrary since that\n",
    "    #point could be an outlier in the cluster and hence might cause some problems.\n",
    "    #But I have to start somewhere - and can refine it later if needed.\n",
    "    \n",
    "    nonZeroCoords=initialiseAllNonZeroCoords(tfidVectors)\n",
    "    score=0\n",
    "    outGood=0\n",
    "    outBad=0\n",
    "    inGood=0\n",
    "    inBad=0\n",
    "    for story, storyArticles in storyMap.items():\n",
    "        leadArticleIndex=articleDataFrame[articleDataFrame['id']==storyArticles[0]].index[0]\n",
    "        #Compute score of all articles in corpus relative to the first article in the story ( dot product)\n",
    "        #Then count through the list relative to the threshold (add one for a good result, subtract one for a bad result)\n",
    "        scores=productRelatednessScores(tfidfVectors,nonZeroCoords,leadArticleIndex)\n",
    "        rankedIndices=np.argsort(scores)\n",
    "        foundRelatedArticles=[]\n",
    "        # The sorting here is not strictly required,ubt i could use it so that once the threshold is passed\n",
    "        # in the loop, then i infer the remaining results\n",
    "        for article in reversed(rankedIndices):\n",
    "            thisArticleIndex=articleDataFrame['id'][article]\n",
    "            if thisArticleIndex in storyArticles:\n",
    "                if scores[article]>=threshold:\n",
    "                    score+=1\n",
    "                    inGood+=1\n",
    "                else:\n",
    "                    score-=1\n",
    "                    inBad+=1\n",
    "                    if printErrors:\n",
    "                        print(\"ERROR:\",thisArticleIndex,\"should be in\", story)\n",
    "            else: # article not supposed to be in range\n",
    "                if scores[article]<=threshold:\n",
    "                    score+=1\n",
    "                    outGood+=1\n",
    "                else:\n",
    "                    score-=1\n",
    "                    outBad+=1\n",
    "                    if printErrors:\n",
    "                        print(\"ERROR:\", thisArticleIndex,\"should NOT be in\", story)\n",
    "        scoreDict={'score':score, 'inGood': inGood, 'inBad': inBad, \"outGood\": outGood, 'outBad': outBad}\n",
    "        return scoreDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "483"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_df[article_df['id']==storyArticles[0]].index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trump meeting [151832, 110126, 172078, 48306, 57365, 190512, 26536, 71335, 21499, 23872, 142033, 110133, 23888, 71336, 57366, 71339]\n",
      "Brazil impeachment [120639, 80103, 25225, 21502, 57362, 120636, 110141]\n",
      "Kaepernick [40617, 40543, 39520, 80109, 80101, 47403]\n",
      "Clinton Guccifer [214888, 85803, 47979]\n",
      "Farage [37252, 37468, 46175]\n",
      "Anthony Weiner [49480, 110144, 142300, 214934]\n",
      "SpaceX [38658, 134545, 172095, 214894]\n",
      "Safe space [21448, 78169, 78171]\n",
      "Lauer debate [43447, 47078, 138709]\n",
      "Venezuela [172079, 57375, 190522]\n",
      "Iran deal [158005, 48823, 57373, 120634]\n",
      "Penn State [80094, 157527, 214892]\n",
      "David Brown [172085, 80096, 141886]\n"
     ]
    }
   ],
   "source": [
    "for story, storyArticles in storyMap.items():\n",
    "    print(story, storyArticles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def productRelatednessScores(tfidVectors,nonZeroCoords,refRow):\n",
    "    # instantiates a matrix of zeros with tfidVectors.shape[0] rows corresponding to the number of rows in the tfidVectors array\n",
    "    scores = [0]*tfidVectors.shape[0]\n",
    "    for toRow in range(tfidVectors.shape[0]):\n",
    "        scores[toRow] = sum([(tfidVectors[toRow,w]*tfidVectors[refRow,w]) for w in nonZeroCoords[refRow] if w in nonZeroCoords[toRow]])\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parameterGrid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-8aee55dbb36c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# If not doing grid search, will just pass through the loop once\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mbestParamScoreDict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'score'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1000000\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mbetsParams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparameterGrid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcurrentParams\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparameterGrid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'parameterGrid' is not defined"
     ]
    }
   ],
   "source": [
    "# Loop across all parameter combinations in grid to determine best set\n",
    "# If not doing grid search, will just pass through the loop once\n",
    "bestParamScoreDict={'score':-1000000}\n",
    "betsParams=parameterGrid[0]\n",
    "\n",
    "for i,currentParams in enumerate(parameterGrid):\n",
    "    if len(parameterGrid)>1:\n",
    "        print(\"Combination:\", i+1, \"of\", len(parameterGrid))\n",
    "        print(currentParams)\n",
    "        \n",
    "    # Determine tf-idf vectors\n",
    "    # terms is just used later on if analysis of final results is requested\n",
    "    \n",
    "    tfidVectors, terms=preprocessAndVectorize(articleDataFrame,\n",
    "                                             currentParams,\n",
    "                                             pos_nlp_mapping,\n",
    "                                             nlp,\n",
    "                                             nl,\n",
    "                                             wordnet_lemmatizer,\n",
    "                                             stop_words)\n",
    "    \n",
    "    # Computes scores if threshold provided (meaning as part of grid search)\n",
    "    if 'story_threshold' in currentParams and currentParams['story_threshold']!=None:\n",
    "        scoreDict=scoreCurrentParamGuess(tfidVectors,storyMap,articleDataFrame,currentParams['story_threshold'])\n",
    "        print(scoreDict)\n",
    "        \n",
    "        #Update best so far\n",
    "        if scoreDict['score']>=bestParamScoreDict['score']:\n",
    "            if len(parameterGrid)>1:\n",
    "                print(i+1,\"is the best so far!\")\n",
    "            bestParams = currentParams\n",
    "            bestParamScoreDict = scoreDict\n",
    "    # End grid/parameter loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nlp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-f3b6ce3b015f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnlp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'nlp' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing scoring of TF-IDF vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the TF-IDF vector from article_df\n",
    "optArgsForVectorizer = {}\n",
    "tfidfVectors, terms = preprocessAndVectorize(article_df, runParams, pos_nlp_mapping,None,nl,wordnet_lemmatizer,stop_words,**optArgsForVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonZeroCoords=initialiseAllNonZeroCoords(tfidfVectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "404"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nonZeroCoords[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(663, 70425)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nzc=list(zip(*tfidfVectors.nonzero()))\n",
    "nzc[250194]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(664, 188293)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidfVectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trump meeting [151832, 110126, 172078, 48306, 57365, 190512, 26536, 71335, 21499, 23872, 142033, 110133, 23888, 71336, 57366, 71339]\n",
      "Brazil impeachment [120639, 80103, 25225, 21502, 57362, 120636, 110141]\n",
      "Kaepernick [40617, 40543, 39520, 80109, 80101, 47403]\n",
      "Clinton Guccifer [214888, 85803, 47979]\n",
      "Farage [37252, 37468, 46175]\n",
      "Anthony Weiner [49480, 110144, 142300, 214934]\n",
      "SpaceX [38658, 134545, 172095, 214894]\n",
      "Safe space [21448, 78169, 78171]\n",
      "Lauer debate [43447, 47078, 138709]\n",
      "Venezuela [172079, 57375, 190522]\n",
      "Iran deal [158005, 48823, 57373, 120634]\n",
      "Penn State [80094, 157527, 214892]\n",
      "David Brown [172085, 80096, 141886]\n"
     ]
    }
   ],
   "source": [
    "for story, storyArticles in storyMap.items():\n",
    "    leadArticleIndex=article_df[article_df['id']==storyArticles[0]].index[0]\n",
    "    print(leadArticleIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "483\n",
      "395\n",
      "94\n",
      "634\n",
      "68\n",
      "151\n",
      "75\n",
      "4\n",
      "111\n",
      "514\n",
      "510\n",
      "262\n",
      "520\n"
     ]
    }
   ],
   "source": [
    "for story, storyArticles in storyMap.items():\n",
    "    leadArticleIndex=article_df[article_df['id']==storyArticles[0]].index[0]\n",
    "    print(leadArticleIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in order to see the scores, we need to first get the reference rows given by the story map\n",
    "for story, storyArticles in storyMap.items():\n",
    "        leadArticleIndex=article_df[article_df['id']==storyArticles[0]].index[0]\n",
    "        #Compute score of all articles in corpus relative to the first article in the story ( dot product)\n",
    "        #Then count through the list relative to the threshold (add one for a good result, subtract one for a bad result)\n",
    "        scores=productRelatednessScores(tfidfVectors,nonZeroCoords,leadArticleIndex)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = productRelatednessScores(tfidfVectors,nonZeroCoords,483)\n",
    "scores[0]\n",
    "type(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.004660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000831</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0\n",
       "0  0.001967\n",
       "1  0.004660\n",
       "2  0.000832\n",
       "3  0.001971\n",
       "4  0.000831"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df = pd.DataFrame(scores)\n",
    "scores_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "560\n",
      "59438\n",
      "1.0 0.1073909348712558 0.09292303950260095\n"
     ]
    }
   ],
   "source": [
    "#returns the index of each row for the values sorted least to greatest\n",
    "rankedIndices=np.argsort(scores)\n",
    "rankedIndices\n",
    "print(rankedIndices[-3])\n",
    "print(article_df['id'][203])\n",
    "print(scores[483],scores[203], scores[560])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 647, 'inGood': 0, 'inBad': 3, 'outGood': 651, 'outBad': 1}"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for the first article, article 483 of the trump meeting, lets get all the articles which were matching it (in the same cluster)\n",
    "score=0\n",
    "outGood=0\n",
    "outBad=0\n",
    "inGood=0\n",
    "inBad=0\n",
    "threshold=0.25\n",
    "printErrors=None\n",
    "\n",
    "scores = productRelatednessScores(tfidfVectors,nonZeroCoords,483)\n",
    "rankedIndices=np.argsort(scores)\n",
    "for article in reversed(rankedIndices):\n",
    "    try:\n",
    "        thisArticleIndex=article_df['id'][article]\n",
    "        if thisArticleIndex in storyArticles:\n",
    "            if scores[article]>=threshold:\n",
    "                score+=1\n",
    "                inGood+=1\n",
    "            else:\n",
    "                score-=1\n",
    "                inBad+=1\n",
    "                if printErrors:\n",
    "                    print(\"ERROR:\",thisArticleIndex,\"should be in\", story)\n",
    "        else: # article not supposed to be in range\n",
    "            if scores[article]<=threshold:\n",
    "                score+=1\n",
    "                outGood+=1\n",
    "            else:\n",
    "                score-=1\n",
    "                outBad+=1\n",
    "                if printErrors:\n",
    "                    print(\"ERROR:\", thisArticleIndex,\"should NOT be in\", story)\n",
    "    except:\n",
    "        pass\n",
    "scoreDict={'score':score, 'inGood': inGood, 'inBad': inBad, \"outGood\": outGood, 'outBad': outBad}\n",
    "scoreDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22197"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_df['id'][21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5     21454\n",
       "6     21455\n",
       "7     21474\n",
       "8     21485\n",
       "9     21489\n",
       "10    21499\n",
       "11    21500\n",
       "12    21502\n",
       "13    21504\n",
       "14    21506\n",
       "15    21508\n",
       "16    21509\n",
       "17    21510\n",
       "18    21511\n",
       "19    21512\n",
       "21    22197\n",
       "22    22464\n",
       "23    22505\n",
       "24    22573\n",
       "25    22578\n",
       "26    22598\n",
       "27    22613\n",
       "28    22918\n",
       "29    23315\n",
       "30    23346\n",
       "Name: id, dtype: int64"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_df['id'][5:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Trump meeting', '151832', '110126', '172078', '48306', '57365', '190512', '26536', '71335', '21499', '23872', '142033', '110133', '23888', '71336', '57366', '71339']\n",
      "['Brazil impeachment', '120639', '80103', '25225', '21502', '57362', '120636', '110141', '', '', '', '', '', '', '', '', '']\n",
      "['Kaepernick', '40617', '40543', '39520', '80109', '80101', '47403', '', '', '', '', '', '', '', '', '', '']\n",
      "['Clinton Guccifer', '214888', '85803', '47979', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['Farage', '37252', '37468', '46175', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['Anthony Weiner', '49480', '110144', '142300', '214934', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['SpaceX', '38658', '134545', '172095', '214894', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['Safe space', '21448', '78169', '78171', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['Lauer debate', '43447', '47078', '138709', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['Venezuela', '172079', '57375', '190522', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['Iran deal', '158005', '48823', '57373', '120634', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['Penn State', '80094', '157527', '214892', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['David Brown', '172085', '80096', '141886', '', '', '', '', '', '', '', '', '', '', '', '', '']\n"
     ]
    }
   ],
   "source": [
    "with open(r'C:\\Users\\goldm\\Capstone\\storyMapForValidation.csv', 'r') as f:\n",
    "    for row in f:\n",
    "        row=row[:-1] # Exclude the carriage return\n",
    "        row=row.split(\",\")\n",
    "        key=row[0]\n",
    "        vals=row[1:]\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['172085', '80096', '141886', '', '', '', '', '', '', '', '', '', '', '', '', '']\n"
     ]
    }
   ],
   "source": [
    "print(vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[172085, 80096, 141886]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[int(n) for n in vals if n!='']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-40-d38fb8e5e0bb>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-40-d38fb8e5e0bb>\"\u001b[1;36m, line \u001b[1;32m7\u001b[0m\n\u001b[1;33m    df.drop(index=[11:], inplace=True)\u001b[0m\n\u001b[1;37m                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\goldm\\Capstone\\data\\articles_expanded_7.10.21.csv')\n",
    "df.drop_duplicates('content')\n",
    "df = df[~df['content'].isnull()]\n",
    "df=df[df['content'].str.len()>=200]\n",
    "list_cols = df.columns\n",
    "list_cols[9:12]\n",
    "df.drop(index=[11:], inplace=True)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Article counts by publisher:\n",
      "Breitbart         104\n",
      "NY Post            61\n",
      "Reuters            60\n",
      "CNN                58\n",
      "NPR                54\n",
      "                 ... \n",
      "Newsweek            1\n",
      "HowStuffWorks       1\n",
      "@ChinaDailyApp      1\n",
      "T-Roy               1\n",
      "POLITICO            1\n",
      "Name: publication, Length: 63, dtype: int64\n",
      "\n",
      "Article counts by date:\n",
      "9/1/2016     434\n",
      "12/2/2016    349\n",
      "Name: date, dtype: int64\n",
      "\n",
      "Final dataset:\n",
      "\n",
      "Date: 2016-09-01 \n",
      "\n",
      "Series([], Name: publication, dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "filename= r'C:\\Users\\goldm\\Capstone\\data\\articles_expanded_7.10.21.csv'\n",
    "df = pd.read_csv(filename)\n",
    "df.drop_duplicates('content')\n",
    "df = df[~df['content'].isnull()]\n",
    "df=df[df['content'].str.len()>=200]\n",
    "\n",
    "targetString=\"(Want to get this briefing by email?\"\n",
    "df['NYT summary']=df['content'].map(lambda d: d[:len(targetString)]==targetString)\n",
    "df=df[df['NYT summary']==False]\n",
    "\n",
    "# The following removes a warning that appears in many of the Atlantic articles.\n",
    "# Since it is commonly at the beginning, it brings a lot of noise to the search for similar articles\n",
    "# And subsequently to the assessment of sentiment\n",
    "targetString=\"For us to continue writing great stories, we need to display ads.             Please select the extension that is blocking ads.     Please follow the steps below\"\n",
    "df['content']=df['content'].str.replace(targetString,'')\n",
    "\n",
    "# This is also for some Atlantic articles for the same reasons as above\n",
    "targetString=\"This article is part of a feature we also send out via email as The Atlantic Daily, a newsletter with stories, ideas, and images from The Atlantic, written specially for subscribers. To sign up, please enter your email address in the field provided here.\"\n",
    "df=df[df['content'].str.contains(targetString)==False]\n",
    "\n",
    "# This is also for some Atlantic articles for the same reasons as above\n",
    "targetString=\"This article is part of a feature we also send out via email as Politics  Policy Daily, a daily roundup of events and ideas in American politics written specially for newsletter subscribers. To sign up, please enter your email address in the field provided here.\"\n",
    "df=df[df['content'].str.contains(targetString)==False]\n",
    "\n",
    "# More Atlantic-specific removals (for daily summaries with multiple stories contained)\n",
    "df=df[df['content'].str.contains(\"To sign up, please enter your email address in the field\")==False]\n",
    "\n",
    "# Remove daily CNN summary\n",
    "targetString=\"CNN Student News\"\n",
    "df=df[df['content'].str.contains(targetString)==False]\n",
    "\n",
    "\n",
    "print(\"\\nArticle counts by publisher:\")\n",
    "print(df['publication'].value_counts())\n",
    "\n",
    "print(\"\\nArticle counts by date:\")\n",
    "print(df['date'].value_counts())\n",
    "\n",
    "#     Restrict to articles on the provided input date.\n",
    "#     This date is considered mandatory for topic clustering but is not required for sentiment\n",
    "#     since sentiment only processes a specified list of articles.\n",
    "#     For topic clustering it is essential to have the date as it is\n",
    "#     enormously significant in article matching.\n",
    "\n",
    "if processDate!=None:\n",
    "    df=df[df['date']==processDate]\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Remove non-ASCII characters\n",
    "df['content no nonascii']=df['content'].map(lambda x: removeNonASCIICharacters(x))\n",
    "\n",
    "print(\"\\nFinal dataset:\\n\\nDate:\",processDate,\"\\n\")\n",
    "print(df['publication'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-09-01\n"
     ]
    }
   ],
   "source": [
    "print(processDate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "False",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2645\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2646\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2647\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: False",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-3b74b4591c7f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mprocessDate\u001b[0m\u001b[1;33m!=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0mdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'date'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mprocessDate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2798\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2799\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2800\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2801\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2802\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2646\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2647\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2648\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2649\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2650\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: False"
     ]
    }
   ],
   "source": [
    "filename= r'C:\\Users\\goldm\\Capstone\\data\\articles_expanded_7.10.21.csv'\n",
    "df = pd.read_csv(filename)\n",
    "df.drop_duplicates('content')\n",
    "df = df[~df['content'].isnull()]\n",
    "df=df[df['content'].str.len()>=200]\n",
    "targetString=\"(Want to get this briefing by email?\"\n",
    "df['NYT summary']=df['content'].map(lambda d: d[:len(targetString)]==targetString)\n",
    "df=df[df['NYT summary']==False]\n",
    "targetString=\"For us to continue writing great stories, we need to display ads.             Please select the extension that is blocking ads.     Please follow the steps below\"\n",
    "df['content']=df['content'].str.replace(targetString,'')\n",
    "\n",
    "# This is also for some Atlantic articles for the same reasons as above\n",
    "targetString=\"This article is part of a feature we also send out via email as The Atlantic Daily, a newsletter with stories, ideas, and images from The Atlantic, written specially for subscribers. To sign up, please enter your email address in the field provided here.\"\n",
    "df=df[df['content'].str.contains(targetString)==False]\n",
    "\n",
    "# This is also for some Atlantic articles for the same reasons as above\n",
    "targetString=\"This article is part of a feature we also send out via email as Politics  Policy Daily, a daily roundup of events and ideas in American politics written specially for newsletter subscribers. To sign up, please enter your email address in the field provided here.\"\n",
    "df=df[df['content'].str.contains(targetString)==False]\n",
    "\n",
    "# More Atlantic-specific removals (for daily summaries with multiple stories contained)\n",
    "df=df[df['content'].str.contains(\"To sign up, please enter your email address in the field\")==False]\n",
    "\n",
    "# Remove daily CNN summary\n",
    "targetString=\"CNN Student News\"\n",
    "df=df[df['content'].str.contains(targetString)==False]\n",
    "\n",
    "if processDate!=None:\n",
    "    df=df[df['date']==processDate]\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2016-09-01'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processDate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'./data/articlesv2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 811 entries, 0 to 810\n",
      "Columns: 362 entries, Unnamed: 0 to Unnamed: 361\n",
      "dtypes: float64(1), object(361)\n",
      "memory usage: 2.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'id', 'title', 'publication', 'author', 'date', 'year',\n",
       "       'month', 'url', 'content',\n",
       "       ...\n",
       "       'Unnamed: 352', 'Unnamed: 353', 'Unnamed: 354', 'Unnamed: 355',\n",
       "       'Unnamed: 356', 'Unnamed: 357', 'Unnamed: 358', 'Unnamed: 359',\n",
       "       'Unnamed: 360', 'Unnamed: 361'],\n",
       "      dtype='object', length=362)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['id', 'title', 'publication', 'author', 'date', 'year',\n",
    "       'month', 'url', 'content']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: date, dtype: object)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: date, dtype: object)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processDate = '2016-09-01'\n",
    "if processDate!=None:\n",
    "    df=df[df['date']==processDate]\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "df['date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_articles_df = pd.read_csv(r'./data/articles.csv')\n",
    "new_articles_df = pd.read_csv(r\"C:\\Users\\goldm\\News API\\combinedTopics_1-5_import_ready.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'id', 'title', 'publication', 'author', 'date', 'year',\n",
       "       'month', 'url', 'content'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_articles_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'time', 'article.text', 'article.author', 'article.pub_date',\n",
       "       'article.is_article', 'article.url', 'article.title',\n",
       "       'article.language', 'article.summary', 'article.modified_date',\n",
       "       'article.site_name', 'article.encoding', 'Topic Classification', 'url'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_articles_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>publication</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>217417</td>\n",
       "      <td>Global News</td>\n",
       "      <td>7/8/2021 14:23</td>\n",
       "      <td>Haiti in state of emergency after President Jo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>217418</td>\n",
       "      <td>@ChinaDailyApp</td>\n",
       "      <td>1/4/2021 11:00</td>\n",
       "      <td>MEXICO CITY - Haitian police have arrested two...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>217419</td>\n",
       "      <td>Reuters</td>\n",
       "      <td>7/7/2021 13:43</td>\n",
       "      <td>Haiti's President Jovenel Moise speaks during ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>217420</td>\n",
       "      <td>The White House</td>\n",
       "      <td>7/7/2021 14:53</td>\n",
       "      <td>We are shocked and saddened to hear of the hor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>217421</td>\n",
       "      <td>nypost.com</td>\n",
       "      <td>7/9/2021</td>\n",
       "      <td>1  of\\n\\nThe late Jovenel Moise, whom was kill...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id      publication            date  \\\n",
       "0  217417      Global News  7/8/2021 14:23   \n",
       "1  217418   @ChinaDailyApp  1/4/2021 11:00   \n",
       "2  217419          Reuters  7/7/2021 13:43   \n",
       "3  217420  The White House  7/7/2021 14:53   \n",
       "4  217421       nypost.com        7/9/2021   \n",
       "\n",
       "                                             content  \n",
       "0  Haiti in state of emergency after President Jo...  \n",
       "1  MEXICO CITY - Haitian police have arrested two...  \n",
       "2  Haiti's President Jovenel Moise speaks during ...  \n",
       "3  We are shocked and saddened to hear of the hor...  \n",
       "4  1  of\\n\\nThe late Jovenel Moise, whom was kill...  "
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_articles_df_shortened = new_articles_df[['id','article.site_name', 'article.pub_date', 'article.text']]\n",
    "new_articles_df_shortened.columns = ['id','publication','date','content']\n",
    "new_articles_df_shortened.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-236-c6fec7e3cf40>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_articles_df_shortened['date'] = pd.DataFrame(['2016-09-01' for date in range(0,new_articles_df_shortened.shape[0])])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0      2016-09-01\n",
       "1      2016-09-01\n",
       "2      2016-09-01\n",
       "3      2016-09-01\n",
       "4      2016-09-01\n",
       "          ...    \n",
       "117    2016-09-01\n",
       "118    2016-09-01\n",
       "119    2016-09-01\n",
       "120    2016-09-01\n",
       "121    2016-09-01\n",
       "Name: date, Length: 122, dtype: object"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#changing the date to be the exact same as the '2016-09-01'\n",
    "new_articles_df_shortened['date'] = pd.DataFrame(['2016-09-01' for date in range(0,new_articles_df_shortened.shape[0])])\n",
    "new_articles_df_shortened['date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_articles_df_shortened = orig_articles_df[['id','publication','date','content']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = orig_articles_df_shortened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'publication', 'date', 'content'], dtype='object')"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df = combined_df.append(new_articles_df_shortened)\n",
    "combined_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>publication</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20694</td>\n",
       "      <td>NY Times</td>\n",
       "      <td>2016-12-02</td>\n",
       "      <td>If you've seen one movie apocalypse, you have ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20875</td>\n",
       "      <td>NY Times</td>\n",
       "      <td>2016-12-02</td>\n",
       "      <td>There was the breadcrumb dropped on Valentine'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20986</td>\n",
       "      <td>NY Times</td>\n",
       "      <td>2016-12-02</td>\n",
       "      <td>CLEVELAND     This was supposed to be Fox New...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21413</td>\n",
       "      <td>NY Times</td>\n",
       "      <td>2016-09-01</td>\n",
       "      <td>Another Earth could be circling the star right...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21448</td>\n",
       "      <td>NY Times</td>\n",
       "      <td>2016-09-01</td>\n",
       "      <td>The anodyne welcome letter to incoming freshme...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id publication        date  \\\n",
       "0  20694    NY Times  2016-12-02   \n",
       "1  20875    NY Times  2016-12-02   \n",
       "2  20986    NY Times  2016-12-02   \n",
       "3  21413    NY Times  2016-09-01   \n",
       "4  21448    NY Times  2016-09-01   \n",
       "\n",
       "                                             content  \n",
       "0  If you've seen one movie apocalypse, you have ...  \n",
       "1  There was the breadcrumb dropped on Valentine'...  \n",
       "2  CLEVELAND     This was supposed to be Fox New...  \n",
       "3  Another Earth could be circling the star right...  \n",
       "4  The anodyne welcome letter to incoming freshme...  "
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "795"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "795"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(orig_articles_df) + len(new_articles_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_articles_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "673"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(orig_articles_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>publication</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>217535</td>\n",
       "      <td>Business Insider</td>\n",
       "      <td>2016-09-01</td>\n",
       "      <td>-  Adam Kinzinger told NYT Magazine that he su...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id       publication        date  \\\n",
       "118  217535  Business Insider  2016-09-01   \n",
       "\n",
       "                                               content  \n",
       "118  -  Adam Kinzinger told NYT Magazine that he su...  "
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df[combined_df['id']==217535]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      2016-12-02\n",
       "1      2016-12-02\n",
       "2      2016-12-02\n",
       "3      2016-09-01\n",
       "4      2016-09-01\n",
       "          ...    \n",
       "117    2016-09-01\n",
       "118    2016-09-01\n",
       "119    2016-09-01\n",
       "120    2016-09-01\n",
       "121    2016-09-01\n",
       "Name: date, Length: 795, dtype: object"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df['date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "430"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(combined_df)\n",
    "processDate = '2016-09-01'\n",
    "if processDate!=None:\n",
    "    df=df[df['date']==processDate]\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "len(df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "430"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export the combined articles file to csv\n",
    "combined_df.to_csv(r'C:\\Users\\goldm\\Capstone\\data\\articles_combined_formattedv1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setupStoryMapAndReportList(args=None, reportArticleList=None,storyMapFileName=None):\n",
    "    # Story Map is used in fitting if grid search is applied (As ground truth)\n",
    "    # It is also used in graph if no threshold provided (to determine colours, not to determine location)\n",
    "    # Report Article List is used at the end to create a report with, for each\n",
    "    # article in the list, the set of articles within tolerance, and the key words for each\n",
    "    if args==None:\n",
    "        articleList=reportArticleList\n",
    "        fileName=storyMapFileName\n",
    "    else:\n",
    "        articleList=args['article_id_list']\n",
    "        fileName=args['story_map_validation']\n",
    "    \n",
    "    reportArticleList=articleList\n",
    "    if fileName!=None:\n",
    "        storyMap=readStoryMapFromFile(fileName)\n",
    "        if reportArticleList==None:\n",
    "            reportArticleList=[]\n",
    "            for story, articleList in storyMap.items():\n",
    "                reportArticleList.append(articleList[0])\n",
    "    else:\n",
    "        storyMap=None\n",
    "    return storyMap,reportArticleList\n",
    "\n",
    "def readStoryMapFromFile(filename):\n",
    "    return readDictFromCsvFile(filename,'StoryMap')\n",
    "\n",
    "def readGridParameterRangeFromFile(filename):\n",
    "    return readDictFromCsvFile(filename, 'GridParameters')\n",
    "\n",
    "def readDictFromCsvFile(filename,schema):\n",
    "    gridParamDict={} \n",
    "    with open(filename,'r') as f:\n",
    "        for row in f:\n",
    "            row=row[:-1] # Exclude the carriage return\n",
    "            row=row.split(',')\n",
    "            key=row[0]\n",
    "            vals=row[1:]\n",
    "            \n",
    "            if schema=='GridParameters':\n",
    "                if key in ['story_threshold','tfidf_maxdf']:\n",
    "                    finalVals=list(float(n) for n in vals)\n",
    "                elif key in ['ngram_max','tfidf_mindf','max_length']:\n",
    "                    finalVals=list(int(n) for n in vals)\n",
    "                elif key in ['lemma_conversion','tfidf_binary']:\n",
    "                    finalVals = list(str2bool(n) for n in vals)\n",
    "                elif key in ['parts_of_speech']:\n",
    "                    listlist=[]\n",
    "                    for v in vals:\n",
    "                        listlist.append(v_split('+'))\n",
    "                    finalVals = listlist\n",
    "                elif key in ['tfidf_norm','nlp_library']:\n",
    "                    finalVals=vals\n",
    "                else:\n",
    "                    print(key)\n",
    "                    print(\"KEY ERROR\")\n",
    "                    return\n",
    "            elif schema == 'StoryMap':\n",
    "                finalVals = list(int(n) for n in vals if n!='')\n",
    "            else:\n",
    "                print(schema)\n",
    "                print('SCHEMA ERROR')\n",
    "                return\n",
    "            \n",
    "            gridParamDict[key]=finalVals\n",
    "    return gridParamDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'story_map_for_validation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-229-fcfaa1515f2c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# getting the information to make the story validation map\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mstory_map_for_validation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'story_map_for_validation' is not defined"
     ]
    }
   ],
   "source": [
    "# getting the information to make the story validation map\n",
    "story_map_for_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(795, 4)"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>publication</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>217490</td>\n",
       "      <td>Hosted</td>\n",
       "      <td>2016-09-01</td>\n",
       "      <td>TOPICS\\n, , , -  Economy  ,\\n-  Coronavirus  ,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id publication        date  \\\n",
       "73  217490      Hosted  2016-09-01   \n",
       "\n",
       "                                              content  \n",
       "73  TOPICS\\n, , , -  Economy  ,\\n-  Coronavirus  ,...  "
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df[combined_df['id']== 217490]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_articles_df_classifications = new_articles_df[['id','Topic Classification']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_articles_df_classifications.to_csv(r'new_article_classificationsv1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "storyMap,reportArticleList=setupStoryMapAndReportList(storyMapFileName='storyMapForValidation_expanded.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for story, storyArticles in storyMap.items():\n",
    "    leadArticleIndex=articleDataFrame[articleDataFrame['id']==storyArticles[0]].index[0]\n",
    "    # Compute score of all articles in corpus relative to first article in story (.product)\n",
    "    # Then count through list relative to threshold (add one for a good result, subtract one for a bad result)\n",
    "    scores=productRelatednessScores(tfidfVectors,nonZeroCoords,leadArticleIndex)\n",
    "    rankedIndices=np.argsort(scores)\n",
    "    foundRelatedArticles=[]\n",
    "    # THE SORTING HERE IS NOT STRICTLY REQUIRED, BUT I COULD USE IT SO THAT ONCE THE THRESHOLD IS PASSED\n",
    "    # IN THE LOOP, THEN I INFER THE REMAINING RESULTS\n",
    "    for article in reversed(rankedIndices):\n",
    "        thisArticleIndex=articleDataFrame['id'][article]\n",
    "        if thisArticleIndex in storyArticles:\n",
    "            if scores[article]>=threshold: # article IS supposed to be in range\n",
    "                score+=1\n",
    "                inGood+=1\n",
    "                #appending the article and its mapping according to the predictions of our model\n",
    "                final_mapping_list.append([thisArticleIndex, story, 'TP'])\n",
    "            else:\n",
    "                score-=1\n",
    "                inBad+=1\n",
    "                final_mapping_list.append([thisArticleIndex, 'No Mapping', 'FN'])\n",
    "                if printErrors:\n",
    "                    print(\"ERROR:\",thisArticleIndex,\"should be in\",story)\n",
    "        else: # article not supposed to be in range\n",
    "            if scores[article]<=threshold:\n",
    "                score+=1\n",
    "                outGood+=1\n",
    "                final_mapping_list.append([thisArticleIndex, 'No Mapping', 'TN'])\n",
    "            else:\n",
    "                score-=1\n",
    "                outBad+=1\n",
    "                final_mapping_list.append([thisArticleIndex, story, 'FP'])\n",
    "                if printErrors:\n",
    "                    print(\"ERROR:\",thisArticleIndex,\"should NOT be in\",story)\n",
    "\n",
    "#### ---- Richard Modification- adding in code to print out a df of articles in the story map and their respective categor\n",
    "final_mapping_df = pd.DataFrame(final_mapping_list, columns = ['article', 'cateogry ID', 'FP/FN/TP/TN'])\n",
    "final_mapping_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
