{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part One - Clustering news articles according to inferred shared stories\n",
    "There are two parts to this project. The first part deals with takes a body of news articles and clustering them into stories. That part is covered in this workbook.\n",
    "The second part takes the results of the clustering (i.e. a list of articles pertaining to a single story) and ranks them in order to determine if the set of articles correspond to a balanced representation of the story, or a biased representation. This second part is covered in a second notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To clarify some terms:\n",
    "- ARTICLE - a single article printed by one news publication\n",
    "- STORY - the underlying event that an article is in reference to\n",
    "\n",
    "In many instances the universe of publications will feature multiple articles on any one story. The questions were are ultimately seeking to address here are:\n",
    "- FURTHER READING - given that a user has read one article, which other articles should the user read in order to not have a biased perspective on the underlying event?\n",
    "- FAIRNESS - is the complete set of published articles on that story biased or fair?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This workbook contains code required to:\n",
    "- load a large corpus of new articles\n",
    "- process them using a series of NLP methods\n",
    "- group the articles into inferred stories\n",
    "- analyse and graph the results\n",
    "- perform a grid search if required in order to optimise the hyper parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "### Imports\n",
    "The first step is to import some of the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter configuration\n",
    "The parameters used to control the NLP-related calculations, and to specify the domain for any grid search are captured in the runParams dict. This includes specification of the location of the key input files.\n",
    "The runParams dict is converted into an sklearn ParameterGrid, even if there is no grid search requirement (in which case it's processed as a single scenario grid search).\n",
    "\n",
    "NB All parameters need to be lists (or lists of lists) - requirement of ParameterGrid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "runParams={'tfidf_maxdf':      [0.5],\n",
    "           'input_file':       ['./data/articles.csv'],\n",
    "           'story_threshold':  [0.26],\n",
    "           'process_date':     ['2016-09-01'],\n",
    "           'parts_of_speech':  [['PROPER', 'VERB']],\n",
    "           'lemma_conversion': [False],\n",
    "           'ngram_max':        [3],\n",
    "           'tfidf_binary':     [False],\n",
    "           'tfidf_norm':       ['l2'],\n",
    "           'nlp_library':      ['nltk'],\n",
    "           'max_length':       [50],\n",
    "           'stop_words_file':  ['./data/stopWords.txt'],\n",
    "           'tfidf_mindf':      [2],\n",
    "           'display_graph':    [True],\n",
    "           'article_stats':    [False]}\n",
    "\n",
    "# Use parameter grid even if there is only set of parameters\n",
    "parameterGrid=ParameterGrid(runParams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Libraries\n",
    "Two NLP libraries are used in this worksheet - ntkl and spaCy. Depending on which is/are requested in the run parameters, the following section of code loads the relevant packages. In addition it initialises a dictionary to translate a common set of Parts of Speech into the corresponding set of tokens specific to each library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and initialise required NLP libraries\n",
    "pos_nlp_mapping={}\n",
    "nl=None\n",
    "wordnet_lemmatizer=None\n",
    "nlp=None\n",
    "if 'spaCy' in runParams['nlp_library']:\n",
    "\timport spacy\n",
    "\tnlp=spacy.load('en')\n",
    "\tpos_nlp_mapping['spaCy']={'VERB':['VERB'],'PROPER':['PROPN'],'COMMON':['NOUN']}\n",
    "\n",
    "if 'nltk' in runParams['nlp_library']:\n",
    "\timport nltk as nl\n",
    "\tif True in runParams['lemma_conversion']:\n",
    "\t\tfrom nltk.stem import WordNetLemmatizer\n",
    "\t\twordnet_lemmatizer=WordNetLemmatizer()\n",
    "\telse:\n",
    "\t\twordnet_lemmatizer=None\n",
    "\tpos_nlp_mapping['nltk']={'VERB':['VB','VBD','VBG','VBN','VBP','VBZ'],'PROPER':['NNP','NNPS'],'COMMON':['NN','NNS']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File loader for news article corpus\n",
    "The following function loads the text file specified in the run parameters and converts it into a Pandas data frame.\n",
    "It proceeds to perform some clean up on the data, effectively removing articles that are in some sense or other corrupt and will not be able to processed by the algorithm.\n",
    "This set includes summary articles which effectively contain a single sentence on a large number of stories. It also removes some standardised text common to certain articles, since that text contains no information about the story itself and hence will create noise in the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note on article dates\n",
    "The loader function below needs to be called with the selected articles to be constrained to a single date. This is because a key feature of \"news\" reporting is that it is current. As a result two articles are hugely more likely to pertain to the same story if they are published on the same day. Including the full set of dates has the effect of creating a lot of noise for the vectorizing and clustering, thus resulting in both slow performance and inaccurate results.\n",
    "There are additional techniques for pairing articles across different dates. These are discussed in the project report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInputDataAndDisplayStats(filename,processDate,printSummary=False):\n",
    "\n",
    "\tdf=pd.read_csv(filename)\n",
    "\tdf=df.drop_duplicates('content')\n",
    "\tdf=df[~df['content'].isnull()]\n",
    "\n",
    "\t# There are a large number of junk articles, many of which either don't make sense or\n",
    "\t# just contain a headline - as such they are useless for this analysis and may distort\n",
    "\t# results if left in place\n",
    "\tdf=df[df['content'].str.len()>=200]\n",
    "\n",
    "\t# Find and remove summary NYT \"briefing\" articles to avoid confusing the clustering\n",
    "\ttargetString=\"(Want to get this briefing by email?\"\n",
    "\tdf['NYT summary']=df['content'].map(lambda d: d[:len(targetString)]==targetString)\n",
    "\tdf=df[df['NYT summary']==False]\n",
    "\n",
    "\t# The following removes a warning that appears in many of the Atlantic articles.\n",
    "\t# Since it is commonly at the beginning, it brings a lot of noise to the search for similar articles\n",
    "\t# And subsequently to the assessment of sentiment\n",
    "\ttargetString=\"For us to continue writing great stories, we need to display ads.             Please select the extension that is blocking ads.     Please follow the steps below\"\n",
    "\tdf['content']=df['content'].str.replace(targetString,'')\n",
    "\n",
    "\t# This is also for some Atlantic articles for the same reasons as above\n",
    "\ttargetString=\"This article is part of a feature we also send out via email as The Atlantic Daily, a newsletter with stories, ideas, and images from The Atlantic, written specially for subscribers. To sign up, please enter your email address in the field provided here.\"\n",
    "\tdf=df[df['content'].str.contains(targetString)==False]\n",
    "\n",
    "\t# This is also for some Atlantic articles for the same reasons as above\n",
    "\ttargetString=\"This article is part of a feature we also send out via email as Politics  Policy Daily, a daily roundup of events and ideas in American politics written specially for newsletter subscribers. To sign up, please enter your email address in the field provided here.\"\n",
    "\tdf=df[df['content'].str.contains(targetString)==False]\n",
    "\n",
    "\t# More Atlantic-specific removals (for daily summaries with multiple stories contained)\n",
    "\tdf=df[df['content'].str.contains(\"To sign up, please enter your email address in the field\")==False]\n",
    "\n",
    "\t# Remove daily CNN summary\n",
    "\ttargetString=\"CNN Student News\"\n",
    "\tdf=df[df['content'].str.contains(targetString)==False]\n",
    "\n",
    "\tif printSummary:\n",
    "\t\tprint(\"\\nArticle counts by publisher:\")\n",
    "\t\tprint(df['publication'].value_counts())\n",
    "\n",
    "\t\tprint(\"\\nArticle counts by date:\")\n",
    "\t\tprint(df['date'].value_counts())\n",
    "\t\t\n",
    "\t# Restrict to articles on the provided input date.\n",
    "\t# This date is considered mandatory for topic clustering but is not required for sentiment\n",
    "\t# since sentiment only processes a specified list of articles.\n",
    "\t# For topic clustering it is essential to have the date as it is\n",
    "\t# enormously significant in article matching.\n",
    "\tif processDate!=None:\n",
    "\t\tdf=df[df['date']==processDate]\n",
    "\tdf.reset_index(inplace=True, drop=True)\n",
    "\n",
    "\t# Remove non-ASCII characters\n",
    "\tdf['content no nonascii']=df['content'].map(lambda x: removeNonASCIICharacters(x))\n",
    "\n",
    "\tprint(\"\\nFinal dataset:\\n\\nDate:\",processDate,\"\\n\")\n",
    "\tprint(df['publication'].value_counts())\n",
    "\n",
    "\treturn df\n",
    "\n",
    "##########################################################################################\n",
    "\n",
    "def removeNonASCIICharacters(textString): \n",
    "    return \"\".join(i for i in textString if ord(i)<128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the articles from the corpus\n",
    "In addition the function will return the number of articles per publication (for the requested run date). Here we see there is a relatively good mix of political viewpoints covered. More discussion of this is provided in the project report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final dataset:\n",
      "\n",
      "Date: 2016-09-01 \n",
      "\n",
      "Breitbart           50\n",
      "Buzzfeed News       35\n",
      "NY Times            28\n",
      "NY Post             27\n",
      "NPR                 26\n",
      "Atlantic            24\n",
      "Washington Post     22\n",
      "CNN                 20\n",
      "Reuters             15\n",
      "Business Insider    15\n",
      "Guardian            14\n",
      "Fox News            14\n",
      "National Review     12\n",
      "Name: publication, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load corpus of articles from file\n",
    "# 0 index is required because the parameters are forced to be lists by ParameterGrid\n",
    "articleDataFrame=getInputDataAndDisplayStats(runParams['input_file'][0],\n",
    "\t\t\t\t\t\t\t\t\t\t\t runParams['process_date'][0],\n",
    "\t\t\t\t\t\t\t\t\t\t\t runParams['article_stats'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect loaded articles\n",
    "Now that the articles are loaded, the only attributes that will be used are the ID and the 'content no non-ascii' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>publication</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "      <th>NYT summary</th>\n",
       "      <th>content no nonascii</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3722</td>\n",
       "      <td>21413</td>\n",
       "      <td>One Star Over, a Planet That Might Be Another ...</td>\n",
       "      <td>NY Times</td>\n",
       "      <td>Kenneth Chang</td>\n",
       "      <td>2016-09-01</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Another Earth could be circling the star right...</td>\n",
       "      <td>False</td>\n",
       "      <td>Another Earth could be circling the star right...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3748</td>\n",
       "      <td>21448</td>\n",
       "      <td>University of Chicago Strikes Back Against Cam...</td>\n",
       "      <td>NY Times</td>\n",
       "      <td>Richard Pérez-Peña, Mitch Smith and Stephanie ...</td>\n",
       "      <td>2016-09-01</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The anodyne welcome letter to incoming freshme...</td>\n",
       "      <td>False</td>\n",
       "      <td>The anodyne welcome letter to incoming freshme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3754</td>\n",
       "      <td>21454</td>\n",
       "      <td>Quake Exposes Italy's Challenge to Retrofit It...</td>\n",
       "      <td>NY Times</td>\n",
       "      <td>Gaia Pianigiani and Elisabetta Povoledo</td>\n",
       "      <td>2016-09-01</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CASETTA, Italy  —   Romano Camassi, a seismolo...</td>\n",
       "      <td>False</td>\n",
       "      <td>CASETTA, Italy     Romano Camassi, a seismolog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3755</td>\n",
       "      <td>21455</td>\n",
       "      <td>A Cheaper Airbag, and Takata's Road to a Deadl...</td>\n",
       "      <td>NY Times</td>\n",
       "      <td>Hiroko Tabuchi</td>\n",
       "      <td>2016-09-01</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>In the late 1990s, General Motors got an unexp...</td>\n",
       "      <td>False</td>\n",
       "      <td>In the late 1990s, General Motors got an unexp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3772</td>\n",
       "      <td>21474</td>\n",
       "      <td>Gene Wilder Dies at 83 Star of ‘Willy Wonka' a...</td>\n",
       "      <td>NY Times</td>\n",
       "      <td>Daniel Lewis</td>\n",
       "      <td>2016-09-01</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Gene Wilder, who established himself as one of...</td>\n",
       "      <td>False</td>\n",
       "      <td>Gene Wilder, who established himself as one of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>143552</td>\n",
       "      <td>214894</td>\n",
       "      <td>Watch SpaceX's rocket explode in a massive fir...</td>\n",
       "      <td>Washington Post</td>\n",
       "      <td>Christian Davenport</td>\n",
       "      <td>2016-09-01</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>https://web.archive.org/web/20160902000145/htt...</td>\n",
       "      <td>As SpaceX prepared to test fire the p...</td>\n",
       "      <td>False</td>\n",
       "      <td>As SpaceX prepared to test fire the p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>143553</td>\n",
       "      <td>214895</td>\n",
       "      <td>This photographer's photos show the tender mom...</td>\n",
       "      <td>Washington Post</td>\n",
       "      <td>Kenneth Dickerman</td>\n",
       "      <td>2016-09-01</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>https://web.archive.org/web/20160902000145/htt...</td>\n",
       "      <td>Old family photographs on Marisa Vesco...</td>\n",
       "      <td>False</td>\n",
       "      <td>Old family photographs on Marisa Vesco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>143584</td>\n",
       "      <td>214934</td>\n",
       "      <td>How Anthony Weinerâ€™s risque messages shaped ...</td>\n",
       "      <td>Washington Post</td>\n",
       "      <td>Sarah Jeong</td>\n",
       "      <td>2016-09-01</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>https://web.archive.org/web/20160904003253/htt...</td>\n",
       "      <td>In the aftermath of Anthony Weiner s late...</td>\n",
       "      <td>False</td>\n",
       "      <td>In the aftermath of Anthony Weiner s late...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>143592</td>\n",
       "      <td>214943</td>\n",
       "      <td>Stop touting the crazy hours you work. It help...</td>\n",
       "      <td>Washington Post</td>\n",
       "      <td>Jena McGregor</td>\n",
       "      <td>2016-09-01</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>https://web.archive.org/web/20160904003253/htt...</td>\n",
       "      <td>As Labor Day approaches, and a single day...</td>\n",
       "      <td>False</td>\n",
       "      <td>As Labor Day approaches, and a single day...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>143618</td>\n",
       "      <td>214973</td>\n",
       "      <td>One dietitianâ€™s secret weapon for healthy ea...</td>\n",
       "      <td>Washington Post</td>\n",
       "      <td>Jae Berman</td>\n",
       "      <td>2016-09-01</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>https://web.archive.org/web/20160905002504/htt...</td>\n",
       "      <td>I am going to fill you in on a major secret o...</td>\n",
       "      <td>False</td>\n",
       "      <td>I am going to fill you in on a major secret o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>302 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0      id                                              title  \\\n",
       "0          3722   21413  One Star Over, a Planet That Might Be Another ...   \n",
       "1          3748   21448  University of Chicago Strikes Back Against Cam...   \n",
       "2          3754   21454  Quake Exposes Italy's Challenge to Retrofit It...   \n",
       "3          3755   21455  A Cheaper Airbag, and Takata's Road to a Deadl...   \n",
       "4          3772   21474  Gene Wilder Dies at 83 Star of ‘Willy Wonka' a...   \n",
       "..          ...     ...                                                ...   \n",
       "297      143552  214894  Watch SpaceX's rocket explode in a massive fir...   \n",
       "298      143553  214895  This photographer's photos show the tender mom...   \n",
       "299      143584  214934  How Anthony Weinerâ€™s risque messages shaped ...   \n",
       "300      143592  214943  Stop touting the crazy hours you work. It help...   \n",
       "301      143618  214973  One dietitianâ€™s secret weapon for healthy ea...   \n",
       "\n",
       "         publication                                             author  \\\n",
       "0           NY Times                                      Kenneth Chang   \n",
       "1           NY Times  Richard Pérez-Peña, Mitch Smith and Stephanie ...   \n",
       "2           NY Times            Gaia Pianigiani and Elisabetta Povoledo   \n",
       "3           NY Times                                     Hiroko Tabuchi   \n",
       "4           NY Times                                       Daniel Lewis   \n",
       "..               ...                                                ...   \n",
       "297  Washington Post                                Christian Davenport   \n",
       "298  Washington Post                                  Kenneth Dickerman   \n",
       "299  Washington Post                                        Sarah Jeong   \n",
       "300  Washington Post                                      Jena McGregor   \n",
       "301  Washington Post                                         Jae Berman   \n",
       "\n",
       "           date    year  month  \\\n",
       "0    2016-09-01  2016.0    9.0   \n",
       "1    2016-09-01  2016.0    9.0   \n",
       "2    2016-09-01  2016.0    9.0   \n",
       "3    2016-09-01  2016.0    9.0   \n",
       "4    2016-09-01  2016.0    9.0   \n",
       "..          ...     ...    ...   \n",
       "297  2016-09-01  2016.0    9.0   \n",
       "298  2016-09-01  2016.0    9.0   \n",
       "299  2016-09-01  2016.0    9.0   \n",
       "300  2016-09-01  2016.0    9.0   \n",
       "301  2016-09-01  2016.0    9.0   \n",
       "\n",
       "                                                   url  \\\n",
       "0                                                  NaN   \n",
       "1                                                  NaN   \n",
       "2                                                  NaN   \n",
       "3                                                  NaN   \n",
       "4                                                  NaN   \n",
       "..                                                 ...   \n",
       "297  https://web.archive.org/web/20160902000145/htt...   \n",
       "298  https://web.archive.org/web/20160902000145/htt...   \n",
       "299  https://web.archive.org/web/20160904003253/htt...   \n",
       "300  https://web.archive.org/web/20160904003253/htt...   \n",
       "301  https://web.archive.org/web/20160905002504/htt...   \n",
       "\n",
       "                                               content  NYT summary  \\\n",
       "0    Another Earth could be circling the star right...        False   \n",
       "1    The anodyne welcome letter to incoming freshme...        False   \n",
       "2    CASETTA, Italy  —   Romano Camassi, a seismolo...        False   \n",
       "3    In the late 1990s, General Motors got an unexp...        False   \n",
       "4    Gene Wilder, who established himself as one of...        False   \n",
       "..                                                 ...          ...   \n",
       "297           As SpaceX prepared to test fire the p...        False   \n",
       "298          Old family photographs on Marisa Vesco...        False   \n",
       "299       In the aftermath of Anthony Weiner s late...        False   \n",
       "300       As Labor Day approaches, and a single day...        False   \n",
       "301   I am going to fill you in on a major secret o...        False   \n",
       "\n",
       "                                   content no nonascii  \n",
       "0    Another Earth could be circling the star right...  \n",
       "1    The anodyne welcome letter to incoming freshme...  \n",
       "2    CASETTA, Italy     Romano Camassi, a seismolog...  \n",
       "3    In the late 1990s, General Motors got an unexp...  \n",
       "4    Gene Wilder, who established himself as one of...  \n",
       "..                                                 ...  \n",
       "297           As SpaceX prepared to test fire the p...  \n",
       "298          Old family photographs on Marisa Vesco...  \n",
       "299       In the aftermath of Anthony Weiner s late...  \n",
       "300       As Labor Day approaches, and a single day...  \n",
       "301   I am going to fill you in on a major secret o...  \n",
       "\n",
       "[302 rows x 12 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(articleDataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect full article corpus\n",
    "Although the remaining set of data are not used, for the purpose of exploring the dataset, the following breakdown can be obtained. Note that at the bottom of the list are some misformatted dates. These are not pertinent to the date being used for the example here, so it is not necessary to address that at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Article counts by publisher:\n",
      "Breitbart           104\n",
      "NY Post              61\n",
      "CNN                  57\n",
      "Reuters              56\n",
      "NPR                  54\n",
      "NY Times             50\n",
      "Washington Post      50\n",
      "Buzzfeed News        48\n",
      "Atlantic             48\n",
      "Business Insider     41\n",
      "Guardian             35\n",
      "National Review      32\n",
      "Fox News             28\n",
      "Name: publication, dtype: int64\n",
      "\n",
      "Article counts by date:\n",
      "2016-12-02    362\n",
      "2016-09-01    302\n",
      "Name: date, dtype: int64\n",
      "\n",
      "Final dataset:\n",
      "\n",
      "Date: 2016-09-01 \n",
      "\n",
      "Breitbart           50\n",
      "Buzzfeed News       35\n",
      "NY Times            28\n",
      "NY Post             27\n",
      "NPR                 26\n",
      "Atlantic            24\n",
      "Washington Post     22\n",
      "CNN                 20\n",
      "Reuters             15\n",
      "Business Insider    15\n",
      "Guardian            14\n",
      "Fox News            14\n",
      "National Review     12\n",
      "Name: publication, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>publication</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "      <th>NYT summary</th>\n",
       "      <th>content no nonascii</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3722</td>\n",
       "      <td>21413</td>\n",
       "      <td>One Star Over, a Planet That Might Be Another ...</td>\n",
       "      <td>NY Times</td>\n",
       "      <td>Kenneth Chang</td>\n",
       "      <td>2016-09-01</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Another Earth could be circling the star right...</td>\n",
       "      <td>False</td>\n",
       "      <td>Another Earth could be circling the star right...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3748</td>\n",
       "      <td>21448</td>\n",
       "      <td>University of Chicago Strikes Back Against Cam...</td>\n",
       "      <td>NY Times</td>\n",
       "      <td>Richard Pérez-Peña, Mitch Smith and Stephanie ...</td>\n",
       "      <td>2016-09-01</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The anodyne welcome letter to incoming freshme...</td>\n",
       "      <td>False</td>\n",
       "      <td>The anodyne welcome letter to incoming freshme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3754</td>\n",
       "      <td>21454</td>\n",
       "      <td>Quake Exposes Italy's Challenge to Retrofit It...</td>\n",
       "      <td>NY Times</td>\n",
       "      <td>Gaia Pianigiani and Elisabetta Povoledo</td>\n",
       "      <td>2016-09-01</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CASETTA, Italy  —   Romano Camassi, a seismolo...</td>\n",
       "      <td>False</td>\n",
       "      <td>CASETTA, Italy     Romano Camassi, a seismolog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3755</td>\n",
       "      <td>21455</td>\n",
       "      <td>A Cheaper Airbag, and Takata's Road to a Deadl...</td>\n",
       "      <td>NY Times</td>\n",
       "      <td>Hiroko Tabuchi</td>\n",
       "      <td>2016-09-01</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>In the late 1990s, General Motors got an unexp...</td>\n",
       "      <td>False</td>\n",
       "      <td>In the late 1990s, General Motors got an unexp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3772</td>\n",
       "      <td>21474</td>\n",
       "      <td>Gene Wilder Dies at 83 Star of ‘Willy Wonka' a...</td>\n",
       "      <td>NY Times</td>\n",
       "      <td>Daniel Lewis</td>\n",
       "      <td>2016-09-01</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Gene Wilder, who established himself as one of...</td>\n",
       "      <td>False</td>\n",
       "      <td>Gene Wilder, who established himself as one of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>143552</td>\n",
       "      <td>214894</td>\n",
       "      <td>Watch SpaceX's rocket explode in a massive fir...</td>\n",
       "      <td>Washington Post</td>\n",
       "      <td>Christian Davenport</td>\n",
       "      <td>2016-09-01</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>https://web.archive.org/web/20160902000145/htt...</td>\n",
       "      <td>As SpaceX prepared to test fire the p...</td>\n",
       "      <td>False</td>\n",
       "      <td>As SpaceX prepared to test fire the p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>143553</td>\n",
       "      <td>214895</td>\n",
       "      <td>This photographer's photos show the tender mom...</td>\n",
       "      <td>Washington Post</td>\n",
       "      <td>Kenneth Dickerman</td>\n",
       "      <td>2016-09-01</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>https://web.archive.org/web/20160902000145/htt...</td>\n",
       "      <td>Old family photographs on Marisa Vesco...</td>\n",
       "      <td>False</td>\n",
       "      <td>Old family photographs on Marisa Vesco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>143584</td>\n",
       "      <td>214934</td>\n",
       "      <td>How Anthony Weinerâ€™s risque messages shaped ...</td>\n",
       "      <td>Washington Post</td>\n",
       "      <td>Sarah Jeong</td>\n",
       "      <td>2016-09-01</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>https://web.archive.org/web/20160904003253/htt...</td>\n",
       "      <td>In the aftermath of Anthony Weiner s late...</td>\n",
       "      <td>False</td>\n",
       "      <td>In the aftermath of Anthony Weiner s late...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>143592</td>\n",
       "      <td>214943</td>\n",
       "      <td>Stop touting the crazy hours you work. It help...</td>\n",
       "      <td>Washington Post</td>\n",
       "      <td>Jena McGregor</td>\n",
       "      <td>2016-09-01</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>https://web.archive.org/web/20160904003253/htt...</td>\n",
       "      <td>As Labor Day approaches, and a single day...</td>\n",
       "      <td>False</td>\n",
       "      <td>As Labor Day approaches, and a single day...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>143618</td>\n",
       "      <td>214973</td>\n",
       "      <td>One dietitianâ€™s secret weapon for healthy ea...</td>\n",
       "      <td>Washington Post</td>\n",
       "      <td>Jae Berman</td>\n",
       "      <td>2016-09-01</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>https://web.archive.org/web/20160905002504/htt...</td>\n",
       "      <td>I am going to fill you in on a major secret o...</td>\n",
       "      <td>False</td>\n",
       "      <td>I am going to fill you in on a major secret o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>302 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0      id                                              title  \\\n",
       "0          3722   21413  One Star Over, a Planet That Might Be Another ...   \n",
       "1          3748   21448  University of Chicago Strikes Back Against Cam...   \n",
       "2          3754   21454  Quake Exposes Italy's Challenge to Retrofit It...   \n",
       "3          3755   21455  A Cheaper Airbag, and Takata's Road to a Deadl...   \n",
       "4          3772   21474  Gene Wilder Dies at 83 Star of ‘Willy Wonka' a...   \n",
       "..          ...     ...                                                ...   \n",
       "297      143552  214894  Watch SpaceX's rocket explode in a massive fir...   \n",
       "298      143553  214895  This photographer's photos show the tender mom...   \n",
       "299      143584  214934  How Anthony Weinerâ€™s risque messages shaped ...   \n",
       "300      143592  214943  Stop touting the crazy hours you work. It help...   \n",
       "301      143618  214973  One dietitianâ€™s secret weapon for healthy ea...   \n",
       "\n",
       "         publication                                             author  \\\n",
       "0           NY Times                                      Kenneth Chang   \n",
       "1           NY Times  Richard Pérez-Peña, Mitch Smith and Stephanie ...   \n",
       "2           NY Times            Gaia Pianigiani and Elisabetta Povoledo   \n",
       "3           NY Times                                     Hiroko Tabuchi   \n",
       "4           NY Times                                       Daniel Lewis   \n",
       "..               ...                                                ...   \n",
       "297  Washington Post                                Christian Davenport   \n",
       "298  Washington Post                                  Kenneth Dickerman   \n",
       "299  Washington Post                                        Sarah Jeong   \n",
       "300  Washington Post                                      Jena McGregor   \n",
       "301  Washington Post                                         Jae Berman   \n",
       "\n",
       "           date    year  month  \\\n",
       "0    2016-09-01  2016.0    9.0   \n",
       "1    2016-09-01  2016.0    9.0   \n",
       "2    2016-09-01  2016.0    9.0   \n",
       "3    2016-09-01  2016.0    9.0   \n",
       "4    2016-09-01  2016.0    9.0   \n",
       "..          ...     ...    ...   \n",
       "297  2016-09-01  2016.0    9.0   \n",
       "298  2016-09-01  2016.0    9.0   \n",
       "299  2016-09-01  2016.0    9.0   \n",
       "300  2016-09-01  2016.0    9.0   \n",
       "301  2016-09-01  2016.0    9.0   \n",
       "\n",
       "                                                   url  \\\n",
       "0                                                  NaN   \n",
       "1                                                  NaN   \n",
       "2                                                  NaN   \n",
       "3                                                  NaN   \n",
       "4                                                  NaN   \n",
       "..                                                 ...   \n",
       "297  https://web.archive.org/web/20160902000145/htt...   \n",
       "298  https://web.archive.org/web/20160902000145/htt...   \n",
       "299  https://web.archive.org/web/20160904003253/htt...   \n",
       "300  https://web.archive.org/web/20160904003253/htt...   \n",
       "301  https://web.archive.org/web/20160905002504/htt...   \n",
       "\n",
       "                                               content  NYT summary  \\\n",
       "0    Another Earth could be circling the star right...        False   \n",
       "1    The anodyne welcome letter to incoming freshme...        False   \n",
       "2    CASETTA, Italy  —   Romano Camassi, a seismolo...        False   \n",
       "3    In the late 1990s, General Motors got an unexp...        False   \n",
       "4    Gene Wilder, who established himself as one of...        False   \n",
       "..                                                 ...          ...   \n",
       "297           As SpaceX prepared to test fire the p...        False   \n",
       "298          Old family photographs on Marisa Vesco...        False   \n",
       "299       In the aftermath of Anthony Weiner s late...        False   \n",
       "300       As Labor Day approaches, and a single day...        False   \n",
       "301   I am going to fill you in on a major secret o...        False   \n",
       "\n",
       "                                   content no nonascii  \n",
       "0    Another Earth could be circling the star right...  \n",
       "1    The anodyne welcome letter to incoming freshme...  \n",
       "2    CASETTA, Italy     Romano Camassi, a seismolog...  \n",
       "3    In the late 1990s, General Motors got an unexp...  \n",
       "4    Gene Wilder, who established himself as one of...  \n",
       "..                                                 ...  \n",
       "297           As SpaceX prepared to test fire the p...  \n",
       "298          Old family photographs on Marisa Vesco...  \n",
       "299       In the aftermath of Anthony Weiner s late...  \n",
       "300       As Labor Day approaches, and a single day...  \n",
       "301   I am going to fill you in on a major secret o...  \n",
       "\n",
       "[302 rows x 12 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getInputDataAndDisplayStats(runParams['input_file'][0],\n",
    "                            runParams['process_date'][0],\n",
    "                            printSummary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Processing\n",
    "### Stop words\n",
    "In processing natural language, it is necessary to suppress words that convey little value. Typically these are common words such as \"a\", \"man\", \"Friday\", etc. They are referred to as Stop Words. A list of these files is in an included file and is loaded below. This file is independent of whether nltk or spaCy is used for some of the other NLP features. (it will only be applied towards the end of the NLP processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadStopWords(stopWordsFileName):\n",
    "\tstop_words=[]\n",
    "\tf=open(stopWordsFileName, 'r')\n",
    "\tfor l in f.readlines():\n",
    "\t\tstop_words.append(l.replace('\\n', ''))\n",
    "\treturn stop_words\n",
    "\n",
    "\n",
    "# Load stop words now - these will be deleted from final text by processor before vectorizing\n",
    "# 0 index is required because the parameters are forced to be lists by ParameterGrid\n",
    "stop_words=loadStopWords(runParams['stop_words_file'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK\n",
    "The following function provides the NLTK pre-processing. Specifically, it:\n",
    "- restricts the text to the requested parts-of-speech (according to the run parameters). This includes Proper Nouns, Common Nouns, Verbs, etc. Words of different parts-of-speech are more/less important in determining the story relayed by an article - for example, adjectives are not important. The goal is to reduce the number of words in a way that eliminates those that cause noise rather than adding value - and thus makes the algorithm operate more effectively.\n",
    "- applies lemmatisation (optionally), thus substituting a word for its root - the intention here being to reduce the final universe of words in the corpus, and thus make finding related articles easier.\n",
    "- truncates the length of the article to the degree requested. (testing has shown that restricting to the first few paragraph increases the ease with which the algorithm can relate articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stringNLTKProcess(nl,stringToConvert,partsOfSpeech,stop_words,maxWords=None,lemmatizer=None):\n",
    "\tsentences=nl.sent_tokenize(stringToConvert)\n",
    "\tstr=[]\n",
    "\tfor sentence in sentences:\n",
    "\t\twordString=[]\n",
    "\t\tfor word,pos in nl.pos_tag(nl.word_tokenize(sentence)):\n",
    "\t\t\t# The following condition avoids any POS which corresponds to punctuation (and takes all others)\n",
    "\t\t\tif partsOfSpeech==None:\n",
    "\t\t\t\tif pos[0]>='A' and pos[0]<='Z':\n",
    "\t\t\t\t\twordString.append(word)\n",
    "\t\t\telif pos in partsOfSpeech:\n",
    "\t\t\t\twordString.append(word)\n",
    "\t\tfor wrd in wordString:\n",
    "\t\t\twrdlower=wrd.lower()\n",
    "\t\t\tif wrdlower not in stop_words and wrdlower!=\"'s\":\n",
    "\t\t\t\tif maxWords==None or len(str)<maxWords:\n",
    "\t\t\t\t\tif lemmatizer==None:\n",
    "\t\t\t\t\t\tstr.append(wrdlower)\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tstr.append(lemmatizer.lemmatize(wrd.lower(), pos='v'))\n",
    "\t\t\tif maxWords!=None and len(str)==maxWords:\n",
    "\t\t\t\treturn ' '.join(str)\n",
    "\treturn ' '.join(str)\n",
    "\n",
    "##########################################################################################\n",
    "\n",
    "def removeSpacesAndPunctuation(textString): \n",
    "    return \"\".join(i for i in textString if (ord(i)>=48 and ord(i)<=57) or (ord(i)>=97 and ord(i)<=122))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spaCy\n",
    "The second NLP library supported here is spaCy. It is being used to provide the same features as NLTK - although there are pros and cons to the specific implementations of each library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stringSpaCyProcess(nlp,stringToConvert,partsOfSpeech,maxWords,stop_words,lemmatize):\n",
    "\tdoc=nlp(stringToConvert)\n",
    "\tif partsOfSpeech==None:\n",
    "\t\tspacyTokens=[w for w in doc]\n",
    "\telse:\n",
    "\t\tspacyTokens=[w for w in doc if w.pos_ in partsOfSpeech]\n",
    "\n",
    "\tstr=[]\n",
    "\tfor spt in spacyTokens:\n",
    "\t\tif lemmatize:\n",
    "\t\t\twrd=spt.lemma_\n",
    "\t\telse:\n",
    "\t\t\twrd=spt.text\n",
    "\t\twrdlower=removeSpacesAndPunctuation(wrd.lower())\n",
    "\t\t# The middle term below is correctly wrd.lower() not wrdlower since the function call\n",
    "\t\t# above strips out the --, and I don't want to compare with 'pron' in case that\n",
    "\t\t# finds false matches\n",
    "\t\tif wrdlower not in stop_words and wrd.lower()!='-pron-' and not wrdlower=='':\n",
    "\t\t\tif maxWords==None or len(str)<maxWords:\n",
    "\t\t\t\tstr.append(wrdlower)\n",
    "\t\tif maxWords!=None and len(str)==maxWords:\n",
    "\t\t\t\treturn ' '.join(str)\t\t\n",
    "\treturn ' '.join(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for analysing results\n",
    "In order to score how well the algorithm is assigning articles to stories, it is useful (but optional) to provide a file containing a \"story map\". This file effectively specifies which articles belong to which stories. It is incomplete, but it is sufficiently extensive to demonstrate the effectiveness of the results.\n",
    "In addition, a list of article IDs can be provided to drive the post vectorization validation. For the purposes of this workbook, this second list is being derived from the story map.\n",
    "### Setup story map and testing list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setupStoryMapAndReportList(args=None,reportArticleList=None,storyMapFileName=None):\n",
    "\t# Story Map is used in fitting if grid search is applied (As ground truth)\n",
    "\t# It is also used in graph if no threshold provided (to determine colours, not to determine location)\n",
    "\t# Report Article List is used at the end to create a report with, for each\n",
    "\t# article in the list, the set of articles within tolerance, and the key words for each\n",
    "\tif args==None:\n",
    "\t\tarticleList=reportArticleList\n",
    "\t\tfileName=storyMapFileName\n",
    "\telse:\n",
    "\t\tarticleList=args['article_id_list']\n",
    "\t\tfileName=args['story_map_validation']\n",
    "\n",
    "\treportArticleList=articleList\n",
    "\tif fileName!=None:\n",
    "\t\tstoryMap=readStoryMapFromFile(fileName)\n",
    "\t\tif reportArticleList==None:\n",
    "\t\t\treportArticleList=[]\n",
    "\t\t\tfor story, articleList in storyMap.items():\n",
    "\t\t\t\treportArticleList.append(articleList[0])\n",
    "\telse:\n",
    "\t\tstoryMap=None\n",
    "\treturn storyMap,reportArticleList\n",
    "\n",
    "def readStoryMapFromFile(filename):\n",
    "\treturn readDictFromCsvFile(filename,'StoryMap')\n",
    "\n",
    "##########################################################################################\n",
    "\n",
    "def readGridParameterRangeFromFile(filename):\n",
    "\treturn readDictFromCsvFile(filename,'GridParameters')\n",
    "\n",
    "##########################################################################################\n",
    "\n",
    "def readDictFromCsvFile(filename,schema):\n",
    "\tgridParamDict={}\n",
    "\twith open(filename, 'r') as f:\n",
    "\t\tfor row in f:\n",
    "\t\t\trow=row[:-1] # Exclude the carriage return\n",
    "\t\t\trow=row.split(\",\")\n",
    "\t\t\tkey=row[0]\n",
    "\t\t\tvals=row[1:]\n",
    "\t\t\t\n",
    "\t\t\tif schema=='GridParameters':\n",
    "\t\t\t\tif key in ['story_threshold','tfidf_maxdf']:\n",
    "\t\t\t\t\tfinalVals=list(float(n) for n in vals)\n",
    "\t\t\t\telif key in ['ngram_max','tfidf_mindf','max_length']:\n",
    "\t\t\t\t\tfinalVals=list(int(n) for n in vals)\n",
    "\t\t\t\telif key in ['lemma_conversion','tfidf_binary']:\n",
    "\t\t\t\t\tfinalVals=list(str2bool(n) for n in vals)\n",
    "\t\t\t\telif key in ['parts_of_speech']:\n",
    "\t\t\t\t\tlistlist=[]\n",
    "\t\t\t\t\tfor v in vals:\n",
    "\t\t\t\t\t\tlistlist.append(v.split(\"+\"))\n",
    "\t\t\t\t\tfinalVals=listlist\n",
    "\t\t\t\telif key in ['tfidf_norm','nlp_library']:\n",
    "\t\t\t\t\tfinalVals=vals\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tprint(key)\n",
    "\t\t\t\t\tprint(\"KEY ERROR\")\n",
    "\t\t\t\t\treturn\n",
    "\t\t\telif schema=='StoryMap':\n",
    "\t\t\t\tfinalVals=list(int(n) for n in vals)\n",
    "\t\t\telse:\n",
    "\t\t\t\tprint(schema)\n",
    "\t\t\t\tprint(\"SCHEMA ERROR\")\n",
    "\t\t\t\treturn\n",
    "\t\t\t\n",
    "\t\t\tgridParamDict[key]=finalVals\n",
    "\treturn gridParamDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the story map from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "storyMap,reportArticleList=setupStoryMapAndReportList(storyMapFileName='storyMapForValidation.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the story map we see that it forms a dict containing a key corresponding to the name of the story and a value containing a list of the article IDs germane to that story."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trump meeting : [151832, 110126, 172078, 48306, 57365, 190512, 26536, 71335, 21499, 23872, 142033, 110133, 23888, 71336, 57366, 71339]\n",
      "Brazil impeachment : [120639, 80103, 25225, 21502, 57362, 120636, 110141]\n",
      "Kaepernick : [40617, 40543, 39520, 80109, 80101, 47403]\n",
      "Clinton Guccifer : [214888, 85803, 47979]\n",
      "Farage : [37252, 37468, 46175]\n",
      "Anthony Weiner : [49480, 110144, 142300, 214934]\n",
      "SpaceX : [38658, 134545, 172095, 214894]\n",
      "Safe space : [21448, 78169, 78171]\n",
      "Lauer debate : [43447, 47078, 138709]\n",
      "Venezuela : [172079, 57375, 190522]\n",
      "Iran deal : [158005, 48823, 57373, 120634]\n",
      "Penn State : [80094, 157527, 214892]\n",
      "David Brown : [172085, 80096, 141886]\n"
     ]
    }
   ],
   "source": [
    "for story, articleList in storyMap.items():\n",
    "    print(story,\":\",articleList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[151832, 120639, 40617, 214888, 37252, 49480, 38658, 21448, 43447, 172079, 158005, 80094, 172085]\n"
     ]
    }
   ],
   "source": [
    "print(reportArticleList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm preparation\n",
    "The algorithm can be considered as having the following steps:\n",
    "- Preprocessing for NLP features\n",
    "- Conversion to TF-IDF vectors\n",
    "- A Relatedness Score for a set of vectors\n",
    "- A main processing loop for tying everything together\n",
    "\n",
    "### Preprocess and vectorizing\n",
    "This section provides the necessary code for the natural language processing requirements.\n",
    "\n",
    "Two NLP libraries are supported. The user can choose (in the run parameters) between:\n",
    "- NLTK\n",
    "- SpaCy\n",
    "\n",
    "The various natural language functions will be applied (to the extent requested in the run parameters).\n",
    "- Lemmatization\n",
    "- Remove stop words\n",
    "- Restrict to specific parts-of-speech\n",
    "- Constrain overall length\n",
    "- n-grams\n",
    "\n",
    "Once that's done, the body of the processed articles will be analysed and converted into tf-idf values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessAndVectorize(articleDataFrame,args,pos_nlp_mapping,nlp,nl,wordnet_lemmatizer,stop_words):\n",
    "\t# Map the input parts of speech list to the coding required for the specific NLP library\n",
    "\tif args['parts_of_speech'][0]!='ALL':\n",
    "\t\tpartsOfSpeech=[]\n",
    "\t\tfor pos in args['parts_of_speech']:\n",
    "\t\t\tpartsOfSpeech.append(pos_nlp_mapping[args['nlp_library']][pos])\n",
    "\t\tpartsOfSpeech=[item for sublist in partsOfSpeech for item in sublist]\n",
    "\telse:\n",
    "\t\tpartsOfSpeech=None\n",
    "\n",
    "\t# Processing of text depends on NLP library choice\n",
    "\tif args['nlp_library']=='spaCy':\n",
    "\t\tarticleDataFrame['input to vectorizer']=articleDataFrame['content no nonascii'].map(lambda x: stringSpaCyProcess(nlp,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   x,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   partsOfSpeech=partsOfSpeech,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   maxWords=args['max_length'],\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   stop_words=stop_words,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   lemmatize=args['lemma_conversion']))\n",
    "\telif args['nlp_library']=='nltk':\n",
    "\t\tarticleDataFrame['input to vectorizer']=articleDataFrame['content no nonascii'].map(lambda x: stringNLTKProcess(nl,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  x,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  partsOfSpeech=partsOfSpeech,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  stop_words=stop_words,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  maxWords=args['max_length'],\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  lemmatizer=wordnet_lemmatizer))\n",
    "\telse:\n",
    "\t\tprint(\"PROBLEM... NO VALID NLP LIBRARY... MUST BE nltk OR spaCy\")\n",
    "\n",
    "\t# To get default values a couple of parameters need to be not passed if not specified on the command line\n",
    "\t# Passing as None behaves differently to passing no parameter (which would invoke the default value)\n",
    "\toptArgsForVectorizer={}\n",
    "\tif args['tfidf_maxdf'] != None:\n",
    "\t\toptArgsForVectorizer['max_df']=args['tfidf_maxdf']\n",
    "\tif args['tfidf_mindf'] != None:\n",
    "\t\toptArgsForVectorizer['min_df']=args['tfidf_mindf']\n",
    "\n",
    "\t# Create and run the vectorizer\n",
    "\tvectorizer=TfidfVectorizer(analyzer='word',\n",
    "   \t    \t                   ngram_range=(1,args['ngram_max']),\n",
    "       \t    \t               lowercase=True,\n",
    "           \t    \t    \t   binary=args['tfidf_binary'],\n",
    "               \t\t    \t   norm=args['tfidf_norm'],\n",
    "\t\t\t\t\t\t\t   **optArgsForVectorizer)\n",
    "\ttfidfVectors=vectorizer.fit_transform(articleDataFrame['input to vectorizer'])\n",
    "\tterms=vectorizer.get_feature_names()\n",
    "\treturn tfidfVectors, terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring\n",
    "Scores must be computed for each pair of articles, for the following reasons:\n",
    "- To determine the proposed clustering of articles in stories (and to evaluate this clustering against a ground truth story map)\n",
    "- To evaluate the grid parameters in order to choose the preferable combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoreCurrentParamGuess(tfidfVectors,storyMap,articleDataFrame,threshold,printErrors=False):\n",
    "\t# Work with distances relative to first item in each cluster - even though this is clearly arbitrary since that\n",
    "\t# point could be an outlier in the cluster and hence might cause problems.\n",
    "\t# But I have to start somewhere - and can refine it later if needed.\n",
    "\n",
    "\tnonZeroCoords=initialiseAllNonZeroCoords(tfidfVectors)\n",
    "\tscore=0\n",
    "\toutGood=0\n",
    "\toutBad=0\n",
    "\tinGood=0\n",
    "\tinBad=0\n",
    "\tfor story, storyArticles in storyMap.items():\n",
    "\t\tleadArticleIndex=articleDataFrame[articleDataFrame['id']==storyArticles[0]].index[0]\n",
    "\t\t# Compute score of all articles in corpus relative to first article in story (.product)\n",
    "\t\t# Then count through list relative to threshold (add one for a good result, subtract one for a bad result)\n",
    "\t\tscores=productRelatednessScores(tfidfVectors,nonZeroCoords,leadArticleIndex)\n",
    "\t\trankedIndices=np.argsort(scores)\n",
    "\t\tfoundRelatedArticles=[]\n",
    "\t\t# THE SORTING HERE IS NOT STRICTLY REQUIRED, BUT I COULD USE IT SO THAT ONCE THE THRESHOLD IS PASSED\n",
    "\t\t# IN THE LOOP, THEN I INFER THE REMAINING RESULTS\n",
    "\t\tfor article in reversed(rankedIndices):\n",
    "\t\t\tthisArticleIndex=articleDataFrame['id'][article]\n",
    "\t\t\tif thisArticleIndex in storyArticles:\n",
    "\t\t\t\tif scores[article]>=threshold:\n",
    "\t\t\t\t\tscore+=1\n",
    "\t\t\t\t\tinGood+=1\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tscore-=1\n",
    "\t\t\t\t\tinBad+=1\n",
    "\t\t\t\t\tif printErrors:\n",
    "\t\t\t\t\t\tprint(\"ERROR:\",thisArticleIndex,\"should be in\",story)\n",
    "\t\t\telse: # article not supposed to be in range\n",
    "\t\t\t\tif scores[article]<=threshold:\n",
    "\t\t\t\t\tscore+=1\n",
    "\t\t\t\t\toutGood+=1\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tscore-=1\n",
    "\t\t\t\t\toutBad+=1\n",
    "\t\t\t\t\tif printErrors:\n",
    "\t\t\t\t\t\tprint(\"ERROR:\",thisArticleIndex,\"should NOT be in\",story)\n",
    "\tscoreDict={'score':score,'inGood':inGood,'inBad':inBad,'outGood':outGood,'outBad':outBad}\n",
    "\treturn scoreDict\n",
    "\n",
    "##########################################################################################\n",
    "\n",
    "def initialiseAllNonZeroCoords(tfidfVectors):\n",
    "# This function just exists since it seems to be expensive and I'd rather not call it multiple times\n",
    "# Hence it is intended to be called outside of loops in order to simplify the row specific processing\n",
    "\tvalues=[]\n",
    "\tnzc=zip(*tfidfVectors.nonzero())\n",
    "\n",
    "\t# In Python 3 the zip can only be iterated through one time before it is automatically released\n",
    "\t# So need to copy the results otherwise the main loop below will no longer work\n",
    "\tpointList=[]\n",
    "\tfor i,j in nzc:\n",
    "\t\tpointList.append([i,j])\t\t\n",
    "\n",
    "\tfor row in range(tfidfVectors.shape[0]):\n",
    "\t\trowList=[]\n",
    "\t\tfor i,j in pointList:\n",
    "\t\t\tif row==i:\n",
    "\t\t\t\trowList.append(j)\n",
    "\t\tvalues.append(rowList)\n",
    "\n",
    "\treturn values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relatedness Scoring measure\n",
    "The Relatedness Score is computed between a pair of articles by taking the dot product of the values across each dimension of the pair's TF-IDF vectors.\n",
    "This has the following behaviour characteristics:\n",
    "- If a term is important in both articles, that term will have a high impact on the article relatedness\n",
    "- If a term is not important in either or both articles, that term will have a high impact on the article relatedness\n",
    "\n",
    "The non-linearity coming from the product ensures a more contextual and intuitive scoring than the conventional Euclidean measure. (thus resulting in more usable pairings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def productRelatednessScores(tfidfVectors,nonZeroCoords,refRow):\n",
    "\tscores=[0]*tfidfVectors.shape[0]\n",
    "\tfor toRow in range(tfidfVectors.shape[0]):\n",
    "\t\tscores[toRow]=sum([(tfidfVectors[toRow,w]*tfidfVectors[refRow,w]) for w in nonZeroCoords[refRow] if w in nonZeroCoords[toRow]])\n",
    "\treturn scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\goldm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk; nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the algorithm\n",
    "Now that all the pieces are in place, a loop is run to tie everything together - calling the vectorizer and scoring the results.\n",
    "If we are running in GridSearch mode, the loop will repeat and keep track of the best results achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 3914, 'inGood': 60, 'inBad': 2, 'outGood': 3860, 'outBad': 4}\n"
     ]
    }
   ],
   "source": [
    "# Loop across all parameter combinations in grid to determine best set\n",
    "# If not doing grid search, will just pass through the loop once\n",
    "bestParamScoreDict={'score':-1000000}\n",
    "bestParams=parameterGrid[0]\n",
    "for i,currentParams in enumerate(parameterGrid):\n",
    "\tif len(parameterGrid)>1:\n",
    "\t\tprint(\"Combination:\",i+1,\"of\",len(parameterGrid))\n",
    "\t\tprint(currentParams)\n",
    "\n",
    "\t# Determine tf-idf vectors\n",
    "\t# terms is just used later on if analysis of final results is requested\n",
    "\ttfidfVectors,terms=preprocessAndVectorize(articleDataFrame,\n",
    "\t\t\t\t\t\t\t\t\t\t\t  currentParams,\n",
    "\t\t\t\t\t\t\t\t\t\t\t  pos_nlp_mapping,\n",
    "\t\t\t\t\t\t\t\t\t\t\t  nlp,\n",
    "\t\t\t\t\t\t\t\t\t\t\t  nl,\n",
    "\t\t\t\t\t\t\t\t\t\t\t  wordnet_lemmatizer,\n",
    "\t\t\t\t\t\t\t\t\t\t\t  stop_words)\n",
    "\n",
    "\t# Compute scores if threshold provided (meaning as part of grid search)\n",
    "\tif 'story_threshold' in currentParams and currentParams['story_threshold']!=None:\n",
    "\t\tscoreDict=scoreCurrentParamGuess(tfidfVectors,storyMap,articleDataFrame,currentParams['story_threshold'])\n",
    "\t\tprint(scoreDict)\n",
    "\n",
    "\t\t# Update best so far\n",
    "\t\tif scoreDict['score']>=bestParamScoreDict['score']:\n",
    "\t\t\tif len(parameterGrid)>1:\n",
    "\t\t\t\tprint(i+1,\"is the best so far!\")\n",
    "\t\t\tbestParams=currentParams\n",
    "\t\t\tbestParamScoreDict=scoreDict\n",
    "\t# End grid/parameter loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tidy up by restoring to best run before proceeding with analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set threshold to input value from best (and possibly only) run for use in results analysis\n",
    "# Unless not specified at all\n",
    "if 'story_threshold' in bestParams and bestParams['story_threshold']!=None:\n",
    "\tthreshold=bestParams['story_threshold']\n",
    "else:\n",
    "\tthreshold=None\n",
    "\n",
    "\n",
    "# If there was a real parameter grid, then output/refresh results\n",
    "if len(parameterGrid)>1:\n",
    "\tprint(\"BEST PARAMETERS:\")\n",
    "\tprint(bestParams)\n",
    "\tprint(bestParamScoreDict)\n",
    "\tscoreCurrentParamGuess(tfidfVectors,storyMap,articleDataFrame,threshold,printErrors=True)\n",
    "\t# Recreate vector for best results in loop\n",
    "\t# terms is just used later on if analysis of final results is requested\n",
    "\ttfidfVectors,terms=preprocessAndVectorize(articleDataFrame,\n",
    "\t\t\t\t\t\t\t\t\t\t\t  bestParams,\n",
    "\t\t\t\t\t\t\t\t\t\t\t  pos_nlp_mapping,\n",
    "\t\t\t\t\t\t\t\t\t\t\t  nlp,\n",
    "\t\t\t\t\t\t\t\t\t\t\t  nl,\n",
    "\t\t\t\t\t\t\t\t\t\t\t  wordnet_lemmatizer,\n",
    "\t\t\t\t\t\t\t\t\t\t\t  stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of results\n",
    "### Produce graphs\n",
    "In order to produce a graph of the results, the TF-IDF vectors are reduced to two dimensions.\n",
    "\n",
    "Clustering is computed using the full n dimensions, with the threshold determining which articles end up grouped into shared stories.\n",
    "\n",
    "The graphs is ultimately rendered using Bokeh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce vector space to two dimensions\n",
    "# Then produce Bokeh graph\n",
    "def graphVectorSpace(tfidfVectors,extraColumns,dateForTitle,storyMap,threshold):\n",
    "\t# Better results seem to be obtained by breaking the dimensionality reduction into two steps\n",
    "\n",
    "\t# First reduce to fifty dimensions with SVD\n",
    "\tfrom sklearn.decomposition import TruncatedSVD\n",
    "\tsvd=TruncatedSVD(n_components=50, random_state=0)\n",
    "\tsvdResults=svd.fit_transform(tfidfVectors)\n",
    "\n",
    "\t# Next continue to two dimensions with TSNE\n",
    "\tfrom sklearn.manifold import TSNE\n",
    "\ttsneModel=TSNE(n_components=2, verbose=0, random_state=0, n_iter=500)\n",
    "\ttsneResults=tsneModel.fit_transform(svdResults)\n",
    "\ttfidf2dDataFrame=pd.DataFrame(tsneResults)\n",
    "\ttfidf2dDataFrame.columns=['x','y']\n",
    "\n",
    "\ttfidf2dDataFrame['publication']=extraColumns['publication']\t\n",
    "\ttfidf2dDataFrame['id']=extraColumns['id']\t\n",
    "\ttfidf2dDataFrame['content']=extraColumns['content no nonascii'].map(lambda x: x[:200])\n",
    "\n",
    "\t# All articles will be marked as NA to indicate that they have not been assigned to a story\n",
    "\t# Then those which have been assigned one will be updated to refer to that\n",
    "\ttfidf2dDataFrame['category']='NA'\n",
    "\n",
    "\t# If the threshold is not provided, then just graph the vector space as is\n",
    "\t# With colours indicating desired story grouping\n",
    "\t# This still has value because it shows how well stories cluster together\n",
    "\tif threshold==None:\n",
    "\t\tgraphTitle=(\"TF-IDF article clustering - story assignment from map - \"+dateForTitle[0])\n",
    "\t\tfor story, storyArticles in storyMap.items():\n",
    "\t\t\tfor article in storyArticles:\n",
    "\t\t\t\tif len(tfidf2dDataFrame[tfidf2dDataFrame['id']==article].index)==1:\n",
    "\t\t\t\t\ti=tfidf2dDataFrame[tfidf2dDataFrame['id']==article].index[0]\n",
    "\t\t\t\t\ttfidf2dDataFrame['category'][i]=story\n",
    "\telse:\n",
    "\t\tgraphTitle=(\"TF-IDF article clustering - story assignment computed - \"+dateForTitle[0])\n",
    "\t\tnonZeroCoords=initialiseAllNonZeroCoords(tfidfVectors)\n",
    "\t\tfor story, storyArticles in storyMap.items():\n",
    "\t\t\tleadArticleIndex=extraColumns[extraColumns['id']==storyArticles[0]].index[0]\n",
    "\t\t\t# Compute score of all articles in corpus relative to first article in story (.product)\n",
    "\t\t\tscores=productRelatednessScores(tfidfVectors,nonZeroCoords,leadArticleIndex)\n",
    "\t\t\trankedIndices=np.argsort(scores)\n",
    "\t\t\tfor article in rankedIndices:\n",
    "\t\t\t\tif scores[article]>=threshold:\n",
    "\t\t\t\t\ttfidf2dDataFrame['category', article]=story\n",
    "\n",
    "\timport bokeh.plotting as bp\n",
    "\tfrom bokeh.models import HoverTool\n",
    "\tfrom bokeh.plotting import show,output_notebook\n",
    "\tfrom bokeh.palettes import d3\n",
    "\timport bokeh.models as bmo\n",
    "\n",
    "\toutput_notebook()\n",
    "\tplot_tfidf=bp.figure(plot_width=800, plot_height=800, title=graphTitle,\n",
    "\t\t\t\t\t\t tools=\"pan,wheel_zoom,box_zoom,reset,hover,previewsave\",\n",
    "\t\t\t\t\t\t x_axis_type=None, y_axis_type=None, min_border=1)\n",
    "\n",
    "\tnumCats=len(tfidf2dDataFrame['category'].unique())\n",
    "\tpalette=d3['Category20'][numCats]\n",
    "\tcolor_map=bmo.CategoricalColorMapper(factors=tfidf2dDataFrame['category'].map(str).unique(), palette=palette)\n",
    "\n",
    "\tplot_tfidf.scatter(x='x', y='y', color={'field': 'category', 'transform': color_map}, \n",
    "\t\t\t\t\t\tlegend='category',source=tfidf2dDataFrame)\n",
    "\thover=plot_tfidf.select(dict(type=HoverTool))\n",
    "\tplot_tfidf.legend.click_policy=\"hide\"\n",
    "\thover.tooltips={\"id\": \"@id\", \"publication\": \"@publication\", \"content\":\"@content\", \"category\":\"@category\"}\n",
    "\n",
    "\tshow(plot_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the graph\n",
    "Each dot on the scattergraph corresponds to an article.\n",
    "Most of the stories covered that day (and available in the dataset) are actually not represented in more than one publication. So many articles are effectively their own unique story - these are indicated by NA.\n",
    "There is a particularly large cluster of these around the center of the graph. This appears to be an artefact of the SVD - the articles don't contain strong distinctive terms, and hence have small values on both axes. (this is likely party a result of the min DF being set to 2 in the example)\n",
    "\n",
    "To view the details of any story, hover over the corresponding dot.\n",
    "\n",
    "Considerably more detail and analysis is provided in the project report document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-35-a722bb9222bb>:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tfidf2dDataFrame['category'][article]=story\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1035\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1035\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error() {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < css_urls.length; i++) {\n",
       "      var url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.1.1.min.js\": \"kLr4fYcqcSpbuI95brIH3vnnYCquzzSxHPU6XGQCIkQRGJwhg0StNbj1eegrHs12\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.1.1.min.js\": \"xIGPmVtaOm+z0BqfSOMn4lOR6ciex448GIKG4eE61LsAvmGj48XcMQZtKcE/UXZe\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.1.1.min.js\": \"Dc9u1wF/0zApGIWoBbH77iWEHtdmkuYWG839Uzmv8y8yBLXebjO9ZnERsde5Ln/P\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.1.1.min.js\": \"cT9JaBz7GiRXdENrJLZNSC6eMNF3nh3fa5fTF51Svp+ukxPdwcU5kGXGPBgDCa2j\"};\n",
       "\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      if (url in hashes) {\n",
       "        element.crossOrigin = \"anonymous\";\n",
       "        element.integrity = \"sha384-\" + hashes[url];\n",
       "      }\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  \n",
       "  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.1.1.min.js\"];\n",
       "  var css_urls = [];\n",
       "  \n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "    \n",
       "    \n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      \n",
       "    for (var i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "    if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1035\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1035\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.1.1.min.js\": \"kLr4fYcqcSpbuI95brIH3vnnYCquzzSxHPU6XGQCIkQRGJwhg0StNbj1eegrHs12\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.1.1.min.js\": \"xIGPmVtaOm+z0BqfSOMn4lOR6ciex448GIKG4eE61LsAvmGj48XcMQZtKcE/UXZe\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.1.1.min.js\": \"Dc9u1wF/0zApGIWoBbH77iWEHtdmkuYWG839Uzmv8y8yBLXebjO9ZnERsde5Ln/P\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.1.1.min.js\": \"cT9JaBz7GiRXdENrJLZNSC6eMNF3nh3fa5fTF51Svp+ukxPdwcU5kGXGPBgDCa2j\"};\n\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      if (url in hashes) {\n        element.crossOrigin = \"anonymous\";\n        element.integrity = \"sha384-\" + hashes[url];\n      }\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  \n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.1.1.min.js\"];\n  var css_urls = [];\n  \n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {\n    \n    \n    }\n  ];\n\n  function run_inline_js() {\n    \n    if (root.Bokeh !== undefined || force === true) {\n      \n    for (var i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n    if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1035\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "unexpected tool name 'previewsave', possible tools are pan, xpan, ypan, xwheel_pan, ywheel_pan, wheel_zoom, xwheel_zoom, ywheel_zoom, zoom_in, xzoom_in, yzoom_in, zoom_out, xzoom_out, yzoom_out, click, tap, crosshair, box_select, xbox_select, ybox_select, poly_select, lasso_select, box_zoom, xbox_zoom, ybox_zoom, save, undo, redo, reset, help, box_edit, line_edit, point_draw, poly_draw, poly_edit or hover",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-fbb772652301>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m graphVectorSpace(tfidfVectors,\n\u001b[0m\u001b[0;32m      2\u001b[0m                                  \u001b[0marticleDataFrame\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'publication'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'content no nonascii'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                                  \u001b[0mrunParams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'process_date'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                                  \u001b[0mstoryMap\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \t\t\t\t threshold)\n",
      "\u001b[1;32m<ipython-input-35-a722bb9222bb>\u001b[0m in \u001b[0;36mgraphVectorSpace\u001b[1;34m(tfidfVectors, extraColumns, dateForTitle, storyMap, threshold)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[0moutput_notebook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m \tplot_tfidf=bp.figure(plot_width=800, plot_height=800, title=graphTitle,\n\u001b[0m\u001b[0;32m     56\u001b[0m                                                  \u001b[0mtools\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"pan,wheel_zoom,box_zoom,reset,hover,previewsave\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \t\t\t\t\t\t x_axis_type=None, y_axis_type=None, min_border=1)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\bokeh\\plotting\\figure.py\u001b[0m in \u001b[0;36mfigure\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1531\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1532\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mFigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1533\u001b[0m \u001b[0mfigure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFigure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1534\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\bokeh\\plotting\\figure.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, *arg, **kw)\u001b[0m\n\u001b[0;32m    174\u001b[0m         \u001b[0mprocess_axis_and_grid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_axis_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_axis_location\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_minor_ticks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_axis_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_range\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 176\u001b[1;33m         \u001b[0mtool_objs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtool_map\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess_tools_arg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtooltips\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    177\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_tools\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtool_objs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[0mprocess_active_tools\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoolbar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtool_map\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactive_drag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactive_inspect\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactive_scroll\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactive_tap\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\bokeh\\plotting\\_tools.py\u001b[0m in \u001b[0;36mprocess_tools_arg\u001b[1;34m(plot, tools, tooltips)\u001b[0m\n\u001b[0;32m    126\u001b[0m         \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mTools\u001b[0m \u001b[0mobjects\u001b[0m \u001b[0madded\u001b[0m \u001b[0mto\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap\u001b[0m \u001b[0mof\u001b[0m \u001b[0msupplied\u001b[0m \u001b[0mstring\u001b[0m \u001b[0mnames\u001b[0m \u001b[0mto\u001b[0m \u001b[0mtools\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m     \"\"\"\n\u001b[1;32m--> 128\u001b[1;33m     \u001b[0mtool_objs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtool_map\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_resolve_tools\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtools\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[0mrepeated_tools\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_collect_repeated_tools\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtool_objs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\bokeh\\plotting\\_tools.py\u001b[0m in \u001b[0;36m_resolve_tools\u001b[1;34m(tools)\u001b[0m\n\u001b[0;32m    166\u001b[0m             \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 168\u001b[1;33m         \u001b[0mtool_obj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTool\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtool\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    169\u001b[0m         \u001b[0mtool_objs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtool_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m         \u001b[0mtool_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtool\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtool_obj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\bokeh\\models\\tools.py\u001b[0m in \u001b[0;36mfrom_string\u001b[1;34m(cls, name)\u001b[0m\n\u001b[0;32m    160\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmatches\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m                 \u001b[0mmatches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mknown_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"possible\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"unexpected tool name '{name}', {text} tools are {nice_join(matches)}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: unexpected tool name 'previewsave', possible tools are pan, xpan, ypan, xwheel_pan, ywheel_pan, wheel_zoom, xwheel_zoom, ywheel_zoom, zoom_in, xzoom_in, yzoom_in, zoom_out, xzoom_out, yzoom_out, click, tap, crosshair, box_select, xbox_select, ybox_select, poly_select, lasso_select, box_zoom, xbox_zoom, ybox_zoom, save, undo, redo, reset, help, box_edit, line_edit, point_draw, poly_draw, poly_edit or hover"
     ]
    }
   ],
   "source": [
    "graphVectorSpace(tfidfVectors,\n",
    "\t\t\t\t articleDataFrame[['id','publication','content no nonascii']],\n",
    "\t\t\t\t runParams['process_date'],\n",
    "\t\t\t\t storyMap,\n",
    "\t\t\t\t threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the avoidance of doubt - recall that the story names in the legend (Safe space, Trump meeting, etc) are taken directly from the input story map titles. These names are not inferred from the data! (just the groupings are inferred)\n",
    "### Investigate inferred clustering vs given story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produceRequestedReportDetails(tfidfVectors,articleDataFrame,reportArticleList,threshold,storyMap,terms):\n",
    "\n",
    "\t# tfidfVectors is a sparse matrix, for efficiency it's useful to determine once only which\n",
    "\t# coordinates have non-zero values\n",
    "\tnonZeroCoords=initialiseAllNonZeroCoords(tfidfVectors)\n",
    "\n",
    "\ttopNwords=25\n",
    "\n",
    "\t# Create list of articles to process\n",
    "\t# If a list is provided in command line arguments, use that\n",
    "\tstoryMapGood=0.0\n",
    "\tencounteredStoriesList=[]\n",
    "\tfor index,row in articleDataFrame.iterrows():\n",
    "\t\tif row['id'] in reportArticleList:\n",
    "\t\t\tref_index=index\n",
    "\t\t\tprint(\"-----\")\n",
    "\t\t\tprint(\"-----\")\n",
    "\t\t\tprint(\"LEAD ARTICLE IN STORY:\",row['id'])\n",
    "\t\t\tprint(\"-----\")\n",
    "\n",
    "\t\t\tif threshold==None:\n",
    "\t\t\t\tarticleIndexList=[index]\n",
    "\t\t\telse:\n",
    "\t\t\t\t# Score and rank all articles relative to this one\n",
    "\t\t\t\t# Count number of items that are greater than or equal to threshold\n",
    "\t\t\t\t# Then truncate the list beyond those items\n",
    "\t\t\t\tscores=productRelatednessScores(tfidfVectors,nonZeroCoords,ref_index)\n",
    "\t\t\t\trankedIndices=np.argsort(scores)\n",
    "\t\t\t\tnumItemsInRange=sum(x>=threshold for x in scores)\n",
    "\t\t\t\tarticleIndexList=rankedIndices[-numItemsInRange:]\n",
    "\n",
    "\t\t\t# If there is a story map, find out which story this article is meant to belong to\n",
    "\t\t\ttargetStory=None\n",
    "\t\t\tif storyMap!=None:\n",
    "\t\t\t\tfor story,articleList in storyMap.items():\n",
    "\t\t\t\t\tif row['id'] in articleList:\n",
    "\t\t\t\t\t\ttargetStory=story\n",
    "\t\t\t\t\t\ttargetArticleList=articleList\n",
    "\t\t\t\t\t\tencounteredStoriesList.append(targetStory)\n",
    "\t\t\t\t\t\n",
    "\t\t\t# For just those articles that are within threshold of the lead article\n",
    "\t\t\t# Print out the key terms and their tf-idf scores\n",
    "\t\t\t# Then count the number of articles that are correctly assigned to the story\n",
    "\t\t\t# (if there is a ground truth storyMap provided)\n",
    "\t\t\tfor article in reversed(articleIndexList):\n",
    "\t\t\t\tif targetStory!=None:\n",
    "\t\t\t\t\t# If this is officially part of the same story, update the counts\n",
    "\t\t\t\t\tif articleDataFrame['id'][article] in targetArticleList:\n",
    "\t\t\t\t\t\tstoryMapGood+=1.0\n",
    "\n",
    "\t\t\t\tprint(\"MEMBER ARTICLE:\",articleDataFrame['id'][article])\n",
    "\t\t\t\tif threshold!=None:\n",
    "\t\t\t\t\tprint(\"Score :\",scores[article])\n",
    "\t\t\t\tprint(articleDataFrame['publication'][article])\n",
    "\t\t\t\tprint(articleDataFrame['content'][article][:500])\n",
    "\t\t\t\tprint(\"PASSED TO VECTORIZER AS:\")\n",
    "\t\t\t\tprint(articleDataFrame['input to vectorizer'][article])\n",
    "\t\t\t\tprint()\n",
    "\t\t\t\tprintTopNwordsForArticle(tfidfVectors,terms,articleNum=article,n=topNwords)\n",
    "\t\t\t\tprint(\"-----\")\n",
    "\t\t\tprint(\"-----\")\n",
    "\n",
    "\t# If there is a storyMap, print out the percentage results for the inferred allocation\n",
    "\t# Note that it should be just relative to the number of stories actually encountered\n",
    "\t# So if the user requests a specific set of articles and those articles don't cover\n",
    "\t# the full set of stories, then they shouldn't be counted as errors.\n",
    "\tif storyMap!=None:\n",
    "\t\tstoryMapSize=sum([len(storyMap[story]) for story in encounteredStoriesList])\n",
    "\t\tprint(\"\\n\\nPERCENTAGE OF STORIES ALLOCATED IN LINE WITH MAP:\",100.*storyMapGood/storyMapSize)\n",
    "\n",
    "\treturn\n",
    "\n",
    "def printTopNwordsForArticle(tfidfVectors,terms,articleNum,n):\n",
    "\tvect=tfidfVectors[articleNum].toarray()[0]\n",
    "\ttopn1=np.argsort(vect)\n",
    "\tfor t in reversed(topn1[-n:]):\n",
    "\t\tif vect[t]>0.001:\n",
    "\t\t\tprint(terms[t],\":\",round(vect[t],5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering report for articles\n",
    "The output of the following cell is broken down as follows:\n",
    "- One section per story in the story map\n",
    "- The ID of the lead story (which is taken from the reportArticleList provided\n",
    "- For each article which has a pair score with this lead article greater than the threshold:\n",
    " - The ID of that related article\n",
    " - The score for the pair\n",
    " - The name of the publisher of the article\n",
    " - The first few lines of the actual article content\n",
    " - The derived corresponding text that is provided to the TF-IDF vectorizer (i.e. processed for lemmatization, parts-of-speech, stop words, etc)\n",
    " - The 25 most significant terms (in TF-IDF) in that article, along with that term's correspondong TF-IDF value for the article\n",
    "At the very end of the report is a line which indicates the percentage of articles which were allocated to the correct stories (according to the story map)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue with outputting from best results if requested\n",
    "produceRequestedReportDetails(tfidfVectors,articleDataFrame,reportArticleList,threshold,storyMap,terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
