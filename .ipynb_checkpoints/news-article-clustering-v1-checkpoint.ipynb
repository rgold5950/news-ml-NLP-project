{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Article counts by publisher:\n",
      "Breitbart           104\n",
      "NY Post              61\n",
      "CNN                  57\n",
      "Reuters              56\n",
      "NPR                  54\n",
      "Washington Post      50\n",
      "NY Times             50\n",
      "Atlantic             48\n",
      "Buzzfeed News        48\n",
      "Business Insider     41\n",
      "Guardian             35\n",
      "National Review      32\n",
      "Fox News             28\n",
      "Name: publication, dtype: int64\n",
      "\n",
      "Article counts by date:\n",
      "2016-12-02    362\n",
      "2016-09-01    302\n",
      "Name: date, dtype: int64\n",
      "\n",
      "Final dataset:\n",
      "\n",
      "Date: 2016-09-01 \n",
      "\n",
      "Breitbart           104\n",
      "NY Post              61\n",
      "CNN                  57\n",
      "Reuters              56\n",
      "NPR                  54\n",
      "Washington Post      50\n",
      "NY Times             50\n",
      "Atlantic             48\n",
      "Buzzfeed News        48\n",
      "Business Insider     41\n",
      "Guardian             35\n",
      "National Review      32\n",
      "Fox News             28\n",
      "Name: publication, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "##########################################################################################\n",
    "processDate = \"2016-09-01\"\n",
    "\n",
    "def removeNonASCIICharacters(textString): \n",
    "    return \"\".join(i for i in textString if ord(i)<128)\n",
    "\n",
    "df = pd.read_csv(r'C:\\Users\\goldm\\Capstone\\data\\articles.csv')\n",
    "df.drop_duplicates('content')\n",
    "df = df[~df['content'].isnull()]\n",
    "df=df[df['content'].str.len()>=200]\n",
    "\n",
    "targetString=\"(Want to get this briefing by email?\"\n",
    "df['NYT summary']=df['content'].map(lambda d: d[:len(targetString)]==targetString)\n",
    "df=df[df['NYT summary']==False]\n",
    "\n",
    "# The following removes a warning that appears in many of the Atlantic articles.\n",
    "# Since it is commonly at the beginning, it brings a lot of noise to the search for similar articles\n",
    "# And subsequently to the assessment of sentiment\n",
    "targetString=\"For us to continue writing great stories, we need to display ads.             Please select the extension that is blocking ads.     Please follow the steps below\"\n",
    "df['content']=df['content'].str.replace(targetString,'')\n",
    "\n",
    "# This is also for some Atlantic articles for the same reasons as above\n",
    "targetString=\"This article is part of a feature we also send out via email as The Atlantic Daily, a newsletter with stories, ideas, and images from The Atlantic, written specially for subscribers. To sign up, please enter your email address in the field provided here.\"\n",
    "df=df[df['content'].str.contains(targetString)==False]\n",
    "\n",
    "# This is also for some Atlantic articles for the same reasons as above\n",
    "targetString=\"This article is part of a feature we also send out via email as Politics  Policy Daily, a daily roundup of events and ideas in American politics written specially for newsletter subscribers. To sign up, please enter your email address in the field provided here.\"\n",
    "df=df[df['content'].str.contains(targetString)==False]\n",
    "\n",
    "# More Atlantic-specific removals (for daily summaries with multiple stories contained)\n",
    "df=df[df['content'].str.contains(\"To sign up, please enter your email address in the field\")==False]\n",
    "\n",
    "# Remove daily CNN summary\n",
    "targetString=\"CNN Student News\"\n",
    "df=df[df['content'].str.contains(targetString)==False]\n",
    "\n",
    "print(\"\\nArticle counts by publisher:\")\n",
    "print(df['publication'].value_counts())\n",
    "\n",
    "print(\"\\nArticle counts by date:\")\n",
    "print(df['date'].value_counts())\n",
    "\n",
    "# Restrict to articles on the provided input date.\n",
    "# This date is considered mandatory for topic clustering but is not required for sentiment\n",
    "# since sentiment only processes a specified list of articles.\n",
    "# For topic clustering it is essential to have the date as it is\n",
    "# enormously significant in article matching.\n",
    "# if processDate!=None:\n",
    "#     df=df[df['date']==processDate]\n",
    "# df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Remove non-ASCII characters\n",
    "df['content no nonascii']=df['content'].map(lambda x: removeNonASCIICharacters(x))\n",
    "\n",
    "print(\"\\nFinal dataset:\\n\\nDate:\",processDate,\"\\n\")\n",
    "print(df['publication'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_df = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadStopWords(stopWordsFileName):\n",
    "    stop_words=[]\n",
    "    f=open(stopWordsFileName,'r')\n",
    "    for l in f.readlines():\n",
    "        stop_words.append(l.replace('\\n', ''))\n",
    "    return stop_words\n",
    "stop_words = loadStopWords(runParams['stop_words_file'][0])\n",
    "#NOTE: alternative to importing a flat file of stop words is to just import stop words from the various different libraries. \n",
    "# from nltk.corpus import stopwords\n",
    "# stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NNP', 'NNPS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import csv\n",
    "\n",
    "runParams={'tfidf_maxdf':      [0.5],\n",
    "           'input_file':       ['./data/articles.csv'],\n",
    "           'story_threshold':  [0.26],\n",
    "           'process_date':     ['2016-09-01'],\n",
    "           'parts_of_speech':  [['PROPER', 'VERB']],\n",
    "           'lemma_conversion': [False],\n",
    "           'ngram_max':        [3],\n",
    "           'tfidf_binary':     [False],\n",
    "           'tfidf_norm':       ['l2'],\n",
    "           'nlp_library':      ['nltk'],\n",
    "           'max_length':       [50],\n",
    "           'stop_words_file':  ['./data/stopWords.txt'],\n",
    "           'tfidf_mindf':      [2],\n",
    "           'display_graph':    [True],\n",
    "           'article_stats':    [False]}\n",
    "# Use parameter grid even if there is only one set of parameters\n",
    "parameterGrid = ParameterGrid(runParams)\n",
    "\n",
    "partsOfSpeech=[]\n",
    "pos_nlp_mapping = {}\n",
    "pos_nlp_mapping['nltk']={'VERB':['VB','VBD','VBG','VBN','VBP','VBZ'],'PROPER':['NNP','NNPS'],'COMMON':['NN','NNS']}\n",
    "\n",
    "for pos in runParams['parts_of_speech'][0]:\n",
    "    partsOfSpeech.append(pos_nlp_mapping['nltk'][pos])\n",
    "partsOfSpeech=[item for sublist in partsOfSpeech for item in sublist]\n",
    "print(partsOfSpeech)\n",
    "\n",
    "import nltk as nl\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "stringToConvert = article_df['content']\n",
    "partsOfSpeech = partsOfSpeech\n",
    "stop_words = stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk as nl\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "def stringNLTKProcess(nl, stringToConvert,partsOfSpeech,stop_words,maxWords=None,lemmatizer=None):\n",
    "    #parses the paragraph into sentences\n",
    "    sentences = nl.sent_tokenize(stringToConvert)\n",
    "    str = []\n",
    "    for sentence in sentences:\n",
    "        wordString=[]\n",
    "        for word, pos in nl.pos_tag(nl.word_tokenize(sentence)):\n",
    "            # The following condition avoids any POS which corresponds to punctuation (and takes all others\n",
    "            if partsOfSpeech == None:\n",
    "                if pos[0]>='A' and pos[0]<='Z':\n",
    "                    wordString.append(word)\n",
    "            elif pos in partsOfSpeech:\n",
    "                wordString.append(word)\n",
    "        for wrd in wordString:\n",
    "            #converts all string characters into lowercase elements\n",
    "            wrdlower=wrd.lower()\n",
    "            if wrdlower not in stop_words and wrdlower!=\"'s'\":\n",
    "                if maxWords==None or len(str)<maxWords:\n",
    "                    if lemmatizer==None:\n",
    "                        str.append(wrdlower)\n",
    "                    else:\n",
    "                        str.append(lemmatizer.lemmatize(wrd.lower(), pos='v'))\n",
    "            if maxWords!=None and len(str)==maxWords:\n",
    "                return ' '.join(str)\n",
    "    return ' '.join(str)\n",
    "\n",
    "def removeSpacesAndPunctuation(textString): \n",
    "    return \"\".join(i for i in textString if (ord(i)>=48 and ord(i)<=57) or (ord(i)>=97 and ord(i)<=122))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_df['input to vectorizer'] = article_df['content no nonascii'].map(lambda x: stringNLTKProcess(nl, x,partsOfSpeech,stop_words,maxWords=None,lemmatizer=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessAndVectorize(articleDataFrame, args, pos_nlp_mapping, nlp,nl, wordnet_lemmatizer,stop_words):\n",
    "    vectorizer = TfidVectorizer(analyzer='word' , ngram_range=(1,))\n",
    "    \n",
    "    vectorizer = TfidVectorizer(analyzer='word',\n",
    "                                ngram_range=(1,args['ngram_max'][0]),\n",
    "                                lowercase=True,\n",
    "                                binary=args['tfidf_binary'][0],\n",
    "                                **optArgsForVectorizer)\n",
    "    tfidVectors=vectorizer.fit_transform(articleDataFrame['input to vectorizer'])\n",
    "    terms=vectorizer.get_feature_names()\n",
    "    return tfidVectors, terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialiseAllNonZeroCoords(tfidVectors):\n",
    "    #This function just exists isnce it seems to be expensive and I'd rather not call it multiple times\n",
    "    #Hence it is intended to be called outside of loops in order to simplify the row specific processing\n",
    "    values=[]\n",
    "    nzc=zip(*tfidVectors.nonzero())\n",
    "    \n",
    "    #In Python 3 the zip can only be iterated through one time before it is automatically realeased\n",
    "    ## So need to copy the results otherwise the main loop below will no longer work\n",
    "    pointList=[]\n",
    "    for i,j in nzc:\n",
    "        pointList.append([i,j])\n",
    "        \n",
    "    for row in range(tfidVectors.shape[0]):\n",
    "        rowList=[]\n",
    "        for i,j in pointList:\n",
    "            if row==i:\n",
    "                rowList.append(j)\n",
    "        values.append(rowList)\n",
    "        \n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoreCurrentParamGuess(tfidVectors,storyMap,articleDataFrame,threshold,printErrors=False):\n",
    "    #Work with distances relative to first item in each cluster - even though this is clearly arbitrary since that\n",
    "    #point could be an outlier in the cluster and hence might cause some problems.\n",
    "    #But I have to start somewhere - and can refine it later if needed.\n",
    "    \n",
    "    nonZeroCoords=initialiseAllNonZeroCoords(tfidVectors)\n",
    "    score=0\n",
    "    outGood=0\n",
    "    outBad=0\n",
    "    inGood=0\n",
    "    inBad=0\n",
    "    for story, storyArticles in storyMap.items():\n",
    "        leadArticleIndex=articleDataFrame[articleDataFrame['id']==storyArticles[0].index[0]]\n",
    "        #Compute score of all articles in corpus relative to the first article in the story ( dot product)\n",
    "        #Then count through the list relative to the threshold (add one for a good result, subtract one for a bad result)\n",
    "        scores=productRelatednessScores(tfidfVectors,nonZeroCoords,leadArticleIndex)\n",
    "        rankedIndices=np.argsort(scores)\n",
    "        foundRelatedArticles=[]\n",
    "        # The sorting here is not strictly required,ubt i could use it so that once the threshold is passed\n",
    "        # in the loop, then i infer the remaining results\n",
    "        for article in reversed(rankedIndices):\n",
    "            thisArticleIndex=articleDataFrame['id'][article]\n",
    "            if thisArticleIndex in storyArticles:\n",
    "                if scores[article]>=threshold:\n",
    "                    score+=1\n",
    "                    inGood+=1\n",
    "                else:\n",
    "                    score-=1\n",
    "                    inBad+=1\n",
    "                    if printErrors:\n",
    "                        print(\"ERROR:\",thisArticleIndex,\"should be in\", story)\n",
    "            else: # article not supposed to be in range\n",
    "                if scores[article]<=threshold:\n",
    "                    score+=1\n",
    "                    outGood+=1\n",
    "                else:\n",
    "                    score-=1\n",
    "                    outBad+=1\n",
    "                    if printErrors:\n",
    "                        print(\"ERROR:\", thisArticleIndex,\"should NOT be in\", story)\n",
    "        scoreDict={'score':score, 'inGood': inGood, 'inBad': inBad, \"outGood\": outGood, 'outBad': outBad}\n",
    "        return scoreDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop across all parameter combinatoins in grid to determine best set\n",
    "# If not doing grid search, will just pass through the loop once\n",
    "bestParamScoreDict={'score':-1000000}\n",
    "betsParams=parameterGrid[0]\n",
    "\n",
    "for i,currentParams in enumerate(parameterGrid):\n",
    "    if len(parameterGrid)>1:\n",
    "        print(\"Combination:\", i+1, \"of\", len(parameterGrid))\n",
    "        print(currentParams)\n",
    "        \n",
    "    # Determine tf-idf vectors\n",
    "    # terms is just used later on if analysis of final results is requested\n",
    "    \n",
    "    tfidVectors, terms=proprocessAndVectorize(articleDataFrame,\n",
    "                                             currentParams,\n",
    "                                             pos_nlp_mapping,\n",
    "                                             nlp,\n",
    "                                             nl,\n",
    "                                             wordnet_lemmatizer,\n",
    "                                             stop_words)\n",
    "    \n",
    "    # Computes scores if threshold provided (meaning as part of grid search)\n",
    "    if 'story_threshold' in currentParams and currentParams['story_threshold']!=None:\n",
    "        scoreDict=scoreCurrentParamGuess(tfidVectors,storyMap,articleDataFrame,currentParams['story_threshold'])\n",
    "        print(scoreDict)\n",
    "        \n",
    "        #Update best so far\n",
    "        if scoreDict['score']>=bestParamScoreDict['score']:\n",
    "            if len(parameterGrid)>1:\n",
    "                print(i+1,\"is the best so far!\")\n",
    "            bestParams = currentParams\n",
    "            bestParamScoreDict = scoreDict\n",
    "    # End grid/parameter loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop across all parameter combinations in grid to determine best set\n",
    "# If not doing grid search, will just pass through the loop once\n",
    "\n",
    "bestParamScoreDict={'score':-1000000}\n",
    "bestParams=parameterGrid[0]\n",
    "\n",
    "for i,currentParams in enumerate(parameterGrid):\n",
    "\tif len(parameterGrid)>1:\n",
    "\t\tprint(\"Combination:\",i+1,\"of\",len(parameterGrid))\n",
    "\t\tprint(currentParams)\n",
    "\n",
    "\t# Determine tf-idf vectors\n",
    "\t# terms is just used later on if analysis of final results is requested\n",
    "\ttfidfVectors,terms=preprocessAndVectorize(articleDataFrame,\n",
    "\t\t\t\t\t\t\t\t\t\t\t  currentParams,\n",
    "\t\t\t\t\t\t\t\t\t\t\t  pos_nlp_mapping,\n",
    "\t\t\t\t\t\t\t\t\t\t\t  nlp,\n",
    "\t\t\t\t\t\t\t\t\t\t\t  nl,\n",
    "\t\t\t\t\t\t\t\t\t\t\t  wordnet_lemmatizer,\n",
    "\t\t\t\t\t\t\t\t\t\t\t  stop_words)\n",
    "\n",
    "\t# Compute scores if threshold provided (meaning as part of grid search)\n",
    "\tif 'story_threshold' in currentParams and currentParams['story_threshold']!=None:\n",
    "\t\tscoreDict=scoreCurrentParamGuess(tfidfVectors,storyMap,articleDataFrame,currentParams['story_threshold'])\n",
    "\t\tprint(scoreDict)\n",
    "\n",
    "\t\t# Update best so far\n",
    "\t\tif scoreDict['score']>=bestParamScoreDict['score']:\n",
    "\t\t\tif len(parameterGrid)>1:\n",
    "\t\t\t\tprint(i+1,\"is the best so far!\")\n",
    "\t\t\tbestParams=currentParams\n",
    "\t\t\tbestParamScoreDict=scoreDict\n",
    "\t# End grid/parameter loop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
