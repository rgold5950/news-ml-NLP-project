{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Monticello wasn't designated as UNESCO World Heritage Site until 1987\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Monticello',\n",
       " \"wasn't\",\n",
       " 'designated',\n",
       " 'as',\n",
       " 'UNESCO',\n",
       " 'World',\n",
       " 'Heritage',\n",
       " 'Site',\n",
       " 'until',\n",
       " '1987']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import regex\n",
    "regex.split(\"[\\s\\.\\,]\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating better tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Monticello',\n",
       " 'was',\n",
       " \"n't\",\n",
       " 'designated',\n",
       " 'as',\n",
       " 'UNESCO',\n",
       " 'World',\n",
       " 'Heritage',\n",
       " 'Site',\n",
       " 'until',\n",
       " '1987']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Better Stemmers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caress >>> caress\n",
      "flies >>> fli\n",
      "dies >>> die\n",
      "mules >>> mule\n",
      "denied >>> deni\n",
      "died >>> die\n",
      "agreed >>> agre\n",
      "owned >>> own\n",
      "humbled >>> humbl\n",
      "sized >>> size\n",
      "itemization >>> item\n",
      " siezing >>>  siez\n",
      "sensational >>> sensat\n",
      "traditional >>> tradit\n",
      "reference >>> refer\n",
      "colonizer >>> colon\n",
      "plotted >>> plot\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "plurals = ['caress', \"flies\", \"dies\", \"mules\", \"denied\", \"died\" , \"agreed\" , \"owned\", \"humbled\", \"sized\", \"itemization\", \" siezing\", \"sensational\" , \"traditional\" , \"reference\" , \"colonizer\" , \"plotted\" ]\n",
    "\n",
    "for word in plurals:\n",
    "    print(f\"{word} >>> {stemmer.stem(word)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('arabic',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'hungarian',\n",
       " 'italian',\n",
       " 'norwegian',\n",
       " 'porter',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'spanish',\n",
       " 'swedish')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "SnowballStemmer.languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn_stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'generous'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sn_stemmer.stem(\"generously\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gener'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(\"generously\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caress >>> caress\n",
      "caress >>> caress\n",
      "flies >>> fly\n",
      "flies >>> fly\n",
      "dies >>> die\n",
      "dies >>> dy\n",
      "mules >>> mules\n",
      "mules >>> mule\n",
      "denied >>> deny\n",
      "denied >>> denied\n",
      "died >>> die\n",
      "died >>> died\n",
      "agreed >>> agree\n",
      "agreed >>> agreed\n",
      "owned >>> own\n",
      "owned >>> owned\n",
      "humbled >>> humble\n",
      "humbled >>> humbled\n",
      "sized >>> size\n",
      "sized >>> sized\n",
      "itemization >>> itemization\n",
      "itemization >>> itemization\n",
      " siezing >>>  siezing\n",
      " siezing >>>  siezing\n",
      "sensational >>> sensational\n",
      "sensational >>> sensational\n",
      "traditional >>> traditional\n",
      "traditional >>> traditional\n",
      "reference >>> reference\n",
      "reference >>> reference\n",
      "colonizer >>> colonizer\n",
      "colonizer >>> colonizer\n",
      "plotted >>> plot\n",
      "plotted >>> plotted\n"
     ]
    }
   ],
   "source": [
    "for word in plurals:\n",
    "    print(f\"{word} >>> {lemmatizer.lemmatize(word, pos= 'v') }\")\n",
    "    print(f\"{word} >>> {lemmatizer.lemmatize(word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing stop words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\goldm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'isn', 'ma', 'he', 'so', 'theirs', \"weren't\", 'herself', 'mustn', 'nor', 'of', 'had', 'or', 're', 'down', \"couldn't\", 'which', 'itself', 'here', 'his', 'now', 'at', 'are', 'above', 'then', 'll', \"won't\", 's', 'this', 'off', 'while', 'few', 'my', 'mightn', 'against', 'doing', 'further', \"didn't\", 'when', \"needn't\", 'each', 'himself', \"you'd\", 'm', 'been', 'those', \"doesn't\", 'about', 'is', 'the', 'some', 'why', \"hadn't\", 'y', 'do', 'until', 'myself', 'themselves', 'can', 't', 'by', 'aren', \"aren't\", 'wasn', 'have', 'don', 'should', 'yourself', 'very', 'won', 'am', 'having', 'an', 'they', 'such', 'weren', 'but', \"you'll\", \"should've\", 'were', 'haven', 'has', 'be', 'what', 'couldn', 'didn', 'other', 'yourselves', 'after', 'only', 'd', 'not', \"mustn't\", \"wasn't\", 'same', \"wouldn't\", 'below', 'ours', 'does', 'from', 'i', 'hers', 'shouldn', 'into', 'there', 'more', 'out', \"hasn't\", 'me', 'up', 'and', 'that', 'both', \"don't\", \"you're\", 'through', \"it's\", \"you've\", 'a', 'than', 'again', 'she', 'because', 'these', 'on', 'you', 'with', 'we', 've', 'under', 'shan', 'all', 'over', 'where', 'being', 'during', 'most', \"haven't\", 'whom', \"isn't\", 'no', 'was', 'it', 'who', 'once', 'how', 'doesn', 'for', 'to', \"mightn't\", 'just', 'between', 'her', 'yours', 'hadn', 'o', 'its', 'before', 'ain', 'own', 'did', 'any', 'if', 'our', \"that'll\", 'too', 'needn', 'their', \"shouldn't\", 'will', 'wouldn', 'hasn', 'as', 'in', \"shan't\", 'ourselves', 'them', 'your', 'him', \"she's\"}\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Welcome you to programming knowledge. Lets start with our first tutorial on NLTK. We shall learn the basics of NLTK here.\"\"\"\n",
    "\n",
    "demoWords = [\"playing\", \"happiness\" , \"going\" , \"doing\" , \"yes\" , \" no\" , \"I\" , \"having\", \"had\" , \"haved\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Welcome', 'you', 'to', 'programming', 'knowledge', '.', 'Lets', 'start', 'with', 'our', 'first', 'tutorial', 'on', 'NLTK', '.', 'We', 'shall', 'learn', 'the', 'basics', 'of', 'NLTK', 'here', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "tokenize_words = word_tokenize(text)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Welcome', 'you', 'to', 'programming', 'knowledge', '.', 'Lets', 'start', 'with', 'our', 'first', 'tutorial', 'on', 'NLTK', '.', 'We', 'shall', 'learn', 'the', 'basics', 'of', 'NLTK', 'here', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "tokenize_words = sent_tokenize(text)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learn', 'NLTK', 'knowledge', 'shall', 'programming', 'Welcome', 'Lets', 'start', 'tutorial', 'basics', '.', 'We', 'first'}\n",
      "{'the', 'you', 'here', 'with', 'of', 'on', 'to', 'our'}\n"
     ]
    }
   ],
   "source": [
    "tokenize_words_without_stop_words = []\n",
    "for word in tokenize_words:\n",
    "    if word not in stop_words:\n",
    "        tokenize_words_without_stop_words.append(word)\n",
    "print(set(tokenize_words_without_stop_words))\n",
    "\n",
    "removed_words = set(tokenize_words) - set(tokenize_words_without_stop_words)\n",
    "print(removed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Welcome', 'JJ'),\n",
       " ('programming', 'NN'),\n",
       " ('knowledge', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Lets', 'NNP'),\n",
       " ('start', 'VBP'),\n",
       " ('first', 'JJ'),\n",
       " ('tutorial', 'JJ'),\n",
       " ('NLTK', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('We', 'PRP'),\n",
       " ('shall', 'MD'),\n",
       " ('learn', 'VB'),\n",
       " ('basics', 'NNS'),\n",
       " ('NLTK', 'NNP'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##testing lemmatization of words\n",
    "nltk.pos_tag(tokenize_words_without_stop_words)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# WordNet POS tags are: NOUN = 'n', ADJ = 's', VERB = 'v', ADV = 'r', ADJ_SAT = 'a'\n",
    "# Descriptions (c) https://web.stanford.edu/~jurafsky/slp3/10.pdf\n",
    "tag_map = {\n",
    "        'CC':None, # coordin. conjunction (and, but, or)  \n",
    "        'CD':wn.NOUN, # cardinal number (one, two)             \n",
    "        'DT':None, # determiner (a, the)                    \n",
    "        'EX':wn.ADV, # existential ‘there’ (there)           \n",
    "        'FW':None, # foreign word (mea culpa)             \n",
    "        'IN':wn.ADV, # preposition/sub-conj (of, in, by)   \n",
    "        'JJ':[wn.ADJ, wn.ADJ_SAT], # adjective (yellow)                  \n",
    "        'JJR':[wn.ADJ, wn.ADJ_SAT], # adj., comparative (bigger)          \n",
    "        'JJS':[wn.ADJ, wn.ADJ_SAT], # adj., superlative (wildest)           \n",
    "        'LS':None, # list item marker (1, 2, One)          \n",
    "        'MD':None, # modal (can, should)                    \n",
    "        'NN':wn.NOUN, # noun, sing. or mass (llama)          \n",
    "        'NNS':wn.NOUN, # noun, plural (llamas)                  \n",
    "        'NNP':wn.NOUN, # proper noun, sing. (IBM)              \n",
    "        'NNPS':wn.NOUN, # proper noun, plural (Carolinas)\n",
    "        'PDT':[wn.ADJ, wn.ADJ_SAT], # predeterminer (all, both)            \n",
    "        'POS':None, # possessive ending (’s )               \n",
    "        'PRP':None, # personal pronoun (I, you, he)     \n",
    "        'PRP$':None, # possessive pronoun (your, one’s)    \n",
    "        'RB':wn.ADV, # adverb (quickly, never)            \n",
    "        'RBR':wn.ADV, # adverb, comparative (faster)        \n",
    "        'RBS':wn.ADV, # adverb, superlative (fastest)     \n",
    "        'RP':[wn.ADJ, wn.ADJ_SAT], # particle (up, off)\n",
    "        'SYM':None, # symbol (+,%, &)\n",
    "        'TO':None, # “to” (to)\n",
    "        'UH':None, # interjection (ah, oops)\n",
    "        'VB':wn.VERB, # verb base form (eat)\n",
    "        'VBD':wn.VERB, # verb past tense (ate)\n",
    "        'VBG':wn.VERB, # verb gerund (eating)\n",
    "        'VBN':wn.VERB, # verb past participle (eaten)\n",
    "        'VBP':wn.VERB, # verb non-3sg pres (eat)\n",
    "        'VBZ':wn.VERB, # verb 3sg pres (eats)\n",
    "        'WDT':None, # wh-determiner (which, that)\n",
    "        'WP':None, # wh-pronoun (what, who)\n",
    "        'WP$':None, # possessive (wh- whose)\n",
    "        'WRB':None, # wh-adverb (how, where)\n",
    "        '$':None, #  dollar sign ($)\n",
    "        '#':None, # pound sign (#)\n",
    "        '“':None, # left quote (‘ or “)\n",
    "        '”':None, # right quote (’ or ”)\n",
    "        '(':None, # left parenthesis ([, (, {, <)\n",
    "        ')':None, # right parenthesis (], ), }, >)\n",
    "        ',':None, # comma (,)\n",
    "        '.':None, # sentence-final punc (. ! ?)\n",
    "        ':':None # mid-sentence punc (: ; ... – -)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Article counts by publisher:\n",
      "Breitbart           104\n",
      "NY Post              61\n",
      "CNN                  57\n",
      "Reuters              56\n",
      "NPR                  54\n",
      "NY Times             50\n",
      "Washington Post      50\n",
      "Buzzfeed News        48\n",
      "Atlantic             48\n",
      "Business Insider     41\n",
      "Guardian             35\n",
      "National Review      32\n",
      "Fox News             28\n",
      "Name: publication, dtype: int64\n",
      "\n",
      "Article counts by date:\n",
      "2016-12-02    362\n",
      "2016-09-01    302\n",
      "Name: date, dtype: int64\n",
      "\n",
      "Final dataset:\n",
      "\n",
      "Date: 2016-09-01 \n",
      "\n",
      "Breitbart           104\n",
      "NY Post              61\n",
      "CNN                  57\n",
      "Reuters              56\n",
      "NPR                  54\n",
      "NY Times             50\n",
      "Washington Post      50\n",
      "Buzzfeed News        48\n",
      "Atlantic             48\n",
      "Business Insider     41\n",
      "Guardian             35\n",
      "National Review      32\n",
      "Fox News             28\n",
      "Name: publication, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "##########################################################################################\n",
    "processDate = \"2016-09-01\"\n",
    "\n",
    "def removeNonASCIICharacters(textString): \n",
    "    return \"\".join(i for i in textString if ord(i)<128)\n",
    "\n",
    "df = pd.read_csv(r'C:\\Users\\goldm\\Capstone\\data\\articles.csv')\n",
    "df.drop_duplicates('content')\n",
    "df = df[~df['content'].isnull()]\n",
    "df=df[df['content'].str.len()>=200]\n",
    "\n",
    "targetString=\"(Want to get this briefing by email?\"\n",
    "df['NYT summary']=df['content'].map(lambda d: d[:len(targetString)]==targetString)\n",
    "df=df[df['NYT summary']==False]\n",
    "\n",
    "# The following removes a warning that appears in many of the Atlantic articles.\n",
    "# Since it is commonly at the beginning, it brings a lot of noise to the search for similar articles\n",
    "# And subsequently to the assessment of sentiment\n",
    "targetString=\"For us to continue writing great stories, we need to display ads.             Please select the extension that is blocking ads.     Please follow the steps below\"\n",
    "df['content']=df['content'].str.replace(targetString,'')\n",
    "\n",
    "# This is also for some Atlantic articles for the same reasons as above\n",
    "targetString=\"This article is part of a feature we also send out via email as The Atlantic Daily, a newsletter with stories, ideas, and images from The Atlantic, written specially for subscribers. To sign up, please enter your email address in the field provided here.\"\n",
    "df=df[df['content'].str.contains(targetString)==False]\n",
    "\n",
    "# This is also for some Atlantic articles for the same reasons as above\n",
    "targetString=\"This article is part of a feature we also send out via email as Politics  Policy Daily, a daily roundup of events and ideas in American politics written specially for newsletter subscribers. To sign up, please enter your email address in the field provided here.\"\n",
    "df=df[df['content'].str.contains(targetString)==False]\n",
    "\n",
    "# More Atlantic-specific removals (for daily summaries with multiple stories contained)\n",
    "df=df[df['content'].str.contains(\"To sign up, please enter your email address in the field\")==False]\n",
    "\n",
    "# Remove daily CNN summary\n",
    "targetString=\"CNN Student News\"\n",
    "df=df[df['content'].str.contains(targetString)==False]\n",
    "\n",
    "print(\"\\nArticle counts by publisher:\")\n",
    "print(df['publication'].value_counts())\n",
    "\n",
    "print(\"\\nArticle counts by date:\")\n",
    "print(df['date'].value_counts())\n",
    "\n",
    "# Restrict to articles on the provided input date.\n",
    "# This date is considered mandatory for topic clustering but is not required for sentiment\n",
    "# since sentiment only processes a specified list of articles.\n",
    "# For topic clustering it is essential to have the date as it is\n",
    "# enormously significant in article matching.\n",
    "# if processDate!=None:\n",
    "#     df=df[df['date']==processDate]\n",
    "# df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Remove non-ASCII characters\n",
    "df['content no nonascii']=df['content'].map(lambda x: removeNonASCIICharacters(x))\n",
    "\n",
    "print(\"\\nFinal dataset:\\n\\nDate:\",processDate,\"\\n\")\n",
    "print(df['publication'].value_counts())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>publication</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "      <th>NYT summary</th>\n",
       "      <th>content no nonascii</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3079</td>\n",
       "      <td>20694</td>\n",
       "      <td>Review: In ‘Independence Day: Resurgence,' the...</td>\n",
       "      <td>NY Times</td>\n",
       "      <td>Manohla Dargis</td>\n",
       "      <td>2016-12-02</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>If you've seen one movie apocalypse, you have ...</td>\n",
       "      <td>False</td>\n",
       "      <td>If you've seen one movie apocalypse, you have ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3241</td>\n",
       "      <td>20875</td>\n",
       "      <td>The Agony of the Digital Tease - The NY Times</td>\n",
       "      <td>NY Times</td>\n",
       "      <td>Jessica Bennett</td>\n",
       "      <td>2016-12-02</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>There was the breadcrumb dropped on Valentine'...</td>\n",
       "      <td>False</td>\n",
       "      <td>There was the breadcrumb dropped on Valentine'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3336</td>\n",
       "      <td>20986</td>\n",
       "      <td>Fox News's Convention Moment Overshadowed by S...</td>\n",
       "      <td>NY Times</td>\n",
       "      <td>Jim Rutenberg</td>\n",
       "      <td>2016-12-02</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CLEVELAND  —   This was supposed to be Fox New...</td>\n",
       "      <td>False</td>\n",
       "      <td>CLEVELAND     This was supposed to be Fox News...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3722</td>\n",
       "      <td>21413</td>\n",
       "      <td>One Star Over, a Planet That Might Be Another ...</td>\n",
       "      <td>NY Times</td>\n",
       "      <td>Kenneth Chang</td>\n",
       "      <td>2016-09-01</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Another Earth could be circling the star right...</td>\n",
       "      <td>False</td>\n",
       "      <td>Another Earth could be circling the star right...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3748</td>\n",
       "      <td>21448</td>\n",
       "      <td>University of Chicago Strikes Back Against Cam...</td>\n",
       "      <td>NY Times</td>\n",
       "      <td>Richard Pérez-Peña, Mitch Smith and Stephanie ...</td>\n",
       "      <td>2016-09-01</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The anodyne welcome letter to incoming freshme...</td>\n",
       "      <td>False</td>\n",
       "      <td>The anodyne welcome letter to incoming freshme...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     id                                              title  \\\n",
       "0        3079  20694  Review: In ‘Independence Day: Resurgence,' the...   \n",
       "1        3241  20875      The Agony of the Digital Tease - The NY Times   \n",
       "2        3336  20986  Fox News's Convention Moment Overshadowed by S...   \n",
       "3        3722  21413  One Star Over, a Planet That Might Be Another ...   \n",
       "4        3748  21448  University of Chicago Strikes Back Against Cam...   \n",
       "\n",
       "  publication                                             author        date  \\\n",
       "0    NY Times                                     Manohla Dargis  2016-12-02   \n",
       "1    NY Times                                    Jessica Bennett  2016-12-02   \n",
       "2    NY Times                                      Jim Rutenberg  2016-12-02   \n",
       "3    NY Times                                      Kenneth Chang  2016-09-01   \n",
       "4    NY Times  Richard Pérez-Peña, Mitch Smith and Stephanie ...  2016-09-01   \n",
       "\n",
       "     year  month  url                                            content  \\\n",
       "0  2016.0   12.0  NaN  If you've seen one movie apocalypse, you have ...   \n",
       "1  2016.0   12.0  NaN  There was the breadcrumb dropped on Valentine'...   \n",
       "2  2016.0   12.0  NaN  CLEVELAND  —   This was supposed to be Fox New...   \n",
       "3  2016.0    9.0  NaN  Another Earth could be circling the star right...   \n",
       "4  2016.0    9.0  NaN  The anodyne welcome letter to incoming freshme...   \n",
       "\n",
       "   NYT summary                                content no nonascii  \n",
       "0        False  If you've seen one movie apocalypse, you have ...  \n",
       "1        False  There was the breadcrumb dropped on Valentine'...  \n",
       "2        False  CLEVELAND     This was supposed to be Fox News...  \n",
       "3        False  Another Earth could be circling the star right...  \n",
       "4        False  The anodyne welcome letter to incoming freshme...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1   2\n",
       "0  1  2   3\n",
       "1  5  6   7\n",
       "2  8  9  10"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.DataFrame([[1,2,3],[5,6,7],[8,9,10]])\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>400</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>1200</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>1800</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0     1   2\n",
       "0  1   400   3\n",
       "1  5  1200   7\n",
       "2  8  1800  10"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testing the map function\n",
    "df2[1] = df2[1].map(lambda x: 200*x)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadStopWords(stopWordsFileName):\n",
    "\tstop_words=[]\n",
    "\tf=open(stopWordsFileName, 'r')\n",
    "\tfor l in f.readlines():\n",
    "\t\tstop_words.append(l.replace('\\n', ''))\n",
    "\treturn stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NNP', 'NNPS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n"
     ]
    }
   ],
   "source": [
    "runParams={'tfidf_maxdf':      [0.5],\n",
    "           'input_file':       ['./data/articles.csv'],\n",
    "           'story_threshold':  [0.26],\n",
    "           'process_date':     ['2016-09-01'],\n",
    "           'parts_of_speech':  [['PROPER', 'VERB']],\n",
    "           'lemma_conversion': [False],\n",
    "           'ngram_max':        [3],\n",
    "           'tfidf_binary':     [False],\n",
    "           'tfidf_norm':       ['l2'],\n",
    "           'nlp_library':      ['nltk'],\n",
    "           'max_length':       [50],\n",
    "           'stop_words_file':  ['./data/stopWords.txt'],\n",
    "           'tfidf_mindf':      [2],\n",
    "           'display_graph':    [True],\n",
    "           'article_stats':    [False]}\n",
    "\n",
    "partsOfSpeech=[]\n",
    "\n",
    "pos_nlp_mapping = {}\n",
    "pos_nlp_mapping['nltk']={'VERB':['VB','VBD','VBG','VBN','VBP','VBZ'],'PROPER':['NNP','NNPS'],'COMMON':['NN','NNS']}\n",
    "\n",
    "for pos in runParams['parts_of_speech'][0]:\n",
    "    partsOfSpeech.append(pos_nlp_mapping['nltk'][pos])\n",
    "partsOfSpeech=[item for sublist in partsOfSpeech for item in sublist]\n",
    "print(partsOfSpeech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk as nl\n",
    "stringToConvert = article_df['content']\n",
    "partsOfSpeech = partsOfSpeech\n",
    "stop_words = stop_words\n",
    "lemmatizer = wordnet_lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello, I am from England.', 'What is your name?']\n",
      "['Hello', ',', 'I', 'am', 'from', 'England', '.', 'What', 'is', 'your', 'name', '?']\n"
     ]
    }
   ],
   "source": [
    "sentence = 'Hello, I am from England. What is your name?'\n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "sentences = nltk.sent_tokenize(sentence)\n",
    "print(sentences)\n",
    "words = nltk.word_tokenize(sentence)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>publication</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "      <th>NYT summary</th>\n",
       "      <th>content no nonascii</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3079</td>\n",
       "      <td>20694</td>\n",
       "      <td>Review: In ‘Independence Day: Resurgence,' the...</td>\n",
       "      <td>NY Times</td>\n",
       "      <td>Manohla Dargis</td>\n",
       "      <td>2016-12-02</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>If you've seen one movie apocalypse, you have ...</td>\n",
       "      <td>False</td>\n",
       "      <td>If you've seen one movie apocalypse, you have ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3241</td>\n",
       "      <td>20875</td>\n",
       "      <td>The Agony of the Digital Tease - The NY Times</td>\n",
       "      <td>NY Times</td>\n",
       "      <td>Jessica Bennett</td>\n",
       "      <td>2016-12-02</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>There was the breadcrumb dropped on Valentine'...</td>\n",
       "      <td>False</td>\n",
       "      <td>There was the breadcrumb dropped on Valentine'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3336</td>\n",
       "      <td>20986</td>\n",
       "      <td>Fox News's Convention Moment Overshadowed by S...</td>\n",
       "      <td>NY Times</td>\n",
       "      <td>Jim Rutenberg</td>\n",
       "      <td>2016-12-02</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CLEVELAND  —   This was supposed to be Fox New...</td>\n",
       "      <td>False</td>\n",
       "      <td>CLEVELAND     This was supposed to be Fox News...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3722</td>\n",
       "      <td>21413</td>\n",
       "      <td>One Star Over, a Planet That Might Be Another ...</td>\n",
       "      <td>NY Times</td>\n",
       "      <td>Kenneth Chang</td>\n",
       "      <td>2016-09-01</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Another Earth could be circling the star right...</td>\n",
       "      <td>False</td>\n",
       "      <td>Another Earth could be circling the star right...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3748</td>\n",
       "      <td>21448</td>\n",
       "      <td>University of Chicago Strikes Back Against Cam...</td>\n",
       "      <td>NY Times</td>\n",
       "      <td>Richard Pérez-Peña, Mitch Smith and Stephanie ...</td>\n",
       "      <td>2016-09-01</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The anodyne welcome letter to incoming freshme...</td>\n",
       "      <td>False</td>\n",
       "      <td>The anodyne welcome letter to incoming freshme...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     id                                              title  \\\n",
       "0        3079  20694  Review: In ‘Independence Day: Resurgence,' the...   \n",
       "1        3241  20875      The Agony of the Digital Tease - The NY Times   \n",
       "2        3336  20986  Fox News's Convention Moment Overshadowed by S...   \n",
       "3        3722  21413  One Star Over, a Planet That Might Be Another ...   \n",
       "4        3748  21448  University of Chicago Strikes Back Against Cam...   \n",
       "\n",
       "  publication                                             author        date  \\\n",
       "0    NY Times                                     Manohla Dargis  2016-12-02   \n",
       "1    NY Times                                    Jessica Bennett  2016-12-02   \n",
       "2    NY Times                                      Jim Rutenberg  2016-12-02   \n",
       "3    NY Times                                      Kenneth Chang  2016-09-01   \n",
       "4    NY Times  Richard Pérez-Peña, Mitch Smith and Stephanie ...  2016-09-01   \n",
       "\n",
       "     year  month  url                                            content  \\\n",
       "0  2016.0   12.0  NaN  If you've seen one movie apocalypse, you have ...   \n",
       "1  2016.0   12.0  NaN  There was the breadcrumb dropped on Valentine'...   \n",
       "2  2016.0   12.0  NaN  CLEVELAND  —   This was supposed to be Fox New...   \n",
       "3  2016.0    9.0  NaN  Another Earth could be circling the star right...   \n",
       "4  2016.0    9.0  NaN  The anodyne welcome letter to incoming freshme...   \n",
       "\n",
       "   NYT summary                                content no nonascii  \n",
       "0        False  If you've seen one movie apocalypse, you have ...  \n",
       "1        False  There was the breadcrumb dropped on Valentine'...  \n",
       "2        False  CLEVELAND     This was supposed to be Fox News...  \n",
       "3        False  Another Earth could be circling the star right...  \n",
       "4        False  The anodyne welcome letter to incoming freshme...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_df = df\n",
    "article_df.head()\n",
    "# article_df['content'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stringNLTKProcess(nl, stringToConvert,partsOfSpeech,stop_words,maxWords=None,lemmatizer=None):\n",
    "    #parses the paragraph into sentences\n",
    "    sentences = nl.sent_tokenize(stringToConvert)\n",
    "    str = []\n",
    "    for sentence in sentences:\n",
    "        wordString=[]\n",
    "        for word, pos in nl.pos_tag(nl.word_tokenize(sentence)):\n",
    "            # The following condition avoids any POS which corresponds to punctuation (and takes all others\n",
    "            if partsOfSpeech == None:\n",
    "                if pos[0]>='A' and pos[0]<='Z':\n",
    "                    wordString.append(word)\n",
    "            elif pos in partsOfSpeech:\n",
    "                wordString.append(word)\n",
    "        for wrd in wordString:\n",
    "            #converts all string characters into lowercase elements\n",
    "            wrdlower=wrd.lower()\n",
    "            if wrdlower not in stop_words and wrdlower!=\"'s'\":\n",
    "                if maxWords==None or len(str)<maxWords:\n",
    "                    if lemmatizer==None:\n",
    "                        str.append(wrdlower)\n",
    "                    else:\n",
    "                        str.append(lemmatizer.lemmatize(wrd.lower(), pos='v'))\n",
    "            if maxWords!=None and len(str)==maxWords:\n",
    "                return ' '.join(str)\n",
    "    return ' '.join(str)\n",
    "\n",
    "def removeSpacesAndPunctuation(textString): \n",
    "    return \"\".join(i for i in textString if (ord(i)>=48 and ord(i)<=57) or (ord(i)>=97 and ord(i)<=122))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_df['input to vectorizer'] = article_df['content no nonascii'].map(lambda x: stringNLTKProcess(nl,x,partsOfSpeech,stop_words,maxWords=None,lemmatizer=wordnet_lemmatizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'ve see see direct roland emmerich mr. emmerich carve mayan melt scale narrow white house mr. emmerich go go independence day independence day amuse remember white house blow funny contemplate say think mr. emmerich encourage engineer generate boom activate 's resurgence spur need know take cook hinge erupt put include mr. emmerich dean devlin sound much resurgence pop feature lift smith mr. smith decline appear leave independence day bill pullman jeff goldblum give judd hirsch brent spiner wink 're strand crowd give right mr. emmerich manage personalize sprinkle use clean seem try summon dredge feel glance independence day liam hemsworth punch mr. smith earth mr. hemsworth mr. smith independence day sell cut independence day rat parent caution bloodless run\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_df['input to vectorizer'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing out TfidVectorizer on sample content\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(analyzer = 'word', ngram_range = (1,5), lowercase=\"true\", binary=\"fales\", norm=\"l2\")\n",
    "\n",
    "corpus = ['This is the first document.','This is the second second document.','And the third one.','Is this the first document?'] \n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(analyzer = 'word', ngram_range = (1,5), lowercase=\"true\", binary=\"fales\", norm=\"l2\")\n",
    "tfidVectors = vectorizer.fit_transform(article_df['input to vectorizer'])\n",
    "terms = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<664x442537 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 529960 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidVectors_dense = tfidVectors.todense()\n",
    "df_tfidfVectors = pd.DataFrame(tfidVectors_dense)\n",
    "df_tfidfVectors.head()\n",
    "\n",
    "df.to_csv(r'C:\\Users\\goldm\\Capstone\\tracking files\\testing_tfidfVectors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
