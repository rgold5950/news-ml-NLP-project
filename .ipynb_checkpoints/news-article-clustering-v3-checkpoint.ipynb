{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "runParams={'tfidf_maxdf':      [0.5],\n",
    "           'input_file':       ['./data/articles.csv'],\n",
    "           'story_threshold':  [0.26],\n",
    "           'process_date':     ['2016-09-01'],\n",
    "           'parts_of_speech':  [['PROPER', 'VERB']],\n",
    "           'lemma_conversion': [False],\n",
    "           'ngram_max':        [3],\n",
    "           'tfidf_binary':     [False],\n",
    "           'tfidf_norm':       ['l2'],\n",
    "           'nlp_library':      ['nltk'],\n",
    "           'max_length':       [50],\n",
    "           'stop_words_file':  ['./data/stopWords.txt'],\n",
    "           'tfidf_mindf':      [2],\n",
    "           'display_graph':    [True],\n",
    "           'article_stats':    [False]}\n",
    "\n",
    "# Use parameter grid even if there is only set of parameters\n",
    "parameterGrid=ParameterGrid(runParams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and initialise required NLP libraries\n",
    "pos_nlp_mapping={}\n",
    "nl=None\n",
    "wordnet_lemmatizer=None\n",
    "nlp=None\n",
    "if 'spaCy' in runParams['nlp_library']:\n",
    "    import spacy\n",
    "    nlp=spacy.load('en')\n",
    "    pos_nlp_mapping['spaCy']={'VERB':['VERB'],'PROPER':['PROPN'],'COMMON':['NOUN']}\n",
    "    \n",
    "if 'nltk' in runParams['nlp_library']:\n",
    "    import nltk as nl\n",
    "    if True in runParams['lemma_conversion']:\n",
    "        from nltk.stem import WordNetLemmatizer\n",
    "    else:\n",
    "        wordnet_lemmatizer=None\n",
    "    pos_nlp_mapping['nltk']={'VERB': ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'],'PROPER':['NNP','NNPS'],'COMMON':['NN','NNS']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInputDataAndDisplayStats(filename,processDate,printSummary=False):\n",
    "    df = pd.read_csv(filename)\n",
    "    df.drop_duplicates('content')\n",
    "    df = df[~df['content'].isnull()]\n",
    "    df=df[df['content'].str.len()>=200]\n",
    "\n",
    "    targetString=\"(Want to get this briefing by email?\"\n",
    "    df['NYT summary']=df['content'].map(lambda d: d[:len(targetString)]==targetString)\n",
    "    df=df[df['NYT summary']==False]\n",
    "\n",
    "    # The following removes a warning that appears in many of the Atlantic articles.\n",
    "    # Since it is commonly at the beginning, it brings a lot of noise to the search for similar articles\n",
    "    # And subsequently to the assessment of sentiment\n",
    "    targetString=\"For us to continue writing great stories, we need to display ads.             Please select the extension that is blocking ads.     Please follow the steps below\"\n",
    "    df['content']=df['content'].str.replace(targetString,'')\n",
    "\n",
    "    # This is also for some Atlantic articles for the same reasons as above\n",
    "    targetString=\"This article is part of a feature we also send out via email as The Atlantic Daily, a newsletter with stories, ideas, and images from The Atlantic, written specially for subscribers. To sign up, please enter your email address in the field provided here.\"\n",
    "    df=df[df['content'].str.contains(targetString)==False]\n",
    "\n",
    "    # This is also for some Atlantic articles for the same reasons as above\n",
    "    targetString=\"This article is part of a feature we also send out via email as Politics  Policy Daily, a daily roundup of events and ideas in American politics written specially for newsletter subscribers. To sign up, please enter your email address in the field provided here.\"\n",
    "    df=df[df['content'].str.contains(targetString)==False]\n",
    "\n",
    "    # More Atlantic-specific removals (for daily summaries with multiple stories contained)\n",
    "    df=df[df['content'].str.contains(\"To sign up, please enter your email address in the field\")==False]\n",
    "\n",
    "    # Remove daily CNN summary\n",
    "    targetString=\"CNN Student News\"\n",
    "    df=df[df['content'].str.contains(targetString)==False]\n",
    "    \n",
    "    \n",
    "    print(\"\\nArticle counts by publisher:\")\n",
    "    print(df['publication'].value_counts())\n",
    "\n",
    "    print(\"\\nArticle counts by date:\")\n",
    "    print(df['date'].value_counts())\n",
    "\n",
    "#     Restrict to articles on the provided input date.\n",
    "#     This date is considered mandatory for topic clustering but is not required for sentiment\n",
    "#     since sentiment only processes a specified list of articles.\n",
    "#     For topic clustering it is essential to have the date as it is\n",
    "#     enormously significant in article matching.\n",
    "\n",
    "    if processDate!=None:\n",
    "        df=df[df['date']==processDate]\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    # Remove non-ASCII characters\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    df['content no nonascii']=df['content'].map(lambda x: removeNonASCIICharacters(x))\n",
    "\n",
    "    print(\"\\nFinal dataset:\\n\\nDate:\",processDate,\"\\n\")\n",
    "    print(df['publication'].value_counts())\n",
    "    df.to_csv(r'C:\\Users\\goldm\\Capstone\\tracking files\\getInputDataAndDisplayStats.csv')\n",
    "    \n",
    "    return df\n",
    "\n",
    "##########################################################################################\n",
    "\n",
    "def removeNonASCIICharacters(textString): \n",
    "    return \"\".join(i for i in textString if ord(i)<128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Article counts by publisher:\n",
      "Breitbart           104\n",
      "NY Post              61\n",
      "CNN                  57\n",
      "Reuters              56\n",
      "NPR                  54\n",
      "Washington Post      50\n",
      "NY Times             50\n",
      "Atlantic             48\n",
      "Buzzfeed News        48\n",
      "Business Insider     41\n",
      "Guardian             35\n",
      "National Review      32\n",
      "Fox News             28\n",
      "Name: publication, dtype: int64\n",
      "\n",
      "Article counts by date:\n",
      "2016-12-02    362\n",
      "2016-09-01    302\n",
      "Name: date, dtype: int64\n",
      "\n",
      "Final dataset:\n",
      "\n",
      "Date: 2016-09-01 \n",
      "\n",
      "Breitbart           50\n",
      "Buzzfeed News       35\n",
      "NY Times            28\n",
      "NY Post             27\n",
      "NPR                 26\n",
      "Atlantic            24\n",
      "Washington Post     22\n",
      "CNN                 20\n",
      "Reuters             15\n",
      "Business Insider    15\n",
      "Guardian            14\n",
      "Fox News            14\n",
      "National Review     12\n",
      "Name: publication, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "articleDataFrame=getInputDataAndDisplayStats(runParams['input_file'][0],\n",
    "                                             runParams['process_date'][0],\n",
    "                                             runParams['article_stats'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Article counts by publisher:\n",
      "Breitbart           104\n",
      "NY Post              61\n",
      "CNN                  57\n",
      "Reuters              56\n",
      "NPR                  54\n",
      "Washington Post      50\n",
      "NY Times             50\n",
      "Atlantic             48\n",
      "Buzzfeed News        48\n",
      "Business Insider     41\n",
      "Guardian             35\n",
      "National Review      32\n",
      "Fox News             28\n",
      "Name: publication, dtype: int64\n",
      "\n",
      "Article counts by date:\n",
      "2016-12-02    362\n",
      "2016-09-01    302\n",
      "Name: date, dtype: int64\n",
      "\n",
      "Final dataset:\n",
      "\n",
      "Date: 2016-09-01 \n",
      "\n",
      "Breitbart           50\n",
      "Buzzfeed News       35\n",
      "NY Times            28\n",
      "NY Post             27\n",
      "NPR                 26\n",
      "Atlantic            24\n",
      "Washington Post     22\n",
      "CNN                 20\n",
      "Reuters             15\n",
      "Business Insider    15\n",
      "Guardian            14\n",
      "Fox News            14\n",
      "National Review     12\n",
      "Name: publication, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>publication</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "      <th>NYT summary</th>\n",
       "      <th>content no nonascii</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3722</td>\n",
       "      <td>21413</td>\n",
       "      <td>One Star Over, a Planet That Might Be Another ...</td>\n",
       "      <td>NY Times</td>\n",
       "      <td>Kenneth Chang</td>\n",
       "      <td>2016-09-01</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Another Earth could be circling the star right...</td>\n",
       "      <td>False</td>\n",
       "      <td>Another Earth could be circling the star right...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3748</td>\n",
       "      <td>21448</td>\n",
       "      <td>University of Chicago Strikes Back Against Cam...</td>\n",
       "      <td>NY Times</td>\n",
       "      <td>Richard Pérez-Peña, Mitch Smith and Stephanie ...</td>\n",
       "      <td>2016-09-01</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The anodyne welcome letter to incoming freshme...</td>\n",
       "      <td>False</td>\n",
       "      <td>The anodyne welcome letter to incoming freshme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3754</td>\n",
       "      <td>21454</td>\n",
       "      <td>Quake Exposes Italy's Challenge to Retrofit It...</td>\n",
       "      <td>NY Times</td>\n",
       "      <td>Gaia Pianigiani and Elisabetta Povoledo</td>\n",
       "      <td>2016-09-01</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CASETTA, Italy  —   Romano Camassi, a seismolo...</td>\n",
       "      <td>False</td>\n",
       "      <td>CASETTA, Italy     Romano Camassi, a seismolog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3755</td>\n",
       "      <td>21455</td>\n",
       "      <td>A Cheaper Airbag, and Takata's Road to a Deadl...</td>\n",
       "      <td>NY Times</td>\n",
       "      <td>Hiroko Tabuchi</td>\n",
       "      <td>2016-09-01</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>In the late 1990s, General Motors got an unexp...</td>\n",
       "      <td>False</td>\n",
       "      <td>In the late 1990s, General Motors got an unexp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3772</td>\n",
       "      <td>21474</td>\n",
       "      <td>Gene Wilder Dies at 83 Star of ‘Willy Wonka' a...</td>\n",
       "      <td>NY Times</td>\n",
       "      <td>Daniel Lewis</td>\n",
       "      <td>2016-09-01</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Gene Wilder, who established himself as one of...</td>\n",
       "      <td>False</td>\n",
       "      <td>Gene Wilder, who established himself as one of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>143552</td>\n",
       "      <td>214894</td>\n",
       "      <td>Watch SpaceX's rocket explode in a massive fir...</td>\n",
       "      <td>Washington Post</td>\n",
       "      <td>Christian Davenport</td>\n",
       "      <td>2016-09-01</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>https://web.archive.org/web/20160902000145/htt...</td>\n",
       "      <td>As SpaceX prepared to test fire the p...</td>\n",
       "      <td>False</td>\n",
       "      <td>As SpaceX prepared to test fire the p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>143553</td>\n",
       "      <td>214895</td>\n",
       "      <td>This photographer's photos show the tender mom...</td>\n",
       "      <td>Washington Post</td>\n",
       "      <td>Kenneth Dickerman</td>\n",
       "      <td>2016-09-01</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>https://web.archive.org/web/20160902000145/htt...</td>\n",
       "      <td>Old family photographs on Marisa Vesco...</td>\n",
       "      <td>False</td>\n",
       "      <td>Old family photographs on Marisa Vesco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>143584</td>\n",
       "      <td>214934</td>\n",
       "      <td>How Anthony Weinerâ€™s risque messages shaped ...</td>\n",
       "      <td>Washington Post</td>\n",
       "      <td>Sarah Jeong</td>\n",
       "      <td>2016-09-01</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>https://web.archive.org/web/20160904003253/htt...</td>\n",
       "      <td>In the aftermath of Anthony Weiner s late...</td>\n",
       "      <td>False</td>\n",
       "      <td>In the aftermath of Anthony Weiner s late...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>143592</td>\n",
       "      <td>214943</td>\n",
       "      <td>Stop touting the crazy hours you work. It help...</td>\n",
       "      <td>Washington Post</td>\n",
       "      <td>Jena McGregor</td>\n",
       "      <td>2016-09-01</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>https://web.archive.org/web/20160904003253/htt...</td>\n",
       "      <td>As Labor Day approaches, and a single day...</td>\n",
       "      <td>False</td>\n",
       "      <td>As Labor Day approaches, and a single day...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>143618</td>\n",
       "      <td>214973</td>\n",
       "      <td>One dietitianâ€™s secret weapon for healthy ea...</td>\n",
       "      <td>Washington Post</td>\n",
       "      <td>Jae Berman</td>\n",
       "      <td>2016-09-01</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>https://web.archive.org/web/20160905002504/htt...</td>\n",
       "      <td>I am going to fill you in on a major secret o...</td>\n",
       "      <td>False</td>\n",
       "      <td>I am going to fill you in on a major secret o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>302 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0      id                                              title  \\\n",
       "0          3722   21413  One Star Over, a Planet That Might Be Another ...   \n",
       "1          3748   21448  University of Chicago Strikes Back Against Cam...   \n",
       "2          3754   21454  Quake Exposes Italy's Challenge to Retrofit It...   \n",
       "3          3755   21455  A Cheaper Airbag, and Takata's Road to a Deadl...   \n",
       "4          3772   21474  Gene Wilder Dies at 83 Star of ‘Willy Wonka' a...   \n",
       "..          ...     ...                                                ...   \n",
       "297      143552  214894  Watch SpaceX's rocket explode in a massive fir...   \n",
       "298      143553  214895  This photographer's photos show the tender mom...   \n",
       "299      143584  214934  How Anthony Weinerâ€™s risque messages shaped ...   \n",
       "300      143592  214943  Stop touting the crazy hours you work. It help...   \n",
       "301      143618  214973  One dietitianâ€™s secret weapon for healthy ea...   \n",
       "\n",
       "         publication                                             author  \\\n",
       "0           NY Times                                      Kenneth Chang   \n",
       "1           NY Times  Richard Pérez-Peña, Mitch Smith and Stephanie ...   \n",
       "2           NY Times            Gaia Pianigiani and Elisabetta Povoledo   \n",
       "3           NY Times                                     Hiroko Tabuchi   \n",
       "4           NY Times                                       Daniel Lewis   \n",
       "..               ...                                                ...   \n",
       "297  Washington Post                                Christian Davenport   \n",
       "298  Washington Post                                  Kenneth Dickerman   \n",
       "299  Washington Post                                        Sarah Jeong   \n",
       "300  Washington Post                                      Jena McGregor   \n",
       "301  Washington Post                                         Jae Berman   \n",
       "\n",
       "           date    year  month  \\\n",
       "0    2016-09-01  2016.0    9.0   \n",
       "1    2016-09-01  2016.0    9.0   \n",
       "2    2016-09-01  2016.0    9.0   \n",
       "3    2016-09-01  2016.0    9.0   \n",
       "4    2016-09-01  2016.0    9.0   \n",
       "..          ...     ...    ...   \n",
       "297  2016-09-01  2016.0    9.0   \n",
       "298  2016-09-01  2016.0    9.0   \n",
       "299  2016-09-01  2016.0    9.0   \n",
       "300  2016-09-01  2016.0    9.0   \n",
       "301  2016-09-01  2016.0    9.0   \n",
       "\n",
       "                                                   url  \\\n",
       "0                                                  NaN   \n",
       "1                                                  NaN   \n",
       "2                                                  NaN   \n",
       "3                                                  NaN   \n",
       "4                                                  NaN   \n",
       "..                                                 ...   \n",
       "297  https://web.archive.org/web/20160902000145/htt...   \n",
       "298  https://web.archive.org/web/20160902000145/htt...   \n",
       "299  https://web.archive.org/web/20160904003253/htt...   \n",
       "300  https://web.archive.org/web/20160904003253/htt...   \n",
       "301  https://web.archive.org/web/20160905002504/htt...   \n",
       "\n",
       "                                               content  NYT summary  \\\n",
       "0    Another Earth could be circling the star right...        False   \n",
       "1    The anodyne welcome letter to incoming freshme...        False   \n",
       "2    CASETTA, Italy  —   Romano Camassi, a seismolo...        False   \n",
       "3    In the late 1990s, General Motors got an unexp...        False   \n",
       "4    Gene Wilder, who established himself as one of...        False   \n",
       "..                                                 ...          ...   \n",
       "297           As SpaceX prepared to test fire the p...        False   \n",
       "298          Old family photographs on Marisa Vesco...        False   \n",
       "299       In the aftermath of Anthony Weiner s late...        False   \n",
       "300       As Labor Day approaches, and a single day...        False   \n",
       "301   I am going to fill you in on a major secret o...        False   \n",
       "\n",
       "                                   content no nonascii  \n",
       "0    Another Earth could be circling the star right...  \n",
       "1    The anodyne welcome letter to incoming freshme...  \n",
       "2    CASETTA, Italy     Romano Camassi, a seismolog...  \n",
       "3    In the late 1990s, General Motors got an unexp...  \n",
       "4    Gene Wilder, who established himself as one of...  \n",
       "..                                                 ...  \n",
       "297           As SpaceX prepared to test fire the p...  \n",
       "298          Old family photographs on Marisa Vesco...  \n",
       "299       In the aftermath of Anthony Weiner s late...  \n",
       "300       As Labor Day approaches, and a single day...  \n",
       "301   I am going to fill you in on a major secret o...  \n",
       "\n",
       "[302 rows x 12 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getInputDataAndDisplayStats(runParams['input_file'][0],\n",
    "                            runParams['process_date'][0],\n",
    "                            printSummary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadStopWords(stopWordsFileName):\n",
    "    stop_words=[]\n",
    "    f=open(stopWordsFileName, 'r')\n",
    "    for l in f.readlines():\n",
    "        stop_words.append(l.replace('\\n', ''))\n",
    "    return stop_words\n",
    "\n",
    "stop_words=loadStopWords(runParams['stop_words_file'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stringNLTKProcess(nl,stringToConvert,partsOfSpeech,stop_words,maxWords=None,lemmatizer=None):\n",
    "    sentences=nl.sent_tokenize(stringToConvert)\n",
    "    str=[]\n",
    "    for sentence in sentences:\n",
    "        wordString=[]\n",
    "        for word,pos in nl.pos_tag(nl.word_tokenize(sentence)):\n",
    "            # The following condition avoids any POS which corresponds to punctuation (and takes all others)\n",
    "            if partsOfSpeech==None:\n",
    "                if pos[0]>='A' and pos[0]<='Z':\n",
    "                    wordString.append(word)\n",
    "            elif pos in partsOfSpeech:\n",
    "                wordString.append(word)\n",
    "        for wrd in wordString:\n",
    "            wrdlower=wrd.lower()\n",
    "            if wrdlower not in stop_words and wrdlower!=\"'s\":\n",
    "                if maxWords==None or len(str)<maxWords:\n",
    "                    if lemmatizer==None:\n",
    "                        str.append(wrdlower)\n",
    "                    else:\n",
    "                        str.append(lemmatizer.lemmatize(wrd.lower(), pos='v'))\n",
    "            if maxWords!=None and len(str)==maxWords:\n",
    "                return ' '.join(str)\n",
    "    return ' '.join(str)\n",
    "\n",
    "def removeSpacesAndPunctuation(textString):\n",
    "    return \"\".join(i for i in textString if (ord(i)>=48 and ord(i)<=57) or (ord(i)>=97 and ord(i)<=122))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setupStoryMapAndReportList(args=None, reportArticleList=None,storyMapFileName=None):\n",
    "    # Story Map is used in fitting if grid search is applied (As ground truth)\n",
    "    # It is also used in graph if no threshold provided (to determine colours, not to determine location)\n",
    "    # Report Article List is used at the end to create a report with, for each\n",
    "    # article in the list, the set of articles within tolerance, and the key words for each\n",
    "    if args==None:\n",
    "        articleList=reportArticleList\n",
    "        fileName=storyMapFileName\n",
    "    else:\n",
    "        articleList=args['article_id_list']\n",
    "        fileName=args['story_map_validation']\n",
    "    \n",
    "    reportArticleList=articleList\n",
    "    if fileName!=None:\n",
    "        storyMap=readStoryMapFromFile(fileName)\n",
    "        if reportArticleList==None:\n",
    "            reportArticleList=[]\n",
    "            for story, articleList in storyMap.items():\n",
    "                reportArticleList.append(articleList[0])\n",
    "    else:\n",
    "        storyMap=None\n",
    "    return storyMap,reportArticleList\n",
    "\n",
    "def readStoryMapFromFile(filename):\n",
    "    return readDictFromCsvFile(filename,'StoryMap')\n",
    "\n",
    "def readGridParameterRangeFromFile(filename):\n",
    "    return readDictFromCsvFile(filename, 'GridParameters')\n",
    "\n",
    "def readDictFromCsvFile(filename,schema):\n",
    "    gridParamDict={} \n",
    "    with open(filename,'r') as f:\n",
    "        for row in f:\n",
    "            row=row[:-1] # Exclude the carriage return\n",
    "            row=row.split(',')\n",
    "            key=row[0]\n",
    "            vals=row[1:]\n",
    "            \n",
    "            if schema=='GridParameters':\n",
    "                if key in ['story_threshold','tfidf_maxdf']:\n",
    "                    finalVals=list(float(n) for n in vals)\n",
    "                elif key in ['ngram_max','tfidf_mindf','max_length']:\n",
    "                    finalVals=list(int(n) for n in vals)\n",
    "                elif key in ['lemma_conversion','tfidf_binary']:\n",
    "                    finalVals = list(str2bool(n) for n in vals)\n",
    "                elif key in ['parts_of_speech']:\n",
    "                    listlist=[]\n",
    "                    for v in vals:\n",
    "                        listlist.append(v_split('+'))\n",
    "                    finalVals = listlist\n",
    "                elif key in ['tfidf_norm','nlp_library']:\n",
    "                    finalVals=vals\n",
    "                else:\n",
    "                    print(key)\n",
    "                    print(\"KEY ERROR\")\n",
    "                    return\n",
    "            elif schema == 'StoryMap':\n",
    "                finalVals = list(int(n) for n in vals if n!='')\n",
    "            else:\n",
    "                print(schema)\n",
    "                print('SCHEMA ERROR')\n",
    "                return\n",
    "            \n",
    "            gridParamDict[key]=finalVals\n",
    "    return gridParamDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "storyMap,reportArticleList=setupStoryMapAndReportList(storyMapFileName='storyMapForValidation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trump meeting : [151832, 110126, 172078, 48306, 57365, 190512, 26536, 71335, 21499, 23872, 142033, 110133, 23888, 71336, 57366, 71339]\n",
      "Brazil impeachment : [120639, 80103, 25225, 21502, 57362, 120636, 110141]\n",
      "Kaepernick : [40617, 40543, 39520, 80109, 80101, 47403]\n",
      "Clinton Guccifer : [214888, 85803, 47979]\n",
      "Farage : [37252, 37468, 46175]\n",
      "Anthony Weiner : [49480, 110144, 142300, 214934]\n",
      "SpaceX : [38658, 134545, 172095, 214894]\n",
      "Safe space : [21448, 78169, 78171]\n",
      "Lauer debate : [43447, 47078, 138709]\n",
      "Venezuela : [172079, 57375, 190522]\n",
      "Iran deal : [158005, 48823, 57373, 120634]\n",
      "Penn State : [80094, 157527, 214892]\n",
      "David Brown : [172085, 80096, 141886]\n"
     ]
    }
   ],
   "source": [
    "for story, articleList in storyMap.items():\n",
    "    print(story,\":\",articleList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessAndVectorize(articleDataFrame,args,pos_nlp_mapping,nlp,nl,wordnet_lemmatizer,stop_words):\n",
    "    # Map the input parts of speech list to the coding required for the specific NLP library\n",
    "    if args['parts_of_speech'][0]!='ALL':\n",
    "        partsOfSpeech=[]\n",
    "        for pos in args['parts_of_speech']:\n",
    "            partsOfSpeech.append(pos_nlp_mapping[args['nlp_library']][pos])\n",
    "        partsOfSpeech=[item for sublist in partsOfSpeech for item in sublist]\n",
    "    else:\n",
    "        partsOfSpeech=None\n",
    "    \n",
    "    # Processing of text depends on NLP library choice\n",
    "    if args['nlp_library']=='spaCy':\n",
    "        articleDataFrame['input to vectorizer']=articleDataFrame['content no nonascii'].map(lambda x: stringSpaCyProcess(nlp,\n",
    "                                                                                                                         x,\n",
    "                                                                                                                         partsOfSpeech=partsOfSpeech,\n",
    "                                                                                                                         maxWords=args['max_length'],\n",
    "                                                                                                                         stop_words=stop_words,\n",
    "                                                                                                                         lemmatize=args['lemma_conversion']))\n",
    "    elif args['nlp_library']=='nltk':\n",
    "        articleDataFrame['input to vectorizer']=articleDataFrame['content no nonascii'].map(lambda x: stringNLTKProcess(nl,\n",
    "                                                                                                                        x,\n",
    "                                                                                                                        partsOfSpeech=partsOfSpeech,\n",
    "                                                                                                                        stop_words=stop_words,\n",
    "                                                                                                                        maxWords=args['max_length'],\n",
    "                                                                                                                        lemmatizer=wordnet_lemmatizer))\n",
    "    else:\n",
    "        print(\"PROBLEM... NO VALID NLP LIBRARY... MUST BE nltk OR spaCy\")\n",
    "\n",
    "    # To get default values a couple of parameters need to be not passed if not specified on the command line\n",
    "    # Passing as None behaves differently to passing no parameter (which would invoke the default value)\n",
    "    optArgsForVectorizer={}\n",
    "    if args['tfidf_maxdf'] != None:\n",
    "        optArgsForVectorizer['max_df']=args['tfidf_maxdf']\n",
    "    if args['tfidf_mindf'] != None:\n",
    "        optArgsForVectorizer['min_df']=args['tfidf_mindf']\n",
    "    # Create and run the vectorize\n",
    "    vectorizer=TfidfVectorizer(analyzer='word',\n",
    "                               ngram_range=(1,args['ngram_max']),\n",
    "                              lowercase=True,\n",
    "                              binary=args['tfidf_binary'],\n",
    "                              norm=args['tfidf_norm'],\n",
    "                              **optArgsForVectorizer)\n",
    "    tfidfVectors=vectorizer.fit_transform(articleDataFrame['input to vectorizer'])\n",
    "    terms=vectorizer.get_feature_names()\n",
    "    return tfidfVectors, terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scoreCurrentParamGuess(tfidfVectors,storyMap,articleDataFrame,threshold,printErrors=False):\n",
    "    # Work with distances relative to first item in each cluster - even though this is clearly arbitrary since that\n",
    "    # point could be an outlier in the cluster and hence might cause problems.\n",
    "    # But I have to start somewhere - and can refine it later if needed.\n",
    "\n",
    "    nonZeroCoords=initialiseAllNonZeroCoords(tfidfVectors)\n",
    "    score=0\n",
    "    outGood=0\n",
    "    outBad=0\n",
    "    inGood=0\n",
    "    inBad=0\n",
    "    \n",
    "    #### ---- Richard Modification- adding in code to print out a df of articles in the story map and their respective category       \n",
    "    final_mapping_list = []\n",
    "    \n",
    "    for story, storyArticles in storyMap.items():\n",
    "        leadArticleIndex=articleDataFrame[articleDataFrame['id']==storyArticles[0]].index[0]\n",
    "        # Compute score of all articles in corpus relative to first article in story (.product)\n",
    "        # Then count through list relative to threshold (add one for a good result, subtract one for a bad result)\n",
    "        scores=productRelatednessScores(tfidfVectors,nonZeroCoords,leadArticleIndex)\n",
    "        rankedIndices=np.argsort(scores)\n",
    "        foundRelatedArticles=[]\n",
    "        # THE SORTING HERE IS NOT STRICTLY REQUIRED, BUT I COULD USE IT SO THAT ONCE THE THRESHOLD IS PASSED\n",
    "        # IN THE LOOP, THEN I INFER THE REMAINING RESULTS\n",
    "        for article in reversed(rankedIndices):\n",
    "            thisArticleIndex=articleDataFrame['id'][article]\n",
    "            if thisArticleIndex in storyArticles:\n",
    "                if scores[article]>=threshold: # article IS supposed to be in range\n",
    "                    score+=1\n",
    "                    inGood+=1\n",
    "                    #appending the article and its mapping according to the predictions of our model\n",
    "                    final_mapping_list.append([thisArticleIndex, story, 'TP'])\n",
    "                else:\n",
    "                    score-=1\n",
    "                    inBad+=1\n",
    "                    final_mapping_list.append([thisArticleIndex, 'No Mapping', 'FN'])\n",
    "                    if printErrors:\n",
    "                        print(\"ERROR:\",thisArticleIndex,\"should be in\",story)\n",
    "            else: # article not supposed to be in range\n",
    "                if scores[article]<=threshold:\n",
    "                    score+=1\n",
    "                    outGood+=1\n",
    "                    final_mapping_list.append([thisArticleIndex, 'No Mapping', 'TN'])\n",
    "                else:\n",
    "                    score-=1\n",
    "                    outBad+=1\n",
    "                    final_mapping_list.append([thisArticleIndex, story, 'FP'])\n",
    "                    if printErrors:\n",
    "                        print(\"ERROR:\",thisArticleIndex,\"should NOT be in\",story)\n",
    "    \n",
    "    #### ---- Richard Modification- adding in code to print out a df of articles in the story map and their respective categor\n",
    "    final_mapping_df = pd.DataFrame(final_mapping_list, columns = ['article', 'cateogry ID', 'FP/FN/TP/TN'])\n",
    "    final_mapping_df.to_csv(r\"C:\\Users\\goldm\\OneDrive\\Desktop\\Richard's Files\\Data Science\\News ML Project\\PredictedMappings.csv\")\n",
    "    \n",
    "    scoreDict={'score':score,'inGood':inGood,'inBad':inBad,'outGood':outGood,'outBad':outBad}\n",
    "    return scoreDict\n",
    "\n",
    "##########################################################################################\n",
    "\n",
    "def initialiseAllNonZeroCoords(tfidfVectors):\n",
    "# This function just exists since it seems to be expensive and I'd rather not call it multiple times\n",
    "# Hence it is intended to be called outside of loops in order to simplify the row specific processing\n",
    "    values=[]\n",
    "    nzc=zip(*tfidfVectors.nonzero())\n",
    "\n",
    "    # In Python 3 the zip can only be iterated through one time before it is automatically released\n",
    "    # So need to copy the results otherwise the main loop below will no longer work\n",
    "    pointList=[]\n",
    "    for i,j in nzc:\n",
    "        pointList.append([i,j])\t\t\n",
    "\n",
    "    for row in range(tfidfVectors.shape[0]):\n",
    "        rowList=[]\n",
    "        for i,j in pointList:\n",
    "            if row==i:\n",
    "                rowList.append(j)\n",
    "        values.append(rowList)\n",
    "\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def productRelatednessScores(tfidfVectors,nonZeroCoords,refRow):\n",
    "    # instantiates a matrix of zeros with tfidVectors.shape[0] rows corresponding to the number of rows in the tfidVectors array\n",
    "    scores = [0]*tfidfVectors.shape[0]\n",
    "    for toRow in range(tfidfVectors.shape[0]):\n",
    "        scores[toRow] = sum([(tfidfVectors[toRow,w]*tfidfVectors[refRow,w]) for w in nonZeroCoords[refRow] if w in nonZeroCoords[toRow]])\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\goldm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk; nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 3914, 'inGood': 60, 'inBad': 2, 'outGood': 3860, 'outBad': 4}\n"
     ]
    }
   ],
   "source": [
    "# Loop across all parameter combinations in grid to determine best set\n",
    "# If not doing grid search, will just pass through the loop once\n",
    "bestParamScoreDict={'score':-1000000}\n",
    "bestParams=parameterGrid[0]\n",
    "for i,currentParams in enumerate(parameterGrid):\n",
    "    if len(parameterGrid)>1:\n",
    "        print(\"Combination:\",i+1,\"of\",len(parameterGrid))\n",
    "        print(currentParams)\n",
    "        \n",
    "        # Determine tf-idf vectors\n",
    "        # terms is just used later on if analysis of final results is requested\n",
    "    tfidfVectors,terms=preprocessAndVectorize(articleDataFrame,\n",
    "                                              currentParams,\n",
    "                                              pos_nlp_mapping,\n",
    "                                              nlp,\n",
    "                                              nl,\n",
    "                                              wordnet_lemmatizer,\n",
    "                                              stop_words)\n",
    "\n",
    "    # Compute scores if threshold provided (meaning as part of grid search)\n",
    "    if 'story_threshold' in currentParams and currentParams['story_threshold']!=None:\n",
    "        scoreDict = scoreCurrentParamGuess(tfidfVectors,storyMap,articleDataFrame,currentParams['story_threshold'])\n",
    "        print(scoreDict)\n",
    "\n",
    "        # Update best so far\n",
    "        if scoreDict['score']>=bestParamScoreDict['score']:\n",
    "            if len(parameterGrid)>1:\n",
    "                print(i+1,\"is the best so far!\")\n",
    "            bestParams=currentParams\n",
    "            bestParamScoreDict=scoreDict\n",
    "    # End grid/parameter loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST PARAMETERS:\n",
      "{'article_stats': False, 'display_graph': True, 'input_file': './data/articles.csv', 'lemma_conversion': False, 'max_length': 50, 'ngram_max': 3, 'nlp_library': 'nltk', 'parts_of_speech': ['PROPER', 'VERB'], 'process_date': '2016-09-01', 'stop_words_file': './data/stopWords.txt', 'story_threshold': 0.26, 'tfidf_binary': False, 'tfidf_maxdf': 0.5, 'tfidf_mindf': 2, 'tfidf_norm': 'l2'}\n",
      "{'score': 3914, 'inGood': 60, 'inBad': 2, 'outGood': 3860, 'outBad': 4}\n",
      "ERROR: 214876 should NOT be in Trump meeting\n",
      "ERROR: 57366 should be in Trump meeting\n",
      "ERROR: 71339 should be in Trump meeting\n",
      "ERROR: 44642 should NOT be in Safe space\n",
      "ERROR: 39232 should NOT be in David Brown\n",
      "ERROR: 71350 should NOT be in David Brown\n"
     ]
    }
   ],
   "source": [
    "# Set threshold to input value from best (and possibly only) run for use in results analysis\n",
    "# Unless not specified at all\n",
    "if 'story_threshold' in bestParams and bestParams['story_threshold']!=None:\n",
    "    threshold=bestParams['story_threshold']\n",
    "else:\n",
    "    threshold=None\n",
    "\n",
    "\n",
    "# If there was a real parameter grid, then output/refresh results\n",
    "if len(parameterGrid)>=1:\n",
    "    print(\"BEST PARAMETERS:\")\n",
    "    print(bestParams)\n",
    "    print(bestParamScoreDict)\n",
    "    scoreCurrentParamGuess(tfidfVectors,storyMap,articleDataFrame,threshold,printErrors=True)\n",
    "    # Recreate vector for best results in loop\n",
    "    # terms is just used later on if analysis of final results is requested\n",
    "    tfidfVectors,terms=preprocessAndVectorize(articleDataFrame,\n",
    "                                            bestParams,\n",
    "                                            pos_nlp_mapping,\n",
    "                                            nlp,\n",
    "                                            nl,\n",
    "                                            wordnet_lemmatizer,\n",
    "                                            stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
