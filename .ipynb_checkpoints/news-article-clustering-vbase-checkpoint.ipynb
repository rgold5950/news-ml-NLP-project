{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runParams={'tfidf_maxdf':      [0.5],\n",
    "           'input_file':       ['./data/articles_expanded_7.10.21.csv'],\n",
    "           'story_threshold':  [0.26],\n",
    "           'process_date':     ['2016-09-01'],\n",
    "           'parts_of_speech':  [['PROPER', 'VERB']],\n",
    "           'lemma_conversion': [False],\n",
    "           'ngram_max':        [3],\n",
    "           'tfidf_binary':     [False],\n",
    "           'tfidf_norm':       ['l2'],\n",
    "           'nlp_library':      ['nltk'],\n",
    "           'max_length':       [50],\n",
    "           'stop_words_file':  ['./data/stopWords.txt'],\n",
    "           'tfidf_mindf':      [2],\n",
    "           'display_graph':    [True],\n",
    "           'article_stats':    [False]}\n",
    "\n",
    "# Use parameter grid even if there is only set of parameters\n",
    "parameterGrid=ParameterGrid(runParams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and initialise required NLP libraries\n",
    "pos_nlp_mapping={}\n",
    "nl=None\n",
    "wordnet_lemmatizer=None\n",
    "nlp=None\n",
    "if 'spaCy' in runParams['nlp_library']:\n",
    "    import spacy\n",
    "    nlp=spacy.load('en')\n",
    "    pos_nlp_mapping['spaCy']={'VERB':['VERB'],'PROPER':['PROPN'],'COMMON':['NOUN']}\n",
    "    \n",
    "if 'nltk' in runParams['nlp_library']:\n",
    "    import nltk as nl\n",
    "    if True in runParams['lemma_conversion']:\n",
    "        from nltk.stem import WordNetLemmatizer\n",
    "    else:\n",
    "        wordnet_lemmatizer=None\n",
    "    pos_nlp_mapping['nltk']={'VERB': ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'],'PROPER':['NNP','NNPS'],'COMMON':['NN','NNS']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInputDataAndDisplayStats(filename,processDate,printSummary=False):\n",
    "    df = pd.read_csv(filename)\n",
    "    df.drop_duplicates('content')\n",
    "    df = df[~df['content'].isnull()]\n",
    "    df=df[df['content'].str.len()>=200]\n",
    "\n",
    "    targetString=\"(Want to get this briefing by email?\"\n",
    "    df['NYT summary']=df['content'].map(lambda d: d[:len(targetString)]==targetString)\n",
    "    df=df[df['NYT summary']==False]\n",
    "\n",
    "    # The following removes a warning that appears in many of the Atlantic articles.\n",
    "    # Since it is commonly at the beginning, it brings a lot of noise to the search for similar articles\n",
    "    # And subsequently to the assessment of sentiment\n",
    "    targetString=\"For us to continue writing great stories, we need to display ads.             Please select the extension that is blocking ads.     Please follow the steps below\"\n",
    "    df['content']=df['content'].str.replace(targetString,'')\n",
    "\n",
    "    # This is also for some Atlantic articles for the same reasons as above\n",
    "    targetString=\"This article is part of a feature we also send out via email as The Atlantic Daily, a newsletter with stories, ideas, and images from The Atlantic, written specially for subscribers. To sign up, please enter your email address in the field provided here.\"\n",
    "    df=df[df['content'].str.contains(targetString)==False]\n",
    "\n",
    "    # This is also for some Atlantic articles for the same reasons as above\n",
    "    targetString=\"This article is part of a feature we also send out via email as Politics  Policy Daily, a daily roundup of events and ideas in American politics written specially for newsletter subscribers. To sign up, please enter your email address in the field provided here.\"\n",
    "    df=df[df['content'].str.contains(targetString)==False]\n",
    "\n",
    "    # More Atlantic-specific removals (for daily summaries with multiple stories contained)\n",
    "    df=df[df['content'].str.contains(\"To sign up, please enter your email address in the field\")==False]\n",
    "\n",
    "    # Remove daily CNN summary\n",
    "    targetString=\"CNN Student News\"\n",
    "    df=df[df['content'].str.contains(targetString)==False]\n",
    "    \n",
    "    \n",
    "    print(\"\\nArticle counts by publisher:\")\n",
    "    print(df['publication'].value_counts())\n",
    "\n",
    "    print(\"\\nArticle counts by date:\")\n",
    "    print(df['date'].value_counts())\n",
    "\n",
    "#     Restrict to articles on the provided input date.\n",
    "#     This date is considered mandatory for topic clustering but is not required for sentiment\n",
    "#     since sentiment only processes a specified list of articles.\n",
    "#     For topic clustering it is essential to have the date as it is\n",
    "#     enormously significant in article matching.\n",
    "\n",
    "#     if processDate!=None:\n",
    "#         df=df[df['date']==processDate]\n",
    "#     df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    # Remove non-ASCII characters\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    df['content no nonascii']=df['content'].map(lambda x: removeNonASCIICharacters(x))\n",
    "\n",
    "    print(\"\\nFinal dataset:\\n\\nDate:\",processDate,\"\\n\")\n",
    "    print(df['publication'].value_counts())\n",
    "    df.to_csv(r'C:\\Users\\goldm\\Capstone\\tracking files\\getInputDataAndDisplayStats.csv')\n",
    "    \n",
    "    return df\n",
    "\n",
    "##########################################################################################\n",
    "\n",
    "def removeNonASCIICharacters(textString): \n",
    "    return \"\".join(i for i in textString if ord(i)<128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Article counts by publisher:\n",
      "Breitbart          104\n",
      "NY Post             61\n",
      "Reuters             60\n",
      "CNN                 58\n",
      "NPR                 54\n",
      "                  ... \n",
      "@ChinaDailyApp       1\n",
      "Mashable             1\n",
      "VOA                  1\n",
      "The White House      1\n",
      "thestar.com          1\n",
      "Name: publication, Length: 63, dtype: int64\n",
      "\n",
      "Article counts by date:\n",
      "9/1/2016     434\n",
      "12/2/2016    349\n",
      "Name: date, dtype: int64\n",
      "\n",
      "Final dataset:\n",
      "\n",
      "Date: 2016-09-01 \n",
      "\n",
      "Breitbart          104\n",
      "NY Post             61\n",
      "Reuters             60\n",
      "CNN                 58\n",
      "NPR                 54\n",
      "                  ... \n",
      "@ChinaDailyApp       1\n",
      "Mashable             1\n",
      "VOA                  1\n",
      "The White House      1\n",
      "thestar.com          1\n",
      "Name: publication, Length: 63, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "articleDataFrame=getInputDataAndDisplayStats(runParams['input_file'][0],\n",
    "                                             runParams['process_date'][0],\n",
    "                                             runParams['article_stats'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Article counts by publisher:\n",
      "Breitbart          104\n",
      "NY Post             61\n",
      "Reuters             60\n",
      "CNN                 58\n",
      "NPR                 54\n",
      "                  ... \n",
      "@ChinaDailyApp       1\n",
      "Mashable             1\n",
      "VOA                  1\n",
      "The White House      1\n",
      "thestar.com          1\n",
      "Name: publication, Length: 63, dtype: int64\n",
      "\n",
      "Article counts by date:\n",
      "9/1/2016     434\n",
      "12/2/2016    349\n",
      "Name: date, dtype: int64\n",
      "\n",
      "Final dataset:\n",
      "\n",
      "Date: 2016-09-01 \n",
      "\n",
      "Breitbart          104\n",
      "NY Post             61\n",
      "Reuters             60\n",
      "CNN                 58\n",
      "NPR                 54\n",
      "                  ... \n",
      "@ChinaDailyApp       1\n",
      "Mashable             1\n",
      "VOA                  1\n",
      "The White House      1\n",
      "thestar.com          1\n",
      "Name: publication, Length: 63, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>publication</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "      <th>...</th>\n",
       "      <th>Unnamed: 331</th>\n",
       "      <th>Unnamed: 332</th>\n",
       "      <th>Unnamed: 333</th>\n",
       "      <th>Unnamed: 334</th>\n",
       "      <th>Unnamed: 335</th>\n",
       "      <th>Unnamed: 336</th>\n",
       "      <th>Unnamed: 337</th>\n",
       "      <th>Unnamed: 338</th>\n",
       "      <th>NYT summary</th>\n",
       "      <th>content no nonascii</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3079.0</td>\n",
       "      <td>20694.0</td>\n",
       "      <td>Review: In ‘Independence Day: Resurgence,' the...</td>\n",
       "      <td>NY Times</td>\n",
       "      <td>Manohla Dargis</td>\n",
       "      <td>12/2/2016</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>If you've seen one movie apocalypse, you have ...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>If you've seen one movie apocalypse, you have ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3241.0</td>\n",
       "      <td>20875.0</td>\n",
       "      <td>The Agony of the Digital Tease - The NY Times</td>\n",
       "      <td>NY Times</td>\n",
       "      <td>Jessica Bennett</td>\n",
       "      <td>12/2/2016</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>There was the breadcrumb dropped on Valentine'...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>There was the breadcrumb dropped on Valentine'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3336.0</td>\n",
       "      <td>20986.0</td>\n",
       "      <td>Fox News's Convention Moment Overshadowed by S...</td>\n",
       "      <td>NY Times</td>\n",
       "      <td>Jim Rutenberg</td>\n",
       "      <td>12/2/2016</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CLEVELAND  —   This was supposed to be Fox New...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>CLEVELAND     This was supposed to be Fox News...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3722.0</td>\n",
       "      <td>21413.0</td>\n",
       "      <td>One Star Over, a Planet That Might Be Another ...</td>\n",
       "      <td>NY Times</td>\n",
       "      <td>Kenneth Chang</td>\n",
       "      <td>9/1/2016</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Another Earth could be circling the star right...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>Another Earth could be circling the star right...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3748.0</td>\n",
       "      <td>21448.0</td>\n",
       "      <td>University of Chicago Strikes Back Against Cam...</td>\n",
       "      <td>NY Times</td>\n",
       "      <td>Richard Pérez-Peña, Mitch Smith and Stephanie ...</td>\n",
       "      <td>9/1/2016</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The anodyne welcome letter to incoming freshme...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>The anodyne welcome letter to incoming freshme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>NaN</td>\n",
       "      <td>217535.0</td>\n",
       "      <td>Nancy Pelosi’s office sums up Trump with four ...</td>\n",
       "      <td>The Independent</td>\n",
       "      <td>Louise Hall</td>\n",
       "      <td>9/1/2016</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US House Speaker  Nancy Pelosi  ’s office has ...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>US House Speaker  Nancy Pelosi  s office has g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>NaN</td>\n",
       "      <td>217536.0</td>\n",
       "      <td>Rep. Adam Kinzinger suspects fellow GOP lawmak...</td>\n",
       "      <td>Business Insider</td>\n",
       "      <td>John Haltiwanger</td>\n",
       "      <td>9/1/2016</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-  Adam Kinzinger told NYT Magazine that he su...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>-  Adam Kinzinger told NYT Magazine that he su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780</th>\n",
       "      <td>NaN</td>\n",
       "      <td>217537.0</td>\n",
       "      <td>Yahoo ist jetzt Teil von Verizon Media</td>\n",
       "      <td>news.yahoo.com</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>9/1/2016</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yahoo ist Teil von  Verizon Media  .\\nDurch Kl...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>Yahoo ist Teil von  Verizon Media  .\\nDurch Kl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781</th>\n",
       "      <td>NaN</td>\n",
       "      <td>217538.0</td>\n",
       "      <td>Yahoo ist jetzt Teil von Verizon Media</td>\n",
       "      <td>news.yahoo.com</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>9/1/2016</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yahoo ist Teil von  Verizon Media  .\\nDurch Kl...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>Yahoo ist Teil von  Verizon Media  .\\nDurch Kl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782</th>\n",
       "      <td>NaN</td>\n",
       "      <td>217539.0</td>\n",
       "      <td>Remaining Capitol fence to be removed starting...</td>\n",
       "      <td>TheHill</td>\n",
       "      <td>Rebecca Beitsch</td>\n",
       "      <td>9/1/2016</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>U.S. Capitol Police (USCP) will begin to remov...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>U.S. Capitol Police (USCP) will begin to remov...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>783 rows × 341 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0        id                                              title  \\\n",
       "0        3079.0   20694.0  Review: In ‘Independence Day: Resurgence,' the...   \n",
       "1        3241.0   20875.0      The Agony of the Digital Tease - The NY Times   \n",
       "2        3336.0   20986.0  Fox News's Convention Moment Overshadowed by S...   \n",
       "3        3722.0   21413.0  One Star Over, a Planet That Might Be Another ...   \n",
       "4        3748.0   21448.0  University of Chicago Strikes Back Against Cam...   \n",
       "..          ...       ...                                                ...   \n",
       "778         NaN  217535.0  Nancy Pelosi’s office sums up Trump with four ...   \n",
       "779         NaN  217536.0  Rep. Adam Kinzinger suspects fellow GOP lawmak...   \n",
       "780         NaN  217537.0             Yahoo ist jetzt Teil von Verizon Media   \n",
       "781         NaN  217538.0             Yahoo ist jetzt Teil von Verizon Media   \n",
       "782         NaN  217539.0  Remaining Capitol fence to be removed starting...   \n",
       "\n",
       "          publication                                             author  \\\n",
       "0            NY Times                                     Manohla Dargis   \n",
       "1            NY Times                                    Jessica Bennett   \n",
       "2            NY Times                                      Jim Rutenberg   \n",
       "3            NY Times                                      Kenneth Chang   \n",
       "4            NY Times  Richard Pérez-Peña, Mitch Smith and Stephanie ...   \n",
       "..                ...                                                ...   \n",
       "778   The Independent                                        Louise Hall   \n",
       "779  Business Insider                                   John Haltiwanger   \n",
       "780    news.yahoo.com                                              FALSE   \n",
       "781    news.yahoo.com                                              FALSE   \n",
       "782           TheHill                                    Rebecca Beitsch   \n",
       "\n",
       "          date    year  month  url  \\\n",
       "0    12/2/2016  2016.0   12.0  NaN   \n",
       "1    12/2/2016  2016.0   12.0  NaN   \n",
       "2    12/2/2016  2016.0   12.0  NaN   \n",
       "3     9/1/2016  2016.0    9.0  NaN   \n",
       "4     9/1/2016  2016.0    9.0  NaN   \n",
       "..         ...     ...    ...  ...   \n",
       "778   9/1/2016  2016.0    9.0  NaN   \n",
       "779   9/1/2016  2016.0    9.0  NaN   \n",
       "780   9/1/2016  2016.0    9.0  NaN   \n",
       "781   9/1/2016  2016.0    9.0  NaN   \n",
       "782   9/1/2016  2016.0    9.0  NaN   \n",
       "\n",
       "                                               content  ... Unnamed: 331  \\\n",
       "0    If you've seen one movie apocalypse, you have ...  ...          NaN   \n",
       "1    There was the breadcrumb dropped on Valentine'...  ...          NaN   \n",
       "2    CLEVELAND  —   This was supposed to be Fox New...  ...          NaN   \n",
       "3    Another Earth could be circling the star right...  ...          NaN   \n",
       "4    The anodyne welcome letter to incoming freshme...  ...          NaN   \n",
       "..                                                 ...  ...          ...   \n",
       "778  US House Speaker  Nancy Pelosi  ’s office has ...  ...          NaN   \n",
       "779  -  Adam Kinzinger told NYT Magazine that he su...  ...          NaN   \n",
       "780  Yahoo ist Teil von  Verizon Media  .\\nDurch Kl...  ...          NaN   \n",
       "781  Yahoo ist Teil von  Verizon Media  .\\nDurch Kl...  ...          NaN   \n",
       "782  U.S. Capitol Police (USCP) will begin to remov...  ...          NaN   \n",
       "\n",
       "    Unnamed: 332 Unnamed: 333 Unnamed: 334 Unnamed: 335 Unnamed: 336  \\\n",
       "0            NaN          NaN          NaN          NaN          NaN   \n",
       "1            NaN          NaN          NaN          NaN          NaN   \n",
       "2            NaN          NaN          NaN          NaN          NaN   \n",
       "3            NaN          NaN          NaN          NaN          NaN   \n",
       "4            NaN          NaN          NaN          NaN          NaN   \n",
       "..           ...          ...          ...          ...          ...   \n",
       "778          NaN          NaN          NaN          NaN          NaN   \n",
       "779          NaN          NaN          NaN          NaN          NaN   \n",
       "780          NaN          NaN          NaN          NaN          NaN   \n",
       "781          NaN          NaN          NaN          NaN          NaN   \n",
       "782          NaN          NaN          NaN          NaN          NaN   \n",
       "\n",
       "    Unnamed: 337 Unnamed: 338 NYT summary  \\\n",
       "0            NaN          NaN       False   \n",
       "1            NaN          NaN       False   \n",
       "2            NaN          NaN       False   \n",
       "3            NaN          NaN       False   \n",
       "4            NaN          NaN       False   \n",
       "..           ...          ...         ...   \n",
       "778          NaN          NaN       False   \n",
       "779          NaN          NaN       False   \n",
       "780          NaN          NaN       False   \n",
       "781          NaN          NaN       False   \n",
       "782          NaN          NaN       False   \n",
       "\n",
       "                                   content no nonascii  \n",
       "0    If you've seen one movie apocalypse, you have ...  \n",
       "1    There was the breadcrumb dropped on Valentine'...  \n",
       "2    CLEVELAND     This was supposed to be Fox News...  \n",
       "3    Another Earth could be circling the star right...  \n",
       "4    The anodyne welcome letter to incoming freshme...  \n",
       "..                                                 ...  \n",
       "778  US House Speaker  Nancy Pelosi  s office has g...  \n",
       "779  -  Adam Kinzinger told NYT Magazine that he su...  \n",
       "780  Yahoo ist Teil von  Verizon Media  .\\nDurch Kl...  \n",
       "781  Yahoo ist Teil von  Verizon Media  .\\nDurch Kl...  \n",
       "782  U.S. Capitol Police (USCP) will begin to remov...  \n",
       "\n",
       "[783 rows x 341 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getInputDataAndDisplayStats(runParams['input_file'][0],\n",
    "                            runParams['process_date'][0],\n",
    "                            printSummary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadStopWords(stopWordsFileName):\n",
    "    stop_words=[]\n",
    "    f=open(stopWordsFileName, 'r')\n",
    "    for l in f.readlines():\n",
    "        stop_words.append(l.replace('\\n', ''))\n",
    "    return stop_words\n",
    "\n",
    "stop_words=loadStopWords(runParams['stop_words_file'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stringNLTKProcess(nl,stringToConvert,partsOfSpeech,stop_words,maxWords=None,lemmatizer=None):\n",
    "    sentences=nl.sent_tokenize(stringToConvert)\n",
    "    str=[]\n",
    "    for sentence in sentences:\n",
    "        wordString=[]\n",
    "        for word,pos in nl.pos_tag(nl.word_tokenize(sentence)):\n",
    "            # The following condition avoids any POS which corresponds to punctuation (and takes all others)\n",
    "            if partsOfSpeech==None:\n",
    "                if pos[0]>='A' and pos[0]<='Z':\n",
    "                    wordString.append(word)\n",
    "            elif pos in partsOfSpeech:\n",
    "                wordString.append(word)\n",
    "        for wrd in wordString:\n",
    "            wrdlower=wrd.lower()\n",
    "            if wrdlower not in stop_words and wrdlower!=\"'s\":\n",
    "                if maxWords==None or len(str)<maxWords:\n",
    "                    if lemmatizer==None:\n",
    "                        str.append(wrdlower)\n",
    "                    else:\n",
    "                        str.append(lemmatizer.lemmatize(wrd.lower(), pos='v'))\n",
    "            if maxWords!=None and len(str)==maxWords:\n",
    "                return ' '.join(str)\n",
    "    return ' '.join(str)\n",
    "\n",
    "def removeSpacesAndPunctuation(textString):\n",
    "    return \"\".join(i for i in textString if (ord(i)>=48 and ord(i)<=57) or (ord(i)>=97 and ord(i)<=122))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setupStoryMapAndReportList(args=None, reportArticleList=None,storyMapFileName=None):\n",
    "    # Story Map is used in fitting if grid search is applied (As ground truth)\n",
    "    # It is also used in graph if no threshold provided (to determine colours, not to determine location)\n",
    "    # Report Article List is used at the end to create a report with, for each\n",
    "    # article in the list, the set of articles within tolerance, and the key words for each\n",
    "    if args==None:\n",
    "        articleList=reportArticleList\n",
    "        fileName=storyMapFileName\n",
    "    else:\n",
    "        articleList=args['article_id_list']\n",
    "        fileName=args['story_map_validation']\n",
    "    \n",
    "    reportArticleList=articleList\n",
    "    if fileName!=None:\n",
    "        storyMap=readStoryMapFromFile(fileName)\n",
    "        if reportArticleList==None:\n",
    "            reportArticleList=[]\n",
    "            for story, articleList in storyMap.items():\n",
    "                reportArticleList.append(articleList[0])\n",
    "    else:\n",
    "        storyMap=None\n",
    "    return storyMap,reportArticleList\n",
    "\n",
    "def readStoryMapFromFile(filename):\n",
    "    return readDictFromCsvFile(filename,'StoryMap')\n",
    "\n",
    "def readGridParameterRangeFromFile(filename):\n",
    "    return readDictFromCsvFile(filename, 'GridParameters')\n",
    "\n",
    "def readDictFromCsvFile(filename,schema):\n",
    "    gridParamDict={} \n",
    "    with open(filename,'r') as f:\n",
    "        for row in f:\n",
    "            row=row[:-1] # Exclude the carriage return\n",
    "            row=row.split(',')\n",
    "            key=row[0]\n",
    "            vals=row[1:]\n",
    "            \n",
    "            if schema=='GridParameters':\n",
    "                if key in ['story_threshold','tfidf_maxdf']:\n",
    "                    finalVals=list(float(n) for n in vals)\n",
    "                elif key in ['ngram_max','tfidf_mindf','max_length']:\n",
    "                    finalVals=list(int(n) for n in vals)\n",
    "                elif key in ['lemma_conversion','tfidf_binary']:\n",
    "                    finalVals = list(str2bool(n) for n in vals)\n",
    "                elif key in ['parts_of_speech']:\n",
    "                    listlist=[]\n",
    "                    for v in vals:\n",
    "                        listlist.append(v_split('+'))\n",
    "                    finalVals = listlist\n",
    "                elif key in ['tfidf_norm','nlp_library']:\n",
    "                    finalVals=vals\n",
    "                else:\n",
    "                    print(key)\n",
    "                    print(\"KEY ERROR\")\n",
    "                    return\n",
    "            elif schema == 'StoryMap':\n",
    "                finalVals = list(int(n) for n in vals if n!='')\n",
    "            else:\n",
    "                print(schema)\n",
    "                print('SCHEMA ERROR')\n",
    "                return\n",
    "            \n",
    "            gridParamDict[key]=finalVals\n",
    "    return gridParamDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "storyMap,reportArticleList=setupStoryMapAndReportList(storyMapFileName='storyMapForValidation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trump meeting : [151832, 110126, 172078, 48306, 57365, 190512, 26536, 71335, 21499, 23872, 142033, 110133, 23888, 71336, 57366, 71339]\n",
      "Brazil impeachment : [120639, 80103, 25225, 21502, 57362, 120636, 110141]\n",
      "Kaepernick : [40617, 40543, 39520, 80109, 80101, 47403]\n",
      "Clinton Guccifer : [214888, 85803, 47979]\n",
      "Farage : [37252, 37468, 46175]\n",
      "Anthony Weiner : [49480, 110144, 142300, 214934]\n",
      "SpaceX : [38658, 134545, 172095, 214894]\n",
      "Safe space : [21448, 78169, 78171]\n",
      "Lauer debate : [43447, 47078, 138709]\n",
      "Venezuela : [172079, 57375, 190522]\n",
      "Iran deal : [158005, 48823, 57373, 120634]\n",
      "Penn State : [80094, 157527, 214892]\n",
      "David Brown : [172085, 80096, 141886]\n"
     ]
    }
   ],
   "source": [
    "for story, articleList in storyMap.items():\n",
    "    print(story,\":\",articleList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessAndVectorize(articleDataFrame,args,pos_nlp_mapping,nlp,nl,wordnet_lemmatizer,stop_words):\n",
    "    # Map the input parts of speech list to the coding required for the specific NLP library\n",
    "    if args['parts_of_speech'][0]!='ALL':\n",
    "        partsOfSpeech=[]\n",
    "        for pos in args['parts_of_speech']:\n",
    "            partsOfSpeech.append(pos_nlp_mapping[args['nlp_library']][pos])\n",
    "        partsOfSpeech=[item for sublist in partsOfSpeech for item in sublist]\n",
    "    else:\n",
    "        partsOfSpeech=None\n",
    "    \n",
    "    # Processing of text depends on NLP library choice\n",
    "    if args['nlp_library']=='spaCy':\n",
    "        articleDataFrame['input to vectorizer']=articleDataFrame['content no nonascii'].map(lambda x: stringSpaCyProcess(nlp,\n",
    "                                                                                                                         x,\n",
    "                                                                                                                         partsOfSpeech=partsOfSpeech,\n",
    "                                                                                                                         maxWords=args['max_length'],\n",
    "                                                                                                                         stop_words=stop_words,\n",
    "                                                                                                                         lemmatize=args['lemma_conversion']))\n",
    "    elif args['nlp_library']=='nltk':\n",
    "        articleDataFrame['input to vectorizer']=articleDataFrame['content no nonascii'].map(lambda x: stringNLTKProcess(nl,\n",
    "                                                                                                                        x,\n",
    "                                                                                                                        partsOfSpeech=partsOfSpeech,\n",
    "                                                                                                                        stop_words=stop_words,\n",
    "                                                                                                                        maxWords=args['max_length'],\n",
    "                                                                                                                        lemmatizer=wordnet_lemmatizer))\n",
    "    else:\n",
    "        print(\"PROBLEM... NO VALID NLP LIBRARY... MUST BE nltk OR spaCy\")\n",
    "\n",
    "    # To get default values a couple of parameters need to be not passed if not specified on the command line\n",
    "    # Passing as None behaves differently to passing no parameter (which would invoke the default value)\n",
    "    optArgsForVectorizer={}\n",
    "    if args['tfidf_maxdf'] != None:\n",
    "        optArgsForVectorizer['max_df']=args['tfidf_maxdf']\n",
    "    if args['tfidf_mindf'] != None:\n",
    "        optArgsForVectorizer['min_df']=args['tfidf_mindf']\n",
    "    # Create and run the vectorize\n",
    "    vectorizer=TfidfVectorizer(analyzer='word',\n",
    "                               ngram_range=(1,args['ngram_max']),\n",
    "                              lowercase=True,\n",
    "                              binary=args['tfidf_binary'],\n",
    "                              norm=args['tfidf_norm'],\n",
    "                              **optArgsForVectorizer)\n",
    "    tfidfVectors=vectorizer.fit_transform(articleDataFrame['input to vectorizer'])\n",
    "    terms=vectorizer.get_feature_names()\n",
    "    return tfidfVectors, terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scoreCurrentParamGuess(tfidfVectors,storyMap,articleDataFrame,threshold,printErrors=False):\n",
    "    # Work with distances relative to first item in each cluster - even though this is clearly arbitrary since that\n",
    "    # point could be an outlier in the cluster and hence might cause problems.\n",
    "    # But I have to start somewhere - and can refine it later if needed.\n",
    "\n",
    "    nonZeroCoords=initialiseAllNonZeroCoords(tfidfVectors)\n",
    "    score=0\n",
    "    outGood=0\n",
    "    outBad=0\n",
    "    inGood=0\n",
    "    inBad=0\n",
    "    \n",
    "    #### ---- Richard Modification- adding in code to print out a df of articles in the story map and their respective category       \n",
    "    final_mapping_list = []\n",
    "    \n",
    "    for story, storyArticles in storyMap.items():\n",
    "        leadArticleIndex=articleDataFrame[articleDataFrame['id']==storyArticles[0]].index[0]\n",
    "        # Compute score of all articles in corpus relative to first article in story (.product)\n",
    "        # Then count through list relative to threshold (add one for a good result, subtract one for a bad result)\n",
    "        scores=productRelatednessScores(tfidfVectors,nonZeroCoords,leadArticleIndex)\n",
    "        rankedIndices=np.argsort(scores)\n",
    "        foundRelatedArticles=[]\n",
    "        # THE SORTING HERE IS NOT STRICTLY REQUIRED, BUT I COULD USE IT SO THAT ONCE THE THRESHOLD IS PASSED\n",
    "        # IN THE LOOP, THEN I INFER THE REMAINING RESULTS\n",
    "        for article in reversed(rankedIndices):\n",
    "            thisArticleIndex=articleDataFrame['id'][article]\n",
    "            if thisArticleIndex in storyArticles:\n",
    "                if scores[article]>=threshold: # article IS supposed to be in range\n",
    "                    score+=1\n",
    "                    inGood+=1\n",
    "                    #appending the article and its mapping according to the predictions of our model\n",
    "                    final_mapping_list.append([thisArticleIndex, story, 'TP'])\n",
    "                else:\n",
    "                    score-=1\n",
    "                    inBad+=1\n",
    "                    final_mapping_list.append([thisArticleIndex, 'No Mapping', 'FN'])\n",
    "                    if printErrors:\n",
    "                        print(\"ERROR:\",thisArticleIndex,\"should be in\",story)\n",
    "            else: # article not supposed to be in range\n",
    "                if scores[article]<=threshold:\n",
    "                    score+=1\n",
    "                    outGood+=1\n",
    "                    final_mapping_list.append([thisArticleIndex, 'No Mapping', 'TN'])\n",
    "                else:\n",
    "                    score-=1\n",
    "                    outBad+=1\n",
    "                    final_mapping_list.append([thisArticleIndex, story, 'FP'])\n",
    "                    if printErrors:\n",
    "                        print(\"ERROR:\",thisArticleIndex,\"should NOT be in\",story)\n",
    "    \n",
    "    #### ---- Richard Modification- adding in code to print out a df of articles in the story map and their respective categor\n",
    "    final_mapping_df = pd.DataFrame(final_mapping_list, columns = ['article', 'cateogry ID', 'FP/FN/TP/TN'])\n",
    "    final_mapping_df.to_csv(r\"C:\\Users\\goldm\\OneDrive\\Desktop\\Richard's Files\\Data Science\\News ML Project\\PredictedMappings.csv\")\n",
    "    \n",
    "    scoreDict={'score':score,'inGood':inGood,'inBad':inBad,'outGood':outGood,'outBad':outBad}\n",
    "    return scoreDict\n",
    "\n",
    "##########################################################################################\n",
    "\n",
    "def initialiseAllNonZeroCoords(tfidfVectors):\n",
    "# This function just exists since it seems to be expensive and I'd rather not call it multiple times\n",
    "# Hence it is intended to be called outside of loops in order to simplify the row specific processing\n",
    "    values=[]\n",
    "    nzc=zip(*tfidfVectors.nonzero())\n",
    "\n",
    "    # In Python 3 the zip can only be iterated through one time before it is automatically released\n",
    "    # So need to copy the results otherwise the main loop below will no longer work\n",
    "    pointList=[]\n",
    "    for i,j in nzc:\n",
    "        pointList.append([i,j])\t\t\n",
    "\n",
    "    for row in range(tfidfVectors.shape[0]):\n",
    "        rowList=[]\n",
    "        for i,j in pointList:\n",
    "            if row==i:\n",
    "                rowList.append(j)\n",
    "        values.append(rowList)\n",
    "\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def productRelatednessScores(tfidfVectors,nonZeroCoords,refRow):\n",
    "    # instantiates a matrix of zeros with tfidVectors.shape[0] rows corresponding to the number of rows in the tfidVectors array\n",
    "    scores = [0]*tfidfVectors.shape[0]\n",
    "    for toRow in range(tfidfVectors.shape[0]):\n",
    "        scores[toRow] = sum([(tfidfVectors[toRow,w]*tfidfVectors[refRow,w]) for w in nonZeroCoords[refRow] if w in nonZeroCoords[toRow]])\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\goldm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk; nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop across all parameter combinations in grid to determine best set\n",
    "# If not doing grid search, will just pass through the loop once\n",
    "bestParamScoreDict={'score':-1000000}\n",
    "bestParams=parameterGrid[0]\n",
    "for i,currentParams in enumerate(parameterGrid):\n",
    "    if len(parameterGrid)>1:\n",
    "        print(\"Combination:\",i+1,\"of\",len(parameterGrid))\n",
    "        print(currentParams)\n",
    "        \n",
    "        # Determine tf-idf vectors\n",
    "        # terms is just used later on if analysis of final results is requested\n",
    "    tfidfVectors,terms=preprocessAndVectorize(articleDataFrame,\n",
    "                                              currentParams,\n",
    "                                              pos_nlp_mapping,\n",
    "                                              nlp,\n",
    "                                              nl,\n",
    "                                              wordnet_lemmatizer,\n",
    "                                              stop_words)\n",
    "\n",
    "    # Compute scores if threshold provided (meaning as part of grid search)\n",
    "    if 'story_threshold' in currentParams and currentParams['story_threshold']!=None:\n",
    "        scoreDict = scoreCurrentParamGuess(tfidfVectors,storyMap,articleDataFrame,currentParams['story_threshold'])\n",
    "\n",
    "        # Update best so far\n",
    "        if scoreDict['score']>=bestParamScoreDict['score']:\n",
    "            if len(parameterGrid)>1:\n",
    "                print(i+1,\"is the best so far!\")\n",
    "            bestParams=currentParams\n",
    "            bestParamScoreDict=scoreDict\n",
    "    # End grid/parameter loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST PARAMETERS:\n",
      "{'article_stats': False, 'display_graph': True, 'input_file': './data/articles_expanded_7.10.21.csv', 'lemma_conversion': False, 'max_length': 50, 'ngram_max': 3, 'nlp_library': 'nltk', 'parts_of_speech': ['PROPER', 'VERB'], 'process_date': '2016-09-01', 'stop_words_file': './data/stopWords.txt', 'story_threshold': 0.26, 'tfidf_binary': False, 'tfidf_maxdf': 0.5, 'tfidf_mindf': 2, 'tfidf_norm': 'l2'}\n",
      "{'score': 10149, 'inGood': 54, 'inBad': 8, 'outGood': 10110, 'outBad': 7}\n",
      "ERROR: 110133.0 should be in Trump meeting\n",
      "ERROR: 71336.0 should be in Trump meeting\n",
      "ERROR: 71335.0 should be in Trump meeting\n",
      "ERROR: 142033.0 should be in Trump meeting\n",
      "ERROR: 57366.0 should be in Trump meeting\n",
      "ERROR: 71339.0 should be in Trump meeting\n",
      "ERROR: 120636.0 should be in Brazil impeachment\n",
      "ERROR: 50137.0 should NOT be in Kaepernick\n",
      "ERROR: 80101.0 should be in Kaepernick\n",
      "ERROR: 59412.0 should NOT be in Anthony Weiner\n",
      "ERROR: 193926.0 should NOT be in SpaceX\n",
      "ERROR: 44642.0 should NOT be in Safe space\n",
      "ERROR: 39232.0 should NOT be in David Brown\n",
      "ERROR: 22613.0 should NOT be in David Brown\n",
      "ERROR: 71350.0 should NOT be in David Brown\n"
     ]
    }
   ],
   "source": [
    "# Set threshold to input value from best (and possibly only) run for use in results analysis\n",
    "# Unless not specified at all\n",
    "if 'story_threshold' in bestParams and bestParams['story_threshold']!=None:\n",
    "    threshold=bestParams['story_threshold']\n",
    "else:\n",
    "    threshold=None\n",
    "\n",
    "\n",
    "# If there was a real parameter grid, then output/refresh results\n",
    "if len(parameterGrid)>=1:\n",
    "    print(\"BEST PARAMETERS:\")\n",
    "    print(bestParams)\n",
    "    print(bestParamScoreDict)\n",
    "    scoreCurrentParamGuess(tfidfVectors,storyMap,articleDataFrame,threshold,printErrors=True)\n",
    "    # Recreate vector for best results in loop\n",
    "    # terms is just used later on if analysis of final results is requested\n",
    "    tfidfVectors,terms=preprocessAndVectorize(articleDataFrame,\n",
    "                                            bestParams,\n",
    "                                            pos_nlp_mapping,\n",
    "                                            nlp,\n",
    "                                            nl,\n",
    "                                            wordnet_lemmatizer,\n",
    "                                            stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
